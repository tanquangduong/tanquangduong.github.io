---
title: "[TLDR] KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models"
subtitle: "Combining Large Language Models and Food Knowledge Graphs for Personalized Food Recommendation and Recipe Generation"
date: "2025-05-20"
---

**Authors:** F. Mohbat et al.
**Published on Arxiv:** 2025-05-20
**Link:** <http://arxiv.org/abs/2505.14629v1>
**Institutions:** Rensselaer Polytechnic Institute
**Keywords:** large language models, knowledge graph, retrieval-augmented generation, recipe recommendation, personalization, LoRA, FoodKG, nutrition analysis, Phi-3, recipe generation, constrained question answering, benchmark dataset, personal preferences, food computing

<!-- Context -->

Recent advances in large language models (LLMs) and greater availability of food data have spurred research in food understanding, recipe generation, and personalized food recommendation systems. While prior work has leveraged LLMs or knowledge graphs (KGs) separately, few have unified food KG integration with LLMs to address personal preferences, dietary restrictions, and nutritional needs within a single framework.

To address these gaps, the authors introduce a novel approach, with the following main contributions:

- KERL, a unified system blending LLMs and Food Knowledge Graphs (FoodKG) for personalized recipe recommendation, recipe generation, and micro-nutritional analysis.
- Modular design: three modules (recommendation, recipe generation, nutritional analysis) implemented as LoRA adapters on the same base LLM (Phi-3-mini).
- Intelligent user query parsing and entity extraction; generates SPARQL queries to extract relevant knowledge from FoodKG, enhancing LLM context.
- Evaluation on new, realistic benchmark datasets with complex, constraint-rich queries; extensive comparison with leading LLMs and KG-based systems.

Regarding its effectiveness, the results demonstrate:

- KERL significantly outperforms state-of-the-art LLMs (Llama-2, Llama-3.1, Mistral, Phi-3) and KG-based recommenders across multiple benchmarks.
- KERL-Recom attains an F1 score of 0.973 on the KGQA benchmark, surpassing competitive LLM and embedding-based baselines by substantial margins (+56 over Phi-3-mini-128K).
- The KERL-Recipe module achieves higher BLEU, ROUGE, METEOR, and CIDEr scores compared to models such as LLaVA-Chef and Phi-3, particularly when ingredient information is included.
- The KERL-Nutri nutrition module produces lower mean absolute error for micro-nutrient estimation than fine-tuned LLaVA-Chef.
- Supports broad query diversity and complexity, accommodating various health tags (vegan, low-carb, gluten-free, etc.).

Building on these findings, the conclusions highlight:

- Knowledge-augmented LLMs offer a superior, more comprehensive, and more personalized framework for food recommendation, recipe generation, and nutritional analysis compared to prior solutions.
- The multi-LoRA adapter setup enables efficient, scalable training and inference within a single unified model.
- Publicly available benchmark datasets and code are released to encourage further research.
- Future directions include integrating chain-of-thought reasoning, ingredient substitution, richer health data, and cultural preferences.
