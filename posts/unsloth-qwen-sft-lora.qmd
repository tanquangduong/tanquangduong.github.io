---
title: "Finetuning Qwen2.5-3B with Unscloth"
subtitle: "Finetuning Qwen2.5-3B with SFT-Lora using Unsloth on TinyStories instruction dataset"
author: "Quang T. Duong"
date: "2024-08-24"
categories: [Finetuning, LORA, Unsloth]
image: "/images/unsloth-qwen-sft-lora/thumbnail.jpg" 
---

<center><img src="/images/unsloth-qwen-sft-lora/thumbnail.jpg"></center>

::: {.column-margin}
Getting started GenAI & LLM with my Udemy course, [**Hands-on Generative AI Engineering with Large Language Model**](https://www.udemy.com/course/hands-on-generative-ai-engineering-with-large-language-model/?referralCode=0775DF5DDD432646AD97&couponCode=OF83024E) üëá
<a href="https://www.udemy.com/course/hands-on-generative-ai-engineering-with-large-language-model/?referralCode=0775DF5DDD432646AD97&couponCode=OF83024E"><img src="/images/genai_course/GenAI-Course-Thumbnail.png" id="llm-course"></a>
:::

## ü§ù What is 

* [Qwen2.5-3B](https://huggingface.co/Qwen/Qwen2.5-3B) as a pretraine language model to be finetuned. The model has 3.09B parameters that are not too big to fine-tune with Colab. This fine-tuning part will be presented in another post.

```yaml

```


```python

```
