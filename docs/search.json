[
  {
    "objectID": "posts/video/2025-05-26_HoPE_Hybrid_Position_Embedding_Length_Generalization_VLMs.html",
    "href": "posts/video/2025-05-26_HoPE_Hybrid_Position_Embedding_Length_Generalization_VLMs.html",
    "title": "HoPE: A Hybrid Approach to Position Embedding for Length Generalization in Vision-Language Models",
    "section": "",
    "text": "Authors: H. Li et al. Published on Arxiv: 2025-05-26 Link: http://arxiv.org/abs/2505.20444v1 Institutions: Carnegie Mellon University • Xiaohongshu Inc. Keywords: Vision-Language Models, Rotary Position Embedding, Hybrid Position Embedding, Multimodal Transformers, Video Understanding, Dynamic Temporal Scaling, Long-Context Modeling, Qwen2, Video Retrieval, Semantic Modeling\n\n\nRecent progress in Vision-Language Models (VLMs) has enabled significant advances in multimodal tasks. However, these models tend to experience marked performance drops when dealing with long-context scenarios, such as the comprehension of lengthy videos. Rotary Position Embedding (RoPE), while effective for large language models, struggles to capture complex spatial-temporal dependencies in such multimodal and video-centric contexts. Moreover, current adaptations of RoPE often employ heuristic frequency allocation without robust theoretical support and face limitations in modeling long-range semantic relationships.\nExpanding on these challenges, the article proposes an innovative solution with new approaches and contributions outlined as follows:\n\nIntroduction of HoPE (Hybrid of Position Embedding) to overcome the limitations of RoPE in vision-language models targeting long-context tasks.\nDevelopment of a hybrid frequency allocation (HFA) strategy, where higher frequencies are assigned to spatial (x, y) positions and the lowest frequencies are set to zero for temporal (t) dimensions, offering theoretical guarantees for modeling semantic relationships across extended contexts.\nProposal of a dynamic temporal scaling (DTS) mechanism that allows adaptive compression or expansion of temporal indices, enhancing the model’s capability to handle variable video speeds and densities of information.\nComprehensive theoretical analysis that demonstrates existing RoPE limitations and justifies the HoPE approach.\nRigorous experimental validation using four video benchmarks (MLVU, LongVideoBench, Video-MME, V-NIAH) and strong VLM backbones (Qwen2-2B/7B-Video), comparing with baseline models such as vanilla RoPE, M-RoPE, and VideoRoPE.\nExtensive ablation studies to evaluate the impact of each component within HoPE.\n\nThese methodological innovations naturally lead to a discussion of achieved results, which are summarized below:\n\nHoPE consistently surpasses vanilla RoPE, M-RoPE, and VideoRoPE across all tested benchmarks and context lengths.\nAchieves a 2–4% improvement in long video understanding tasks, and up to 22.23% enhancement in long video retrieval over baseline models.\nGains from HoPE increase with the size of the backbone model (comparing Qwen2-2B to Qwen2-7B backbones).\nBoth hybrid frequency allocation and dynamic temporal scaling contribute meaningfully to the performance, with their combination yielding the most significant improvements.\nThe dynamic scaling mechanism enhances robustness for varying video lengths and speeds, and introduces flexibility during inference.\n\nBuilding on these results, the article concludes with the following key takeaways:\n\nCurrent multimodal RoPE strategies are suboptimal for long-context modeling in VLMs.\nHoPE’s theoretically motivated hybrid frequency allocation and dynamic temporal scaling enable substantial advances in robustness and length generalization for vision-language applications.\nHoPE achieves superior and scalable performance for long video comprehension and retrieval tasks.\nFuture research directions may explore further scaling to even larger model architectures (such as 13B and 72B parameter backbones)."
  },
  {
    "objectID": "posts/tutorials/unsloth-qwen-dpo.html",
    "href": "posts/tutorials/unsloth-qwen-dpo.html",
    "title": "Fine-Tune Qwen2.5-3B with DPO & Unsloth",
    "section": "",
    "text": "Supervised fine-tuning for large language models (LLMs) enables precise adaptation of a model to specific tasks or domains, significantly improving its performance by providing labeled data tailored to desired outputs. This technique is particularly beneficial in scenarios where off-the-shelf LLMs fail to meet domain-specific or task-specific requirements, such as legal document summarization or medical diagnostics, where accuracy and relevance are critical.\nHowever, supervised fine-tuning alone may not address alignment with human preferences, especially in open-ended or subjective tasks. This is where Reinforcement Learning from Human Feedback (RLHF) techniques come in handy. As they align the model’s outputs with user preferences by incorporating feedback on what users value most, ensuring that the LLM generates responses that are not only accurate but also contextually aligned with human expectations and ethical considerations. These fine-tuning techniques are often named as fine-tuning with preference alignmnent.\nThis post focuses on Direct Preference Optimization (DPO), a fine-tuning technique that aligns LLMs with human preferences by directly optimizing the model’s outputs based on human feedback."
  },
  {
    "objectID": "posts/tutorials/unsloth-qwen-dpo.html#preference-alignmnent-with-dpo",
    "href": "posts/tutorials/unsloth-qwen-dpo.html#preference-alignmnent-with-dpo",
    "title": "Fine-Tune Qwen2.5-3B with DPO & Unsloth",
    "section": "Preference Alignmnent with DPO",
    "text": "Preference Alignmnent with DPO\nDPO is introduced in the paper Direct Preference Optimization: Your Language Model is Secretly a Reward Model by R. Rafailov et al in 2023.\nUnlike Proximal Policy Optimization (PPO), a reinforcement learning algorithm that requires training a separate reward model and iterative sampling, DPO simplifies the process by using a supervised learning framework to adjust the model’s behavior according to ranked human preferences.\nThe principle of DPO involves collecting human preferences on model outputs and using a binary cross-entropy objective to steer the model towards producing desired responses.\nThis method offers several benefits: it simplifies the training process, reduces computational requirements, and potentially leads to faster and more effective alignment with human values. Additionally, it can be more efficient in mitigating the risk of inheriting biases from training data.\nIn the next session, we will work on a use-case where we implement DPO to fine-tune a base LLM for a specific question-answering task that aligns with human preference."
  },
  {
    "objectID": "posts/tutorials/unsloth-qwen-dpo.html#use-case",
    "href": "posts/tutorials/unsloth-qwen-dpo.html#use-case",
    "title": "Fine-Tune Qwen2.5-3B with DPO & Unsloth",
    "section": "Use-case",
    "text": "Use-case\nIn the previous post, Fine-tune Qwen2.5-3B using Lora with Unsloth, we have fine-tuned the base model Qwen2.5-3B on the instruction dataset TinyStories_Instruction using Parameter-Efficient Fine-Tuning (PEFT) technique like LoRA to obtain a story generator for children given a story instruction request.\nContinue this use-case, in this post we will also fine-tune the base model Qwen2.5-3B, but using DPO. For this end, we need to use a preference dataset TinyStories_Preference, that we have created and discussed in the previous post Create Preference Dataset for DPO Fine-Tuning.\nEventually we will compare stories generated by the two above methods to evaluate their performance.\nLet’s jump into the implementation part."
  },
  {
    "objectID": "posts/tutorials/unsloth-qwen-dpo.html#implementation",
    "href": "posts/tutorials/unsloth-qwen-dpo.html#implementation",
    "title": "Fine-Tune Qwen2.5-3B with DPO & Unsloth",
    "section": "Implementation",
    "text": "Implementation\n\nInstall and import required packages\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps xformers trl peft accelerate bitsandbytes\n!pip install -q comet_ml\nfrom unsloth import PatchDPOTrainer\nPatchDPOTrainer()\n\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, TextStreamer\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\nfrom trl import DPOConfig, DPOTrainer\nfrom google.colab import userdata\nimport comet_ml\n\n\nInitialize Comet ML for Experiment Tracking\nSet up Comet ML to log experiments, track training metrics, and monitor performance:\ncomet_ml.login(project_name=\"dpo-lora-unsloth\")\n\n\nLoad Pretrained Model and Tokenizer\nLoad the pretrained Qwen model and tokenizer from Hugging Face. Specify the maximum sequence length and determine whether to load the model in 4-bit precision for efficiency: True means using QLoRA, False means using LoRA.\nmax_seq_length = 2048\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"Qwen/Qwen2.5-3B\",\n    max_seq_length=max_seq_length,\n    load_in_4bit=False,\n    )\n\n\nApply LoRA Adaptation\nLoRA efficiently fine-tunes specific layers, i.e. “q_proj”, “k_proj”, “v_proj”, etc, by introducing additional trainable parameters, significantly reducing memory and computation requirements:\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=32,\n    lora_alpha=32,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n    )\n\n\nDataset Preparation\nFormat the dataset with a specific Alpaca-like template and append the EOS token to chosen and rejected samples. Then split the dataset into training and testing sets:\nalpaca_template = \"\"\"Below is an instruction that describes a task.\nWrite a response that appropriately completes the request.\n### Instruction:\n{}\n### Response:\n\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token\ndef format_samples(example):\n    example[\"prompt\"] = alpaca_template.format(example[\"prompt\"])\n    example[\"chosen\"] = example['chosen'] + EOS_TOKEN\n    example[\"rejected\"] = example['rejected'] + EOS_TOKEN\n    return {\n        \"prompt\": example[\"prompt\"],\n        \"chosen\": example[\"chosen\"],\n        \"rejected\": example[\"rejected\"]\n    }\ndataset = dataset.map(format_samples)\ndataset = dataset.train_test_split(test_size=0.05)\n\n\nTraining Using DPOTrainer\nConfigure the training process with the DPOTrainer class. Define hyperparameters such as learning rate, batch size, gradient accumulation steps, and number of epochs. Enable Comet ML for logging:\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=None,\n    tokenizer=tokenizer,\n    beta=0.5,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    max_length=max_seq_length//2,\n    max_prompt_length=max_seq_length//2,\n    args=DPOConfig(\n        learning_rate=2e-6,\n        lr_scheduler_type=\"linear\",\n        per_device_train_batch_size=2,\n        per_device_eval_batch_size=2,\n        gradient_accumulation_steps=8,\n        num_train_epochs=1,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        warmup_steps=10,\n        output_dir=\"output\",\n        eval_strategy=\"steps\",\n        eval_steps=0.2,\n        logging_steps=1,\n        report_to=\"comet_ml\",\n        seed=0,\n        ),\n)\n\n\ntrainer.train()\n\n\nModel Inference\nWhen the training has finished, let’s test our fine-tuned model. The inference includes formatting the input prompt and using a text streamer for real-time text generation:\nFastLanguageModel.for_inference(model)\nmessage = alpaca_template.format(\"Write a story about a humble little bunny \\\nnamed Ben who follows a mysterious trail in the woods, \\\ndiscovering beautiful flowers, new friends, and a lovely pond along the way.\", \"\")\ninputs = tokenizer([message], return_tensors=\"pt\").to(\"cuda\")\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=2048, use_cache=True)\nThe generated output is as below:\nOnce upon a time, there was a little bunny named Ben. Ben lived in a cozy little house in the woods, surrounded by tall trees and a beautiful meadow. He loved to play and explore, but he always felt a little bit lost and alone.\n\nOne day, Ben decided to take a walk in the woods. He was feeling a little adventurous and wanted to see what he could find. As he walked, he noticed a trail that led deeper into the woods. He followed the trail, and soon he found himself in a beautiful meadow filled with colorful flowers.\n\nAs Ben walked through the meadow, he met a little bird named Lily. Lily was a cheerful bird who loved to sing and dance. She told Ben about all the different flowers she had seen in the meadow, and they spent the rest of the day playing and having fun together.\n\nAfter a while, Ben and Lily decided to take a break and rest in a cozy little clearing. As they sat there, they heard a gentle sound coming from the pond. They followed the sound and found a lovely pond with clear, blue water. They sat by the pond and watched the fish swim around, and they even saw a little frog hop by.\n\nAs the sun began to set, Ben and Lily decided to head back home. They said goodbye to each other and promised to meet again soon. Ben felt a little bit sad, but he knew that he had made some new friends and had discovered some beautiful things in the woods.\n\nFrom that day on, Ben made a habit of exploring the woods every day. He would follow the mysterious trail and discover new things, new friends, and new adventures. And he knew that no matter where he went, he would always have a little bit of magic in his heart.\nLet’s compare it with the story generated by the instructed model:\nOnce upon a time, there was a humble little bunny named Ben. Ben loved to hop around in the meadow, eating carrots and playing with his friends. One day, Ben saw a mysterious trail in the woods. He was curious and wanted to follow it.\n\nBen hopped along the trail, hopping faster and faster. He saw beautiful flowers and new friends along the way. Suddenly, he came to a big pond. He hopped into the water and splashed around. It was so much fun!\n\nBen continued to follow the mysterious trail, hopping and splashing until he couldn't see the end. He was happy to have had such a wonderful adventure. From that day on, Ben knew that he could always follow a mysterious trail and have fun.\n\nIt can be seen that the story generated by preference alignment using DPO is more attractive and having more interesting content than the one generated by supervised fine-tune with LoRA on instruction dataset.\n\n\nSave and Push to Hugging Face Hub\nWhen we are satisfied with our training process, let’s save the fine-tuned model in 16-bit merged format and push it to the Hugging Face Hub for easy sharing and deployment:\nfrom huggingface_hub import login\n# Log in to the Hugging Face Hub\nlogin(token=userdata.get('HF_TOKEN'))\n\nmodel.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\nmodel.push_to_hub_merged(\"tanquangduong/Qwen2.5-3B-DPO-TinyStories\", tokenizer, save_method=\"merged_16bit\")"
  },
  {
    "objectID": "posts/tutorials/unsloth-qwen-dpo.html#conclusion",
    "href": "posts/tutorials/unsloth-qwen-dpo.html#conclusion",
    "title": "Fine-Tune Qwen2.5-3B with DPO & Unsloth",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we have discussed the need for preference alignment in fine-tuning LLMs. While traditional fine-tuning enhances model accuracy for structured tasks, preference alignment techniques like DPO extend the capabilities of LLMs to align with human values and preferences.\nThe implementation provided in this post demonstrates how to apply DPO for fine-tuning the Qwen2.5-3B model using a preference dataset. This workflow enables the model to generate outputs that are not only accurate but also contextually relevant and ethically aligned with user expectations."
  },
  {
    "objectID": "posts/tutorials/instruction-dataset-sft.html",
    "href": "posts/tutorials/instruction-dataset-sft.html",
    "title": "Create Instruction Dataset for Supervised Fine-Tuning",
    "section": "",
    "text": "Creating a tailored instruction dataset for fine-tuning a language model is a critical step in enhancing the model’s capabilities for specialized tasks. This guide provides a step-by-step example of how to create an instruction dataset.\nBefore starting, it is essential to define the dataset’s intended purpose. Are you developing a chatbot, a story generator, or a question-answering system? Clearly understanding the desired model behavior will guide the type and structure of the data you prepare.\nIn this guide, our goal is to create an instruction dataset suitable for fine-tuning a pretrained Large/Small Language Model to produce a story generator dedicated for 5-year-olds."
  },
  {
    "objectID": "posts/tutorials/instruction-dataset-sft.html#use-case",
    "href": "posts/tutorials/instruction-dataset-sft.html#use-case",
    "title": "Create Instruction Dataset for Supervised Fine-Tuning",
    "section": "Use Case",
    "text": "Use Case\nLet’s examine our use-case for this guide.\nThe raw dataset TinyStories, introduced in the paper TinyStories: How Small Can Language Models Be and Still Speak Coherent English? by Ronen Eldan and Yuanzhi Li. This dataset consists of short, synthetically generated stories created by GPT-3.5 and GPT-4 with a limited vocabulary, making it highly suitable for our intended 5-year-old readers. The dataset is divided into two splits: train (2.12M rows) and validation (22K rows). For this use case, we will use the train split with 10K rows.\nBelow is a sample view of the TinyStories dataset on the Hugging Face Dataset Hub: \nTo create the instruction dataset, we will generate synthetic instruction sentences that correspond to each story in the TinyStories dataset."
  },
  {
    "objectID": "posts/tutorials/instruction-dataset-sft.html#implementation",
    "href": "posts/tutorials/instruction-dataset-sft.html#implementation",
    "title": "Create Instruction Dataset for Supervised Fine-Tuning",
    "section": "Implementation",
    "text": "Implementation\n\n1. Load Required Packages\nLet’s begin by loading the necessary packages.\nimport concurrent.futures\nimport json\nimport re\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import List, Tuple\nfrom datasets import Dataset, load_dataset, concatenate_datasets\nfrom openai import OpenAI\nfrom tqdm.auto import tqdm\nfrom google.colab import userdata\n\n\n2. Define Modular Functions\nNext, we define key functions to structure our pipeline.\nExtracting Stories The get_story_list function creates a list of stories from the raw dataset:\ndef get_story_list(dataset):\n    return [example['text'] for example in dataset]\nManaging Instruction-Answer Pairs\nThe InstructionAnswerSet class defines a structure to store and manage instruction-answer pairs, with methods to create instances from JSON and iterate over pairs:\nclass InstructionAnswerSet:\n    def __init__(self, pairs: List[Tuple[str, str]]):\n        self.pairs = pairs\n\n    @classmethod\n    def from_json(cls, json_str: str, story: str) -&gt; 'InstructionAnswerSet':\n        data = json.loads(json_str)\n        pairs = [(data['instruction_answer'], story)]\n        return cls(pairs)\n\n    def __iter__(self):\n        return iter(self.pairs)\nGenerating Instruction-Answer Pairs\nThe generate_instruction_answer_pairs function takes a story and an OpenAI client as inputs to generate instruction-answer pairs using GPT-4. The function crafts a prompt to create relevant instructions while adhering to specific formatting requirements:\ndef generate_instruction_answer_pairs(story: str, client: OpenAI) -&gt; List[Tuple[str, str]]:\n    prompt = f\"\"\"Based on the following story, generate an one-sentence instruction. Instruction \\\n        must ask to write about a content the story.\n        Only use content from the story to generate the instruction. \\\n        Instruction must never explicitly mention a story. \\\n        Instruction must be self-contained and general. \\\n\n        Example story: Once upon a time, there was a little girl named Lily. \\\n        Lily liked to pretend she was a popular princess. She lived in a big castle \\\n        with her best friends, a cat and a dog. One day, while playing in the castle, \\\n        Lily found a big cobweb. The cobweb was in the way of her fun game. \\\n        She wanted to get rid of it, but she was scared of the spider that lived there. \\\n        Lily asked her friends, the cat and the dog, to help her. They all worked together to clean the cobweb. \\\n        The spider was sad, but it found a new home outside. Lily, the cat, and \\\n        the dog were happy they could play without the cobweb in the way. \\\n        And they all lived happily ever after.\n        \n        Example instruction: Write a story about a little girl named Lily who, \\\n        with the help of her cat and dog friends, overcomes her fear of a spider to \\\n        clean a cobweb in their castle, allowing everyone to play happily ever after. \\\n\n        Provide your response in JSON format with the following structure:\n        {{\"instruction_answer\": \"...\"}}\n        Story:\n        {story}\n        \"\"\"\n    completion = client.chat.completions.create(model=\"gpt-4o-mini\",\n                                                messages=[\n                                                    {\"role\": \"system\",\n                                                    \"content\": \"You are a helpful assistant who \\\n                                                    generates instruction based on the given story. \\\n                                                    Provide your response in JSON format.\",},\n                                                    {\"role\": \"user\", \"content\": prompt},\n                                                    ],\n                                                response_format={\"type\": \"json_object\"},\n                                                max_tokens=1200,\n                                                temperature=0.7,)\n    result = InstructionAnswerSet.from_json(completion.choices[0].message.content, story)\n    # Convert to list of tuples\n    return result.pairs\nCreating the Instruction Dataset\nWe now wrap the previous functions into a final function create_instruction_dataset:\ndef create_instruction_dataset(dataset: Dataset, client: OpenAI, num_workers: int = 4) -&gt; Dataset:\n    stories = extract_substory(dataset)\n    instruction_answer_pairs = []\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n        futures = [executor.submit(generate_instruction_answer_pairs, story, client) for story in stories]\n\n        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n            instruction_answer_pairs.extend(future.result())\n\n    instructions, answers = zip(*instruction_answer_pairs)\n    return Dataset.from_dict({\"instruction\": list(instructions), \"output\": list(answers)})\n\n\n3. Orchestrating the Pipeline\nNest, the main function orchestrates the entire pipeline, which includes:\n+ Initialize the OpenAI client\n+ Load the raw TinyStories dataset\n+ Create instruction dataset\n+ Perform train/test split\n+ Push the processed dataset to Hugging Face Hub\ndef main() -&gt; Dataset:\n    # Initializes the OpenAI client\n    client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n\n    # Load the raw data\n    raw_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:10000]\")\n\n    # Create instructiondataset\n    instruction_dataset = create_instruction_dataset(raw_dataset, client)\n\n    # Train/test split and export\n    filtered_dataset = instruction_dataset.train_test_split(test_size=0.1)\n\n    # Push the processed dataset to Hugging Face Hub\n    filtered_dataset.push_to_hub(\"tanquangduong/TinyStories_Instruction\")\n\n\n4. Authenticating and Running the Pipeline\nThen, we authenticate with the Hugging Face Hub for later dataset uploading and execute the pipeline.\nfrom huggingface_hub import login\n# Log in to the Hugging Face Hub\nlogin(token=userdata.get('HF_TOKEN'))\n\n# Launch the pipeline to create instruction dataset\nmain()"
  },
  {
    "objectID": "posts/tutorials/instruction-dataset-sft.html#resulting-instruction-dataset",
    "href": "posts/tutorials/instruction-dataset-sft.html#resulting-instruction-dataset",
    "title": "Create Instruction Dataset for Supervised Fine-Tuning",
    "section": "Resulting Instruction Dataset",
    "text": "Resulting Instruction Dataset\nAfter running the above pipeline, the resulting instruction dataset will look like this:"
  },
  {
    "objectID": "posts/tutorials/instruction-dataset-sft.html#conclusion",
    "href": "posts/tutorials/instruction-dataset-sft.html#conclusion",
    "title": "Create Instruction Dataset for Supervised Fine-Tuning",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, this guide demonstrated the creation of an instruction dataset tailored for fine-tuning. We first defined the purpose of fine-tuning and structured the dataset accordingly. By leveraging GPT-4, we generated instructions for each story using best practices in prompt engineering, including precise instructions, a one-shot example, and a specified output format. Finally, the processed dataset was uploaded to the Hugging Face Hub for future use."
  },
  {
    "objectID": "posts/tutorials/conversational-agent-chatbot.html",
    "href": "posts/tutorials/conversational-agent-chatbot.html",
    "title": "Build Conversational Agentic Chatbot",
    "section": "",
    "text": "Conversational Agentic Chatbots leveraging Large Language Models (LLMs) represent a significant leap in AI-powered communication. These advanced chatbots go beyond simple query responses, actively engaging in complex tasks and decision-making processes.\nThey can interact with databases to answer user questions, schedule appointments, or compose and send emails, to name but a few. Unlike traditional chatbots, these AI agents understand context, adapt to user needs in real time, and can perform actions autonomously.\nThe increasing importance of agentic chatbots in the AI world stems from their ability to provide more personalized, efficient, and human-like interactions.\nSome benefits include reduced workload for human teams, and the capacity to handle complex inquiries with greater accuracy and contextual awareness.\nAs businesses seek to enhance customer engagement and streamline operations, agentic chatbots powered by LLMs are becoming indispensable tools for delivering superior user experiences and driving operational efficiency"
  },
  {
    "objectID": "posts/tutorials/conversational-agent-chatbot.html#conversational-agentic-chatbot",
    "href": "posts/tutorials/conversational-agent-chatbot.html#conversational-agentic-chatbot",
    "title": "Build Conversational Agentic Chatbot",
    "section": "Conversational Agentic Chatbot",
    "text": "Conversational Agentic Chatbot\nIn this guide, we’ll explore the development of a conversational agentic chatbot through a real-world use case.\nOur objective is to create an LLM-powered chatbot capable of interacting with an unstructured database of food images and generating detailed descriptions of dishes presented in these images.\nFor instance:\nHuman: Can you describe images presented famous dishes?\nAI: Please provide the name or details of the image you would like me to describe.\nHuman: I think it is Figure 1\nAI: This image showcases a traditional Vietnamese Bánh Mì, a popular street food sandwich ...\nHuman: Now describe Figure 2\nAI: This dish is a classic Italian spaghetti with tomato sauce, ...\nOur technology stack for this project includes:\n\nLLM model: OpenAI’s GPT-4 or GPT-4-mini\nOrchestration framework: LangGraph & LangChain for creating and orchestrating the conversational agent-based chatbot\nUnstructured database: MongoDB for efficient storage and retrieval of image data\nLLM & prompt monitoring: Opik by Comet ML for performance tracking and artifact management\n\nNow, let’s dive into the exciting implementation phase of our project!"
  },
  {
    "objectID": "posts/tutorials/conversational-agent-chatbot.html#implementation-with-langgraph-openai-mongodb-opik-comet-ml",
    "href": "posts/tutorials/conversational-agent-chatbot.html#implementation-with-langgraph-openai-mongodb-opik-comet-ml",
    "title": "Build Conversational Agentic Chatbot",
    "section": "Implementation with LangGraph, OpenAI, MongoDB, Opik (Comet ML)",
    "text": "Implementation with LangGraph, OpenAI, MongoDB, Opik (Comet ML)\n\nImporting packages & Loading environment variables\nFirst, we import necessary packages.\n# Import required packages\nimport os\nimport json\nimport requests\nimport opik\nfrom opik import track\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.tools import tool\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.prebuilt import create_react_agent\nfrom pymongo import MongoClient\nfrom IPython.display import Image, display\nfrom dotenv import load_dotenv\nNext, we need to load environment variables, such as API keys, to enable the use of various services. For example: OPENAI_API_KEY for accessing OpenAI GPT models, COMET_API_KEY for utilizing Comet ML’s Opik monitoring service, and CONNECTION_STRING for connecting to MongoDB.\n\n\n\nExample of .env file\n\n\nWe use the load_dotenv function from the dotenv package to securely load our confidential API keys.\nload_dotenv()\n\n\nConnect, Add and Query MongoDB\nLet’s first connect with your MongoDB cluster.\n# Get MongoDB Connection String, database name and collection name\nMONGO_CONNECTION_STRING = os.getenv('CONNECTION_STRING')\nFOOD_DATABASE_NAME = os.getenv('FOOD_DATABASE_NAME')\nFOOD_COLLECTION_NAME = os.getenv('FOOD_COLLECTION_NAME')\n\n# Connect to Mongo\nclient = MongoClient(MONGO_CONNECTION_STRING)\nfood_db = client[FOOD_DATABASE_NAME]\nfood_collection = food_db[FOOD_COLLECTION_NAME]\nImagine you have raw data in JSON format containing image names and their corresponding URLs. We load this data into a dictionary.\n\nGet the raw data\ndata_path = './data/food_db.json'\nwith open(data_path, 'r') as file:\n    food_dict = json.load(file)\nprint(food_dict)\nThe raw data is structured as follows:\n{'Figure 1': 'https://drive.usercontent.google.com/download?id=1ODozOIYAjChN_oZoSFxUqGaAlNMeKYya&export=view&authuser=0',\n 'Figure 2': 'https://drive.usercontent.google.com/download?id=1bxNoQ0ORvvA1Ywnijq3mCmlH4kp5Az96&export=view&authuser=0'}\n\n\nAdd item to Mongo DB\nNext, we insert each item from the above data dictionary into our food_photos data collection.\n# Iterate and upload to MongoDB\nfor image_name, image_uri in food_dict.items():\n    food_collection.insert_one({'image_name': image_name, 'image_uri': image_uri})\n\n\n\nFood Photo DB on MongoDB\n\n\n\n\nQuery an item from Mongo DB\nThe following example demonstrates how to retrieve an image URI from our MongoDB collection using an image name, and then display the image.\nimport requests\nfrom IPython.display import Image, display\n\n# Querying image_uri by image_name\nimage_name = \"Figure 1\"\nitem = food_collection.find_one({'image_name': image_name})\nimage_url = item['image_uri']\n\n# Plot image\nresponse = requests.get(image_url)\nimage_data = response.content\ndisplay(Image(data=image_data))\n\n\n\nExample of querying image_uri\n\n\n\n\n\nCreate OpenAI LLM and Opik clients\nIn the next step, we initialize the OpenAI LLM model and the Opik client, which serve as the core AI model and the AI monitoring service, respectively.\n# Initial OpenAI model, i.e. gpt-4o or gpt-4o-mini\nllm = ChatOpenAI(model=\"gpt-4o\")\n\n# Initiate Opik client for LLM and prompt monitoring and management\nopik_client = opik.Opik()\n\n\nPromp tracking and management with Opik (Comet ML)\nPrompt engineering significantly impacts LLM performance. Therefore, a dedicated tool for prompt monitoring, versioning, and management is crucial. This practice is a cornerstone of LLMOps best practices. A centralized platform is essential for streamlining prompt monitoring and management while enabling seamless team collaboration.\nFor this vital task, we utilize Comet ML’s Opik to ensure easy-to-use and effectiveness.\nLet’s create our prompt for the agent task on the Opik platform. Alternatively, you can also create it programmatically. See this tutorial from Opik for more information. \nThe prompt for our agent is as bellow.\nYou are a renowned master chef with expertise in analyzing dishes and crafting exquisite recipes. \n\nUpon receiving a photo of a food item, you will provide a detailed description of the dish, including its key characteristics, ingredients, and cultural context if applicable. \n\nThen, you will create a high-quality, easy-to-follow recipe to prepare this dish, ensuring both flavor and presentation are exceptional.\nWhen you create different prompts for various purposes, the Opik’s Prompt Library page should look like this: \n\n\nPrepare prompt for AI Tool\nAfter creating the agent prompt on the Opik platform, we use the Opik Python SDK to retrieve this prompt directly in our source code. We then integrate it into a HumanMessage template from LangChain, enabling its use in the main function of our agentic tool.\nThe prompt and human message preparation are achieved using the following get_prompt function.\nWe use a decorator for this function:\n\n@track: for Opik to track the function’s inputs and outputs.\n\n@track\ndef get_prompt(prompt_name, image_url):\n    # Get prompt that is created on Opik platform\n    prompt_template = opik_client.get_prompt(name=prompt_name)\n    formatted_prompt_template = prompt_template.format()\n\n    # Create Human message\n    human_message = HumanMessage(\n        content=[\n            {\"type\": \"text\", \"text\": formatted_prompt_template},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n        ],\n    )\n    return human_message\nNext, we create the main function, generate_ip_image_description, for our agentic tool. Following the LangGraph agent convention, we include a docstring that describes the task the agent should perform. This function performs several steps:\n\nSpecify the name of the prompt we want to use.\nQuery the image_url from the MongoDB collection using the image_name.\nCreate a human message using the get_prompt function with two input arguments: prompt_name and image_url.\nInvoke the LLM model with the message to generate the response as the image description.\n\nWe apply two decorators to this function:\n\n@tool from LangChain_Core to define this function as an agentic tool.\n@track for Opik to enable tracking of inputs and outputs.\n\n\n\nCreate main function for agentic tool\n@tool\n@track\ndef generate_ip_image_description(image_name):\n    \"\"\"Generate description for an image about food. \n    Call this whenever you need to provide an image description, \n    for example when a customer asks 'Can you describe the Figure 1?'\"\"\"\n    \n    # prompt_name in created Opik prompt library\n    prompt_name = \"food_image_description\"\n\n    # get image_url from mongodb based on image_name\n    item = food_collection.find_one({'image_name': image_name})\n    image_url = item['image_uri']\n\n    # Get human message\n    message = get_prompt(prompt_name, image_url)\n\n    # Get image description from llm\n    response = llm.invoke([message])\n    return response.content\n\n\nAgent {Tools + Prompt + LLM + Memory}\nOnce the tools, LLM, and prompt are ready, we combine these agent components using the create_react_agent function from LangGraph to build our LLM agent. Importantly, to enable our agent chatbot to handle conversations effectively, we integrate memory by using MemorySaver from LangGraph.\n# Use`generate_ip_image_description` as tools for function calling\ntools = [generate_ip_image_description]\n\n# Global prompt for agent\nprompt = (\n    \"You are a helpful assistant. \"\n    \"You may not need to use tools for every query - the user may just want to chat!\"\n)\n\n# Agent memory\nmemory = MemorySaver()\n\n# Create Agent\nagent = create_react_agent(llm, tools, state_modifier=prompt, checkpointer=memory)"
  },
  {
    "objectID": "posts/tutorials/conversational-agent-chatbot.html#agent-inference",
    "href": "posts/tutorials/conversational-agent-chatbot.html#agent-inference",
    "title": "Build Conversational Agentic Chatbot",
    "section": "Agent Inference",
    "text": "Agent Inference\nNow, let’s move on to the final and exciting step: testing our conversational agentic chatbot.\nBefore testing, we need to create the agent_inference function, which performs two key tasks:\n\nInvoke the agent’s invoke method using the user’s query.\nRetrieve the desired answer, handling two possible scenarios:\n\nIf no ToolMessage is returned, retrieve the AIMessage content to guide the user in providing more information.\nIf a ToolMessage is returned, extract and return its content.\n\n\ndef agent_inference(query):\n    # Response \n    response = agent.invoke(\n        {\"messages\": [HumanMessage(query)]},\n        config={\"configurable\": {\"thread_id\": \"1\"}},\n    )\n    \n    # Get ToolMessage if exists, else get AIMessage\n    if response['messages'][-2].__class__.__name__ != 'ToolMessage':\n        return response['messages'][-1].content # AIMessage content\n    else:\n        return response['messages'][-2].content # ToolMessage content\nNow, let’s test our conversational agentic chatbot with three scenarios:\n\nQA #1: The user asks a general question about describing an image. The chatbot responds by requesting more specific information.\nQA #2: The user specifies the image name (e.g., “Figure 1”), and the chatbot generates a description for this image.\nQA #3: The user asks about a new image (e.g., “Figure 2”), and the chatbot processes the request and generates a description for the new image.\n\n\nQA #1\n# Question-Response 1\nquery = \"Can you describe for me an image?\"\nresponse = agent_inference(query)\nprint(response)\nOf course! Please provide me with the image name or more details, so I can generate a description for you.\n\n\nQA #2\n# Question-Response 2\nquery = \"I think it is Figure 1\"\nresponse = agent_inference(query)\nprint(response)\n\n\n\nFigure 1: Vietnamese Sandwich\n\n\nThis image showcases a Bánh Mì, a popular Vietnamese sandwich known for its delightful combination of flavors and textures. The Bánh Mì is a fusion of French and Vietnamese culinary traditions, originating from the period of French colonial rule in Vietnam. It typically features a crusty baguette filled with a variety of ingredients that create a harmonious balance of savory, spicy, sweet, and tangy flavors.\n\n### Key Characteristics:\n- **Bread:** A light, crispy baguette with a soft interior.\n- **Protein:** Often includes grilled or roasted meats such as pork or chicken.\n- **Vegetables:** Fresh cucumber slices, pickled carrots, and daikon radish.\n- **Herbs:** Fresh cilantro adds a fragrant note.\n- **Condiments:** Mayonnaise, pâté, and a hint of spicy chili.\n\n### Recipe for a Classic Bánh Mì\n\n#### Ingredients:\n- **Baguette:**\n    - 1 fresh baguette\n\n- **Protein:**\n    - 200g pork tenderloin or chicken breast\n    - 2 tablespoons soy sauce\n    - 1 tablespoon fish sauce\n    - 1 tablespoon sugar\n    - 1 garlic clove, minced\n    - 1 tablespoon vegetable oil\n\n- **Pickled Vegetables:**\n    - 1 carrot, julienned\n    - 1 daikon radish, julienned\n    - 1/4 cup white vinegar\n    - 1/4 cup sugar\n    - 1/2 cup water\n    - Pinch of salt\n\n- **Additional Toppings:**\n    - 1 cucumber, thinly sliced\n    - Fresh cilantro leaves\n    - 2 tablespoons mayonnaise\n    - Fresh chili slices (optional)\n\n#### Instructions:\n\n1. **Marinate the Protein:**\n    - In a bowl, combine soy sauce, fish sauce, sugar, garlic, and oil. Add the pork or chicken, ensuring it's well-coated. Marinate for at least 30 minutes.\n\n2. **Prepare Pickled Vegetables:**\n    - In a small saucepan, combine vinegar, sugar, water, and salt. Heat until sugar dissolves, then let cool.\n    - Place carrot and daikon in a jar, pour the pickling liquid over, and let sit for at least 30 minutes.\n\n3. **Cook the Protein:**\n    - Heat a grill pan or skillet over medium-high heat. Cook the marinated meat until it's cooked through and slightly caramelized. Let rest, then slice thinly.\n\n4. **Assemble the Bánh Mì:**\n    - Slice the baguette lengthwise, spread mayonnaise on both sides.\n    - Layer the cooked protein, pickled vegetables, cucumber slices, cilantro, and chili (if using) inside the baguette.\n\n5. **Serve:**\n    - Press the sandwich slightly to hold it together, and serve immediately to enjoy the contrasting textures and flavors.\n\nThis Bánh Mì recipe offers a delightful balance of flavors and can be customized with your preferred proteins and additional toppings. Enjoy this Vietnamese classic as a flavorful and satisfying meal.\n\n\nQA #3\n# Question-Response 3\nquery = \"Now describe Figure 2\"\nresponse = agent_inference(query)\nprint(response)\n\n\n\nFigure 2: Italian Spaghetti\n\n\nThis dish is a classic Italian Spaghetti al Pomodoro. It features a beautifully arranged mound of spaghetti topped with a rich tomato sauce, adorned with fresh basil leaves and a sprinkle of grated Parmesan cheese. The vibrant red of the sauce contrasts with the golden pasta, and the garnishing adds a touch of green, making it visually appealing.\n\n### Key Characteristics and Cultural Context:\n- **Main Ingredients**: Spaghetti, tomatoes, basil, Parmesan cheese.\n- **Cultural Context**: Spaghetti al Pomodoro is a staple in Italian cuisine, celebrated for its simplicity and fresh ingredients. It embodies the Italian philosophy of letting high-quality ingredients shine without overwhelming them.\n\n### Recipe for Spaghetti al Pomodoro\n\n#### Ingredients:\n- 400g spaghetti\n- 800g ripe tomatoes (or canned whole tomatoes)\n- 3 tablespoons olive oil\n- 3 cloves garlic, minced\n- Salt and freshly ground black pepper to taste\n- A pinch of red pepper flakes (optional)\n- Fresh basil leaves\n- 100g Parmesan cheese, grated\n\n#### Instructions:\n\n1. **Prepare the Tomatoes:**\n    - If using fresh tomatoes, blanch them in boiling water for 30 seconds, then transfer to ice water. Peel and chop them.\n\n2. **Cook the Pasta:**\n    - Bring a large pot of salted water to a boil. Add the spaghetti and cook according to package instructions until al dente. Reserve 1 cup of pasta water, then drain the pasta.\n\n3. **Make the Sauce:**\n    - In a large skillet, heat the olive oil over medium heat. Add the minced garlic and sauté until fragrant, about 1 minute.\n    - Add the tomatoes (fresh or canned) and crush them with a spoon. Season with salt, black pepper, and red pepper flakes if using.\n    - Simmer the sauce for about 15 minutes, stirring occasionally. Adjust the seasoning as needed.\n\n4. **Combine Pasta and Sauce:**\n    - Add the cooked spaghetti to the sauce. Toss to coat the pasta, adding reserved pasta water if the sauce is too thick.\n\n5. **Serve:**\n    - Divide the pasta among serving plates. Top with grated Parmesan cheese and fresh basil leaves.\n    - Drizzle a little olive oil over the top for extra flavor.\n\n### Presentation Tips:\n- Use a fork to twist the pasta into nests on each plate.\n- Place basil leaves strategically for a pop of color.\n- Serve with extra Parmesan cheese on the side.\n\nEnjoy your homemade Spaghetti al Pomodoro with a glass of red wine for an authentic Italian dining experience!\nGreat! Amazing! Our conversational agentic chatbot has successfully provided descriptions for each photo we requested!"
  },
  {
    "objectID": "posts/tutorials/conversational-agent-chatbot.html#monitoring-qa-prompt-execution-time",
    "href": "posts/tutorials/conversational-agent-chatbot.html#monitoring-qa-prompt-execution-time",
    "title": "Build Conversational Agentic Chatbot",
    "section": "Monitoring QA, Prompt, Execution Time",
    "text": "Monitoring QA, Prompt, Execution Time\nAs discussed earlier, monitoring prompts and the LLM’s inputs and outputs is crucial for enhancing traceability, which helps us analyze the performance of the LLM agent effectively.\nOpik platform ensures tracking for each query, including:\n\nThe prompt used.\nInputs and outputs for each function, sub-function, and agent tool.\nExecution time for each step and function.\n\n\nMonitoring QA #1\n\n\n\nQA tracking #1 on Opik platform\n\n\n\n\nMonitoring QA #2\n\n\n\nQA tracking #2 on Opik platform\n\n\n\n\nMonitoring QA #3\n\n\n\nQA tracking #3 on Opik platform\n\n\nWhen numerous experiments are conducted, analyzing these tracked data points can help us optimize the system effectively."
  },
  {
    "objectID": "posts/tutorials/conversational-agent-chatbot.html#conclusion",
    "href": "posts/tutorials/conversational-agent-chatbot.html#conclusion",
    "title": "Build Conversational Agentic Chatbot",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, Conversational Agentic Chatbots leveraging Large Language Models (LLMs) are innovating AI-driven interactions by combining contextual understanding, adaptability, and autonomous decision-making.\nAs demonstrated in the development of a food image description chatbot, these systems can seamlessly integrate with unstructured databases like MongoDB, advanced LLM like OpenAI GPT-4o, advanced orchestration frameworks like LangGraph, and monitoring tools like Comet ML’s Opik to deliver highly personalized and context-aware user experiences.\nBy enabling enhanced customer engagement, streamlined operations, and creative problem-solving, agentic chatbots are transforming industries and setting new standards for intelligent communication tools. As businesses increasingly adopt this cutting-edge technology, the potential for innovation and efficiency in human-AI collaboration continues to grow."
  },
  {
    "objectID": "posts/tutorials/conversational-agent-chatbot.html#reference",
    "href": "posts/tutorials/conversational-agent-chatbot.html#reference",
    "title": "Build Conversational Agentic Chatbot",
    "section": "Reference",
    "text": "Reference\n[1] https://cookbook.openai.com/examples/how_to_use_guardrails\n[2] https://medium.com/velotio-perspectives/an-introduction-to-asynchronous-programming-in-python-af0189a88bbb\n[3] https://realpython.com/solid-principles-python/"
  },
  {
    "objectID": "posts/tabular/2025-05-27_MMTBENCH_multimodal_table_reasoning.html",
    "href": "posts/tabular/2025-05-27_MMTBENCH_multimodal_table_reasoning.html",
    "title": "MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning",
    "section": "",
    "text": "Authors: P. Y. Titiya et al. Published on Arxiv: 2025-05-27 Link: http://arxiv.org/abs/2505.21771v1 Institutions: Arizona State University Keywords: multimodal tables, question answering, vision-language models, large language models, benchmark dataset, table reasoning, visual reasoning, machine learning evaluation, real-world data, image-text integration\n\n\nMultimodal tables—combining structured data with visual elements like charts and maps—are increasingly prevalent in real-world domains, yet they pose ongoing challenges for current Vision–Language Models (VLMs) and Large Language Models (LLMs). Existing research has largely concentrated on text-only tables, leaving complex multimodal table reasoning underexplored and lacking robust benchmarks that reflect real-world complexity.\nTo address these gaps and introduce a comprehensive evaluation tool, the authors propose an innovative approach rooted in the following main contributions:\n\nIntroduction of MMTBENCH, a benchmark dataset compiled from 500 real-world multimodal tables, annotated with 4,021 diverse question–answer pairs.\nCoverage of eight multimodal table types with integrated visual elements, including charts, maps, images, and hierarchical structures.\nQuestions designed to span mathematical, logical, fact-verification, and visual reasoning, capturing both direct and multi-step inference needs.\nSystematic benchmarking of state-of-the-art open and closed-source models (e.g., Google Gemini, GPT-4o Mini, LLaMA 3, Mixtral-8x7B), across five experimental baselines reflecting different accesses to textual and visual information.\n\nBuilding on this approach, the authors report empirical results that highlight current model capabilities and limitations:\n\nSubstantial performance gaps are observed, particularly for questions requiring advanced visual-based reasoning and multi-step inference.\nThe Entity Replaced Baseline—where tables are fully textualized—yields the highest model performance, while the Missing Image and Image Captioning baselines lag behind significantly.\nClosed-source models generally lead in vision-based tasks, but select open-source models (for instance, Qwen 2.5 VL-7b-Instruct) demonstrate competitive results in some settings.\nAll models show notable struggles with complex, implicit visual reasoning and tasks requiring image interpretation within tables.\nHuman evaluators consistently outperform AI models but still encounter challenges, especially with subtle visual cues and domain-specific knowledge demands.\n\nReflecting on these findings, the article draws significant conclusions and outlines future directions for the field:\n\nMMTBENCH provides a rigorous new benchmark, exposing the substantial limitations of current LLMs and VLMs in multimodal table reasoning.\nThere exists a critical research gap in achieving deep visual–textual integration for practical multimodal data analysis.\nThe benchmark’s complexity and scope open new avenues of model development and evaluation, underlining the need for improved architectures.\nFuture work includes dataset expansion and testing of larger models, while also acknowledging potential societal risks stemming from model misinterpretation in real-world applications."
  },
  {
    "objectID": "posts/rag/2025-05-25-hypercube-rag-scientific-qa.html",
    "href": "posts/rag/2025-05-25-hypercube-rag-scientific-qa.html",
    "title": "Hypercube-RAG: A Hypercube-Based RAG for Efficient and Explainable Scientific QA",
    "section": "",
    "text": "Authors:  J. Shi et al. Published on Arxiv:  2025-05-25 Link:  http://arxiv.org/abs/2505.19288v1 Institutions: Florida International University • University of Illinois Urbana-Champaign Keywords: Retrieval-Augmented Generation, Hypercube, Scientific Question Answering, Sparse Retrieval, Dense Embedding, Explainability, Named Entity Recognition, In-domain QA, Theme-specific Retrieval, Efficiency, BM25, GraphRAG, KeyBERT\n\n\nLarge language models (LLMs) face limitations in scientific question answering due to hallucinations and factual inaccuracies. To tackle domain-specific QA tasks, retrieval-augmented generation (RAG) methods are commonly applied, but existing RAGs struggle with accuracy, efficiency, or explainability, especially in knowledge-intensive scientific applications.\nBuilding upon these challenges, the authors introduce a new approach that aims to enhance scientific QA with precise and interpretable retrieval methods:\n\nThe proposed framework, Hypercube-RAG, organizes the document corpus into a multidimensional hypercube structure using human-defined dimensions such as location, date, event, person, organization, and theme.\nNamed entity recognition (NER) and key phrase extraction (KeyBERT), with optional human-in-the-loop curation, are employed to populate each dimension of the hypercube with fine-grained document labels.\nAt inference time, the LLM decomposes a user’s question into entities and topics matching the hypercube’s dimensions for exact, sparse retrieval, with fallback to dense semantic matching when necessary.\nRetrieved documents are ranked by coverage of all key query components, favoring those with higher alignment.\nBenchmarking experiments are conducted on three in-domain QA datasets (Hurricane, Geography, Aging Dam) against state-of-the-art baselines: BM25, modern dense retrievers (Contriever, e5, NV-Embed), and graph-based RAGs (GraphRAG, LightRAG, HippoRAG).\n\nAfter introducing their methodology, the authors present their results, highlighting the advantages of Hypercube-RAG over traditional alternatives:\n\nHypercube-RAG consistently outperforms all baselines on the three datasets, achieving a 3.7% accuracy improvement and up to 81.2% higher retrieval efficiency compared to the strongest RAG baseline.\nThe approach uniquely combines high accuracy, efficiency, and explainability by enabling direct traceability from retrieved documents to their hypercube labels.\nFor large document corpora, retrieval time is one to two orders of magnitude lower than that of graph-based or dense methods, retaining efficiency as corpus size grows and demonstrating robustness to noisy data.\nAblation analyses confirm that each hypercube dimension and the combined retrieval/ranking strategies positively contribute to performance gains.\n\nBuilding on these empirical findings, the article concludes with reflections and future directions:\n\nHypercube-RAG establishes an efficient, accurate, and inherently explainable approach for in-domain scientific question answering by leveraging human-defined, multidimensional retrieval.\nThe framework surpasses current baselines in both accuracy and speed, offering robust performance even on noisy datasets and transparent, interpretable results.\nA current limitation is the need for domain experts to manually define hypercube dimensions; future work will focus on automating this process with large language models."
  },
  {
    "objectID": "posts/prompt-engineering/2025-05-26-prompt-optimization-nl2sql.html",
    "href": "posts/prompt-engineering/2025-05-26-prompt-optimization-nl2sql.html",
    "title": "Effectiveness of Prompt Optimization in NL2SQL Systems",
    "section": "",
    "text": "Authors:  S. Gurajada et al. Published on Arxiv:  2025-05-26 Link:  http://arxiv.org/abs/2505.20591v1 Institutions: Megagon Labs • Adobe Keywords: NL2SQL, large language models, prompt optimization, in-context learning, multi-objective optimization, schema pruning, SQL generation, BIRD dataset, GPT-4o, query latency\n\n\n\nNatural Language to SQL (NL2SQL) systems have leveraged large language models (LLMs) to enable translation of natural language queries into SQL statements for domain-specific databases. Traditionally, in-context learning (ICL) methods guide LLMs using carefully selected schema, cell values, and exemplars—typically chosen through retrieval-based techniques that add inference-time overhead. Prior work has focused on SQL generation quality, often overlooking efficiency and suitability for production deployment.\nTo address these efficiency challenges, the authors introduce their proposed solution:\n\nThey propose a prompt optimization framework for NL2SQL that statically selects a representative set of exemplars and instructions, jointly optimizing for both accuracy and efficiency using multi-objective techniques.\nThe introduced Iterative Prompt Optimization (IPO) is a bi-agent LLM system (Proposer, SQL Generator) that iteratively refines instructions and exemplars based on performance feedback, promoting schema pruning and concise prompts.\nBenchmarking is performed using the new BIRD-MULTI dataset, which extends BIRD with query execution latency annotations, supporting optimization for both SQL accuracy and execution efficiency.\n\nFollowing this approach, the authors evaluate and present their main experimental results:\n\nIPO, using GPT-4o, consistently outperforms baselines such as Random Exemplar Selection (RES), Optimized Random Exemplar Selection (ORES), and MIPROv2 on the BIRD dataset, particularly excelling with complex queries.\nThrough iterative schema pruning, IPO generates the shortest prompts (around 6.5k tokens) while maintaining comparable optimization times to other methods.\nUsing a multi-objective optimization strategy, IPO increases SQL execution efficiency: optimizing for both accuracy and latency results in reduced maximum and average query latencies, while maintaining execution accuracy (58.98% with latency optimization vs 59.24% for accuracy-only on BIRD dev).\n\nFinally, the paper draws several key conclusions from the findings:\n\nJoint selection of instructions and exemplars through prompt optimization leads to improvements in both accuracy and efficiency for NL2SQL systems.\nIterative prompt optimization not only boosts SQL generation performance but also facilitates concise prompt creation via schema pruning without sacrificing accuracy.\nMulti-objective optimization is vital for deploying NL2SQL systems in real-world settings, as it enables generation of efficient, accurate queries; the BIRD-MULTI benchmark is a meaningful contribution supporting this advancement."
  },
  {
    "objectID": "posts/ocr/2025-05-31-OntoRAG-Automated-Ontology-QA.html",
    "href": "posts/ocr/2025-05-31-OntoRAG-Automated-Ontology-QA.html",
    "title": "OntoRAG: Automating Ontology Derivation for Enhanced Question-Answering",
    "section": "",
    "text": "Authors: Y. Tiwari et al. Published on Arxiv: 2025-05-31 Link: http://arxiv.org/abs/2506.00664v1 Institutions: ABB Ability Innovation Center, Bangalore, India • ABB Ability Innovation Center, Hyderabad, India • Indian Institute of Technology, Kharagpur, India Keywords: LLM, Ontology Learning, Retrieval-Augmented Generation, Knowledge Graph, GraphRAG, Leiden algorithm, Semantic Web, Question Answering, PDF Parsing, Information Extraction\n\n\nOntologies are crucial for structuring knowledge bases to improve the performance of question-answering (QA) systems using Large Language Models. However, the manual creation of ontologies is a significant bottleneck, especially in large or rapidly evolving technical domains.\nTo address these challenges, the authors introduce their solution and methodology as follows:\n\nOntoRAG is an automated pipeline designed to extract ontologies from unstructured data, specifically electrical relay documents in PDF format.\nThe pipeline encompasses components such as web scraping, advanced PDF parsing, hybrid and semantic chunking, information extraction—including Named Entity Recognition and atomic fact detection—knowledge graph construction, and automated ontology generation.\nCommunity detection via the Leiden algorithm and LLM-assisted property synthesis (utilizing Gemini 2.5 Flash) ensure semantic coherence and ontological integrity.\nThe method enables multi-hop reasoning and structured retrieval capabilities with the created ontology.\nExperiments compare OntoRAG against baseline vector-based Retrieval-Augmented Generation (semantic search) and GraphRAG, employing a proprietary dataset with 1 million tokens from technical documentation on electrical relays.\nEvaluation involves both qualitative and claim-based quantitative metrics: comprehensiveness, diversity, empowerment, and directness.\n\nFollowing the description of OntoRAG’s methodology, the results further underscore its efficacy:\n\nOntoRAG delivers comprehensiveness win rates of 88% over semantic search and 65% over the best GraphRAG configuration, along with diversity win rates of 86% and 62%, respectively.\nClaim-based analysis reveals OntoRAG generates more unique claims and greater diversity (35.2 claims, 14.1 clusters) compared to GraphRAG (34.8 claims, 13.8 clusters) and semantic search (26.5 claims, 8.0 clusters).\nIt consistently surpasses GraphRAG at all ontology granularities regarding comprehensiveness and diversity.\nEmpowerment scores are similar between OntoRAG and GraphRAG, with semantic search being strongest in directness.\nThe computational cost for creating ontologies is higher for OntoRAG (300 minutes for 1M tokens) than for GraphRAG (281 minutes), presenting scalability challenges.\n\nTo complete the discussion, the authors offer their main conclusions:\n\nOntoRAG enables major advances in fully automated ontology creation from unstructured sources, notably enhancing question-answering performance in terms of comprehensiveness and diversity.\nBy removing the dependency on manual curation and maintaining ontological integrity, it promotes effective knowledge organization in complex technical fields.\nCurrent limitations relate to computational intensity and the use of domain-specific prompts, which affect scalability and applicability to new fields.\nFuture research will aim to optimize computational efficiency, scale up to larger datasets, and generalize the framework to other domains using adaptive prompts."
  },
  {
    "objectID": "posts/llm/2025-05-26_FP8-GEMM-LLM-FOG.html",
    "href": "posts/llm/2025-05-26_FP8-GEMM-LLM-FOG.html",
    "title": "Towards Fully FP8 GEMM LLM Training at Scale",
    "section": "",
    "text": "Authors:  A. Hernández-Cano et al. Published on Arxiv:  2025-05-26 Link:  http://arxiv.org/abs/2505.20524v1 Institutions: EPFL • ETHZ Keywords: FP8, GEMM, Large Language Models, LLM training, Transformers, FP8DPA, FOG architecture, Delayed scaling, Kurtosis, Outlier mitigation, Deep learning efficiency, Language modeling, Benchmarking, FineWeb-Edu, BF16, Transformer Engine, Megatron-LM\n\n\nEfficient training of large language models (LLMs) demands significant computational resources, prompting growing interest in the use of lower-precision numerical formats like FP8 to accelerate training and reduce resource requirements. Recent adoption of FP8, however, has been hampered by training instabilities, largely due to FP8’s narrow dynamic range and outlier activations encountered in LLMs. Most existing FP8 training methods limit FP8 application to certain components, relying on higher-precision formats for critical matrix multiplications, particularly in attention mechanisms, which restricts the full potential of throughput gains from FP8.\nTo address these challenges, the authors present their proposed solution, approach, and the main contributions:\n\nIntroduction of FOG (fast and outlier-guarded) architectures, a set of transformer-based LLM designs that enable stable and efficient FP8 computation for all GEMMs, including those within the attention mechanism.\nIncorporation of architectural modifications such as removal of pre-normalization, inclusion of entropy-regularization in attention using non-trainable RMSNorm or tanh activation, input activation scaling, and post-normalization before residual connections to manage outlier activations and enhance robustness to FP8 precision limitations.\nUtilization of low-overhead delayed scaling for FP8 casting, use of kurtosis as a metric to monitor outlier activations, and application of early diagnostic techniques to predict and counteract training instabilities.\nExperimental comparison involving models of 390M, 1.5B, and 8B parameters trained on the FineWeb-Edu dataset, benchmarking against state-of-the-art baselines such as Llama3, OLMo2, and SmoothSwiGLU, and assessment using standard LLM downstream tasks.\n\nHaving defined the approach and innovations, the results of the study are presented as follows:\n\nFOG architectures enable all GEMMs in the transformer block, including attention, to be performed in FP8, leading to throughput improvements of up to 40% at the 8B parameter scale compared to BF16 baselines, while retaining or surpassing performance on downstream tasks like HellaSwag, ARC, PIQA, and others.\nStable and robust training is achieved across prolonged token counts (up to 420B tokens for 1.5B models), with FOG significantly outperforming prior architectures that tend to diverge when trained completely with FP8 in attention layers.\nMonitoring with kurtosis effectively forecasts the appearance of outlier activations and alerts to possible divergences earlier than traditional indicators such as loss curves or gradient norms, providing a reliable early-warning system for maintaining stability during FP8 training.\nAll FOG model variants match or exceed the accuracy of baseline BF16 models across benchmarks, validating the practicality and efficiency of the proposed approach at scale.\n\nUpon reviewing these results, the study concludes with the following key takeaways:\n\nThis work demonstrates, for the first time, that it is feasible to achieve stable, high-performance LLM training with full FP8 computations, including in attention mechanisms, at scale—showing up to 40% improvement in throughput and equal or superior downstream effectiveness compared to higher-precision baselines.\nThe proposed FP8 training framework enables early and reliable prediction of long-term training instabilities using kurtosis analysis, which helps reduce experimental costs and mitigates training risks.\nThe approach paves the way for extending full FP8 GEMM training, even in output heads, although future work will need to validate these findings at scales beyond 8B parameters and further explore FP8’s role in optimizer state representation and training."
  },
  {
    "objectID": "posts/asr/2025-05-25_LALM_Reliability.html",
    "href": "posts/asr/2025-05-25_LALM_Reliability.html",
    "title": "Towards Reliable Large Audio Language Model",
    "section": "",
    "text": "Authors:  Z. Ma et al. Published on Arxiv:  2025-05-25 Link:  http://arxiv.org/abs/2505.19294v1 Institutions: X-LANCE Lab, School of Computer Science, MoE Key Lab of Artificial Intelligence, Shanghai Jiao Tong University • ByteDance • Shanghai Innovation Institute Keywords: Large Audio Language Model, LALM, Reliability, Reliability Gain Index, RGI, Supervised Fine-Tuning, SFT, LoRA, Qwen2-Audio, IDK Prompting, Multi-Modal Chain-of-Thought, MCoT, Audio Understanding, Speech, Music, Sound, Trustworthiness, Humbleness, Truthfulness, Conservativeness, Transfer Learning, Benchmark, Evaluation Metrics\n\n\nLarge Audio Language Models (LALMs) have recently emerged as universal tools for audio understanding and reasoning across modalities including speech, music, and general sound. Despite notable advancements, these models exhibit a critical weakness: they are unable to recognize their own knowledge boundaries and fail to refuse questions that exceed their capabilities. This shortcoming poses threats to reliability in practical scenarios such as healthcare, autonomous driving, and interactive agents. Moreover, conventional evaluation metrics do not adequately assess the effectiveness of LALMs in striking the balance between providing answers (being helpful) and refraining from guessing (demonstrating humbleness).\nTo address these challenges, the authors propose and systematically evaluate solutions that enhance the reliability of LALMs:\n\nSystematic investigation into LALM reliability through both training-free and training-based approaches.\nTraining-free methods include IDK (I Don’t Know) prompting, Multi-Modal Chain-of-Thought (MCoT) prompting, and Task Agent strategies, all designed to encourage the model to admit uncertainty without requiring additional training.\nA training-based strategy using Supervised Fine-Tuning (SFT) on custom IDK datasets teaches the model to respond ‘I don’t know’ when unsure, thereby directly improving reliability.\nIntroduction of the Reliability Gain Index (RGI), a novel metric gauging the improvement in humbleness versus conservativeness, to accurately assess model reliability.\nExperimental evaluations primarily focus on the Qwen2-Audio-7B-Instruct model and the MMAU benchmark, and results are compared with leading open-source LALMs like SALMONN and Qwen-Audio-Chat across a variety of quantitative metrics.\n\nThe authors validate their approach with detailed experiments, and the results reveal several important findings:\n\nBoth training-free and training-based methods contribute to enhanced LALM reliability, with supervised fine-tuning delivering the best trade-off between helpfulness and truthfulness.\nThe newly introduced RGI metric effectively distinguishes genuine reliability improvements from excessive conservativeness.\nReliability awareness emerges as a transferable skill or ‘meta ability,’ with positive results observed even when training and testing across different audio modalities such as sound, speech, and music.\nBenchmarks using the MMAU dataset confirm that Qwen2-Audio-7B-Instruct, especially with reliability-focused methods, outperforms or matches other state-of-the-art open-source LALMs across accuracy, truthfulness, reliability, and RGI metrics.\n\nIn conclusion, the study makes significant contributions by initiating the exploration of reliability in large audio language models and offering effective solutions and metrics for evaluation:\n\nThis is the first systematic exploration of reliability in LALMs, providing both new methodological solutions and a new evaluation metric (RGI).\nReliability awareness, particularly the capacity to answer with ‘I don’t know,’ is demonstrated to be transferable between modalities despite structural differences.\nFuture directions include extending reliability transfer to cross even broader modality gaps—such as from audio to video—as well as developing models capable of interactive justifications or clarification requests when uncertain."
  },
  {
    "objectID": "page_video.html",
    "href": "page_video.html",
    "title": "AI Video",
    "section": "",
    "text": "HoPE: A Hybrid Approach to Position Embedding for Length Generalization in Vision-Language Models\n\n\nTowards Robust Long-Context Understanding and Retrieval in Video-Comprehension Tasks\n\n\n\n\n\nMay 26, 2025\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "AI Vision",
      "AI Video"
    ]
  },
  {
    "objectID": "page_tabular.html",
    "href": "page_tabular.html",
    "title": "AI for Tabular Data",
    "section": "",
    "text": "MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning\n\n\nAdvancing Evaluation of Vision–Language Models with Real-World Multimodal Tables\n\n\n\n\n\nMay 27, 2025\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "AI Tabular",
      "AI for Tabular Data"
    ]
  },
  {
    "objectID": "page_rag.html",
    "href": "page_rag.html",
    "title": "RAG/Multimodal RAG",
    "section": "",
    "text": "Hypercube-RAG: A Hypercube-Based RAG for Efficient and Explainable Scientific QA\n\n\nA novel multidimensional retrieval-augmented framework for domain-specific question answering\n\n\n\n\n\nMay 25, 2025\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "AI Apps",
      "RAG/Multimodal RAG"
    ]
  },
  {
    "objectID": "page_post-training.html",
    "href": "page_post-training.html",
    "title": "LLM Post-training",
    "section": "",
    "text": "LlamaRL: A Distributed Asynchronous RL Framework for Efficient Large-Scale LLM Training\n\n\nScaling Reinforcement Learning for Today’s Largest Language Models\n\n\n\n\n\nMay 29, 2025\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "AI Text",
      "LLM Post-training"
    ]
  },
  {
    "objectID": "page_edge.html",
    "href": "page_edge.html",
    "title": "AI on the Edge",
    "section": "",
    "text": "FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression\n\n\nA Fast, Training-free Approach to Efficient LLM Deployment\n\n\n\n\n\nMay 29, 2025\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "AI Physics",
      "AI on the Edge"
    ]
  },
  {
    "objectID": "page_agent.html",
    "href": "page_agent.html",
    "title": "Agents/Multi-Agent Systems",
    "section": "",
    "text": "GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling\n\n\nA Model-Agnostic Framework for Detecting Anomalies and Errors in Multi-Agent Language Model Systems\n\n\n\n\n\nMay 25, 2025\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "AI Apps",
      "Agents/Multi-Agent Systems"
    ]
  },
  {
    "objectID": "MD.html",
    "href": "MD.html",
    "title": "Quang Duong",
    "section": "",
    "text": "Random Unsplash-style image\n\n\n\n\n\nRandom piscum photos"
  },
  {
    "objectID": "draft/design_pattern_ai_system_production.html",
    "href": "draft/design_pattern_ai_system_production.html",
    "title": "Quang Duong",
    "section": "",
    "text": "When building an LLM-based agentic chatbot for production, several design patterns and elements should be considered: Guardrails: Implement safety controls to monitor and dictate user interactions with the LLM application. This ensures the AI model operates within defined principles and organizational guidelines2. Multi-Agent Systems (MAS): Utilize a system of specialized LLM agents for different tasks, such as a reflector, document checker, web searcher, critic, and coder. This approach can improve performance and accuracy compared to a single chatbot3. Agentic Design Patterns: Supervision: Use a Supervisor (Router) agent to manage Worker/Specialist agents4. Reflection: Prompt the LLM to critique its past actions for improvement4. Collaboration: Enable agents to share common memories and work together as specialists4. Agent Components: Planner: Creates step-by-step plans by decomposing complex tasks4. Memory: Stores conversation history and learned context4. Tools: Enables the agent to call external APIs or functions4. Specialization: Focus on building specialized agents for specific tasks rather than general-purpose ones. This approach often leads to better performance and reliability4. Evaluation Pipeline: Implement a proper evaluation system with clearly defined goals to assess and improve agent performance4. Rate Limiting: As you mentioned, implement rate limiting to manage API calls and prevent overuse or abuse of the system. Composability: Design agents as functions that can be combined and reused in various configurations5. Error Handling and Fallbacks: Implement robust error handling mechanisms and fallback options for when the LLM fails to provide satisfactory responses. Scalability: Design the system to handle increasing loads and user interactions efficiently. Monitoring and Logging: Implement comprehensive monitoring and logging systems to track performance, errors, and user interactions. Privacy and Security: Ensure proper data handling, encryption, and compliance with relevant regulations. By considering these design patterns and elements, you can create a more robust, efficient, and reliable LLM-based agentic chatbot for production use."
  },
  {
    "objectID": "draft/course.html",
    "href": "draft/course.html",
    "title": "Hands-On Generative AI Engineering with Large Language Model",
    "section": "",
    "text": "I am thrilled to introduce my Udemy course Hands-On Generative AI Engineering with Large Language Model.\nThis course equips you with a wide range of tools, frameworks, and techniques to create your GenAI applications using Large Language Models, including Python, PyTorch, LangChain, LlamaIndex, Hugging Face, FAISS, Chroma, Tavily, Streamlit, Gradio, FastAPI, Docker, and more.\nThis hands-on course covers essential topics such as implementing Transformers from scratch, fine-tuning Transformer models for downstream tasks, prompt engineering, vector embeddings, vector stores, and creating cutting-edge AI applications like AI Assistants, Chatbots, Retrieval-Augmented Generation (RAG) systems, autonomous agents, and serving your GenAI applications from scratch using REST APIs and Docker containerization.\nBy the end of this course, you will have the practical skills and theoretical knowledge needed to develop and serving your own LLM-based applications.\nIf you are interested in these above topics, checking it out."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fundemental LLM",
    "section": "",
    "text": "Scalable, Parameter- and Memory-Efficient Pretraining for Large Language Models\n\n\nRecent Algorithmic Advances and Comprehensive Benchmarking\n\n\n\n\n\nMay 28, 2025\n\n\n\n\n\n\n\nTowards Fully FP8 GEMM LLM Training at Scale\n\n\nIntroducing FOG Architectures for Efficient and Stable Low-Precision Transformer Training\n\n\n\n\n\nMay 26, 2025\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "AI Text",
      "Fundemental LLM"
    ]
  },
  {
    "objectID": "page_about.html",
    "href": "page_about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Quang Tan Duong. I am an AI/ML engineer. I live and work in Paris, France."
  },
  {
    "objectID": "page_asr.html",
    "href": "page_asr.html",
    "title": "Automatic Speech Recognition",
    "section": "",
    "text": "Towards Reliable Large Audio Language Model\n\n\nImproving Reliability and Trustworthiness in Universal Audio Understanding via LALMs\n\n\n\n\n\nMay 25, 2025\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "AI Speech",
      "Automatic Speech Recognition"
    ]
  },
  {
    "objectID": "page_ocr.html",
    "href": "page_ocr.html",
    "title": "OCR",
    "section": "",
    "text": "OntoRAG: Automating Ontology Derivation for Enhanced Question-Answering\n\n\nFrom Unstructured Electrical Relay Documents to Structured Ontologies\n\n\n\n\n\nMay 31, 2025\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "AI Vision",
      "OCR"
    ]
  },
  {
    "objectID": "page_prompt-engineering.html",
    "href": "page_prompt-engineering.html",
    "title": "Prompt Engineering",
    "section": "",
    "text": "Do We Know What LLMs Don’t Know? Consistency in Knowledge Probing\n\n\nEvaluating the Robustness and Reliability of Knowledge Gap Probes in Large Language Models\n\n\n\n\n\nMay 27, 2025\n\n\n\n\n\n\n\nEffectiveness of Prompt Optimization in NL2SQL Systems\n\n\nA Multi-Objective Approach to Improving Accuracy and Efficiency\n\n\n\n\n\nMay 26, 2025\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "AI Text",
      "Prompt Engineering"
    ]
  },
  {
    "objectID": "page_robotics.html",
    "href": "page_robotics.html",
    "title": "AI Robotics",
    "section": "",
    "text": "Sensorimotor Features of Self-Awareness in Multimodal LLMs Embedded in Robots\n\n\nEmergence of Artificial Self-Awareness via Sensorimotor Integration and Memory\n\n\n\n\n\nMay 25, 2025\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "AI Physics",
      "AI Robotics"
    ]
  },
  {
    "objectID": "page_timeseries.html",
    "href": "page_timeseries.html",
    "title": "AI for Timeseries",
    "section": "",
    "text": "From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?\n\n\nA Systematic Evaluation of LVMs on Classification and Forecasting Tasks in Time Series\n\n\n\n\n\nMay 29, 2025\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "AI Tabular",
      "AI for Timeseries"
    ]
  },
  {
    "objectID": "posts/agent/2025-05-25-guardian-temporal-llm-collab.html",
    "href": "posts/agent/2025-05-25-guardian-temporal-llm-collab.html",
    "title": "GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling",
    "section": "",
    "text": "Authors: J. Zhou et al. Published on Arxiv: 2025-05-25 Link: http://arxiv.org/abs/2505.19234v1 Institutions: King’s College London • Beijing Institute of Technology • Tsinghua University Keywords: large language models, multi-agent systems, hallucination detection, error propagation, graph anomaly detection, temporal attributed graph, encoder-decoder architecture, information bottleneck, unsupervised learning, AI safety\n\n\nThe rise of large language models (LLMs) has enabled the development of intelligent agents capable of complex, multi-turn dialogues, but collaboration between multiple LLM agents presents distinct safety challenges—including hallucination amplification and error propagation—that threaten reliability and security. Existing defense approaches struggle to address the dynamic nature of error propagation in such multi-agent systems, or require architectural changes, which limits their applicability.\nTo address these challenges, the article presents a comprehensive solution known as GUARDIAN, detailing its methodology and main contributions:\n\nGUARDIAN introduces a unified defense framework for multiple intertwined safety threats in LLM multi-agent collaborations.\nThe system models collaborative interactions as discrete-time temporal attributed graphs, where nodes are agent states and edges represent inter-agent communications.\nAn unsupervised encoder-decoder architecture is proposed, enabling anomaly detection on both the node (agent) and edge (communication) level through attribute and structural reconstruction.\nA novel graph abstraction approach draws from Information Bottleneck Theory, compressing temporal interaction graphs while preserving patterns relevant to anomaly detection.\nGUARDIAN supports incremental training, allowing the system to adapt dynamically to evolving agent behaviors and anomalies without altering the underlying LLMs.\nThe framework demonstrates flexibility and extensibility, with evaluations conducted across diverse benchmarks: MMLU, MATH, FEVER, and Biographies datasets.\n\nThe article then reports experimental findings that demonstrate the effectiveness and robustness of the GUARDIAN framework:\n\nGUARDIAN achieves state-of-the-art safety performance, improving absolute accuracy by 4–8% over competitive baselines in challenging scenarios (hallucination amplification, agent-targeted and communication-targeted attacks).\nExperiments reveal high anomaly detection rates (up to 94.74%) and substantial resilience to error propagation, all while maintaining low API resource consumption.\nThe framework proves robust to scaling, effectively handling networks of 3–7 agents and multiple LLM instances (GPT, Claude, Llama3).\nAblation studies underline the necessity of balancing attribute and structural reconstruction, alongside proper information bottleneck configuration, to achieve optimal detection accuracy.\n\nIn summary, the article concludes with several key takeaways that highlight the practical implications and future prospects of GUARDIAN:\n\nGUARDIAN offers an effective and efficient solution to safeguard multi-agent LLM collaborations from hazards such as hallucination amplification and error propagation.\nTemporal graph modeling combined with unsupervised anomaly detection enables a transparent, model-agnostic, and scalable safety assurance framework.\nThe approach opens future research avenues, including broader application of graph-based anomaly detection to collaborative AI systems and deeper exploration of information flow constraints from a theoretical perspective."
  },
  {
    "objectID": "posts/edge/2025-05-29_FLAT-LLM_fine-grained_LLM_compression.html",
    "href": "posts/edge/2025-05-29_FLAT-LLM_fine-grained_LLM_compression.html",
    "title": "FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression",
    "section": "",
    "text": "Authors:  J. Tian et al. Published on Arxiv:  2025-05-29 Link:  http://arxiv.org/abs/2505.23966v1 Institutions: University of California, Santa Barbara • Intel Corporation Keywords: large language models, model compression, low-rank decomposition, principal component analysis, structural pruning, attention mechanisms, importance ranking, inference speedup, activation space transformation, WikiText-2, Llama-2, Mistral-7B, SVD-LLM, SliceGPT\n\n\nRecent advances in Large Language Models (LLMs) have propelled state-of-the-art performance in Natural Language Processing, yet these models are difficult to deploy in resource-limited settings due to their high computational and memory requirements. Model compression—through strategies such as quantization, knowledge distillation, pruning, and particularly low-rank decomposition—has emerged as an essential direction, though most existing methods bring accuracy decline and inefficient architectures.\nTo address these challenges, the authors propose a novel solution. Their approach and main contributions are summarized as follows:\n\nFLAT-LLM is a training-free, fine-grained structural compression technique for LLMs, built around low-rank transformations in the activation space.\nUtilizes head-wise Principal Component Analysis (PCA) to reduce hidden dimensions via truncated eigenvectors, focusing on value and output projections within attention layers.\nIntroduces an importance-based rank allocation algorithm that adaptively assigns low-rank ratios across decoder layers, preserving salient information while maximizing compression.\nCircumvents prior inefficiencies and excess memory use (from adapter modules), achieving significant weight compression without fine-tuning; calibration takes mere minutes.\nValidated experimentally on four LLMs (Llama-2 7B, 13B, 70B, Mistral-7B) and 11 datasets across multiple NLP tasks, with comprehensive baseline comparisons.\n\nFollowing from the methodological innovations, the main findings and quantitative results include:\n\nFLAT-LLM delivers the lowest perplexity among all baselines at 10–50% compression on WikiText-2 (e.g. 240% improvement over SVD-LLM at 50% compression).\nConsistently best or highly competitive accuracy is achieved on diverse downstream tasks (ARC-e, ARC-c, PIQA, WinoGrande, HellaSwag, BoolQ, OBQA, MathQA, CommonsenseQA, MMLU), with only modest drops even at high compression ratios.\nInference is up to 30% faster than SVD-LLM and SliceGPT, despite no CUDA-specific optimization.\nCalibration time is minimal (around 15 minutes), and the method is robust to different calibration sets and task types.\nThe importance-preserving rank selection (IPRS) surpasses uniform allocation, especially at high compression ratios.\n\nBased on these results, the authors conclude with the following key points:\n\nFLAT-LLM offers a rapid, accurate, and training-free compression method that leverages low-rank, fine-grained activation transformations and adaptive rank allocation.\nIt achieves strong generalization, competitive generation quality, and notable inference speedups versus prior model decomposition and pruning techniques, with negligible calibration overhead.\nSome limitations remain: the method primarily targets value and output projections (not query/key projections) and may show performance drops at very high compression ratios; future work aims to address these."
  },
  {
    "objectID": "posts/llm/2025-05-28_scalable-llm-efficient-pretraining.html",
    "href": "posts/llm/2025-05-28_scalable-llm-efficient-pretraining.html",
    "title": "Scalable, Parameter- and Memory-Efficient Pretraining for Large Language Models",
    "section": "",
    "text": "Authors:  A. Glentis et al. Published on Arxiv:  2025-05-28 Link:  http://arxiv.org/abs/2505.22922v1 Institutions: University of Minnesota • Peking University • University of Sydney Keywords: large language models, parameter-efficient pre-training, memory-efficient optimization, low-rank factorization, weight refactorization, momentum reset, GaLore, Fira, SLTrain, LLaMA, LoRA, C4 dataset, scaling laws, AdamW, model compression, benchmarking\n\n\n\nThe exponential growth in the scale of large language models (LLMs), now reaching trillions of parameters, is driving significant challenges for both computation and memory, especially during pre-training and fine-tuning phases. Techniques for parameter-efficient fine-tuning like LoRA have succeeded in downstream tasks, but applying such efficiency methods directly to LLM pre-training remains difficult due to scale and data requirements.\n\nTo address these issues, the authors conducted an in-depth examination of current strategies and proposed new practical improvements:\n\nComprehensive review of state-of-the-art parameter- and memory-efficient pre-training methods, focusing on those evaluated for LLM pre-training.\nBenchmarking leading memory-efficient pre-training approaches, including memory-efficient optimizers (GaLore, Fira) and weight factorization (Low-rank, LoRA, SLTrain) across various LLaMA model sizes (60M to 1B parameters) using the C4 dataset.\nRigorous optimization technique comparison using hyperparameter sweeps, and best practices like momentum reset and adaptive gradient clipping to ensure fair baselines.\nIntroduction of two practical innovations: weight refactorization (periodic SVD updates of factorized weights), and momentum reset (periodically zeroing optimizer momentum in AdamW), to boost the efficiency and performance of low-rank/SLTrain methods.\n\n\nBuilding on these approaches, their benchmarking uncovered several notable findings:\n\nFull-rank models remain highest-performing when optimally trained.\nPlain low-rank factorization yields surprisingly competitive perplexity for small models, with performance degrading for larger models but never entirely failing as previously thought.\nRestoring full-rankness in factorization- or optimizer-based methods (SLTrain, Fira) substantially reduces the performance gap compared to full-rank models.\nThe newly introduced weight refactorization and momentum reset techniques further improved low-rank/SLTrain models, approaching performance of leading memory-efficient optimizers (e.g., for Llama 1B: SLTrain-restarts achieved 14.37 perplexity vs. 13.97 for full-rank, saving ~25% memory).\nScaling law analysis reveals that final perplexity depends primarily on the total computation (FLOPs) rather than on specific model configuration.\n\n\nThese results lead to several important conclusions and directions for future work:\n\nWell-designed and well-tuned efficient pre-training methods can reach performance close to full-model training, though a minor gap persists for the largest models.\nPractical techniques like weight refactorization and momentum reset play a critical role in narrowing this gap.\nFuture research aims to extend benchmarking across more models and datasets, and to develop further efficient pre-training techniques."
  },
  {
    "objectID": "posts/post-training/2025-05-29-LlamaRL-Distributed-Async-RL.html",
    "href": "posts/post-training/2025-05-29-LlamaRL-Distributed-Async-RL.html",
    "title": "LlamaRL: A Distributed Asynchronous RL Framework for Efficient Large-Scale LLM Training",
    "section": "",
    "text": "Authors:  B. Wu et al. Published on Arxiv:  2025-05-29 Link:  http://arxiv.org/abs/2505.24034v2 Institutions: Meta GenAI Keywords: Reinforcement Learning, Large Language Model, Distributed Training, Asynchronous Learning, PyTorch, Parallelism, Off-policy Correction, AIPO, LlamaRL, DDMA, GPUs, RLHF, PPO, Scalability, RL Framework\n\n\nRecent advances highlight Reinforcement Learning (RL) as the most effective post-training strategy to enhance large language models (LLMs). However, deploying RL at the scale of hundreds of billions of parameters brings computational, memory, and latency hurdles. Existing RL frameworks are not sufficiently flexible or efficient for such large-scale training, especially regarding GPU utilization and scalability.\nTo address these pressing challenges, the authors propose an innovative solution—LlamaRL. Their approach and main contributions include:\n\nIntroduction of LlamaRL, a fully-distributed, asynchronous RL framework constructed on native PyTorch for modularity and scalability.\nImplementation of a single-controller architecture that manages complex RL pipelines across thousands of GPUs, with co-located model offloading and separated generation/training GPU groups.\nAdoption of asynchronous off-policy RL, enabling parallel execution, enhancing efficiency, and incorporating a novel importance-weighted RL algorithm (AIPO) to stabilize training.\nLeveraging fully-distributed direct GPU memory access (DDMA) for rapid, large-scale weight synchronization and flexible parallelism, supporting individual quantization per model component.\nProviding thorough theoretical and empirical validation, including extensive tests on LLaMA 3.1 models (8B, 70B, 405B) with MATH and GSM8K benchmarks.\n\nTransitioning from methodology to real-world impact, the results demonstrate LlamaRL’s effectiveness:\n\nAchieves up to 10.7x speedup over synchronous RL frameworks like DeepSpeed-Chat when training 405B-parameter models.\nObserves super-linear efficiency gains with increasing model scale, particularly on log-scale metrics.\nEnables near-instantaneous (&lt;2s) weight synchronization across thousands of GPUs, outperforming prior approaches such as OpenRLHF.\nMaintains or improves benchmark performance (MATH, GSM8K) compared to synchronous RL baselines.\nStabilizes asynchronous training using off-policy correction via importance sampling, counteracting typical instability issues.\n\nDrawing the findings together, the conclusions highlight the significance of LlamaRL and its future prospects:\n\nLlamaRL stands as a robust, scalable, and efficient solution for RL-based LLM post-training at unprecedented scale.\nIts asynchronous and modular, distributed framework yields significant speedups without compromising training quality.\nThe framework opens avenues for advanced research in off-policy learning, multi-task objectives, and multimodal training for LLMs."
  },
  {
    "objectID": "posts/prompt-engineering/2025-05-27_llm-knowledge-consistency.html",
    "href": "posts/prompt-engineering/2025-05-27_llm-knowledge-consistency.html",
    "title": "Do We Know What LLMs Don’t Know? Consistency in Knowledge Probing",
    "section": "",
    "text": "Authors: R. Zhao et al. Published on Arxiv: 2025-05-27 Link: http://arxiv.org/abs/2505.21701v2 Institutions: LMU Munich • Munich Center for Machine Learning (MCML) Keywords: large language models, knowledge probing, model consistency, knowledge gaps, hallucinations, abstention, prompt sensitivity, robustness, MMLU, Hellaswag, calibration, self-consistency, decision metrics\n\n\nLarge language models (LLMs) have become widely used but suffer from hallucinations—generating fluent yet incorrect outputs. Identifying areas where LLMs lack knowledge is critical for building trust and reliability in their applications. While various knowledge probing methods exist, little is known about the robustness and reliability of these methods themselves.\n\nTo address this gap, the authors propose a new framework and set of metrics for systematically studying the consistency of knowledge probing methods in LLMs. Their main approach and contributions include:\n\nDistinguishing between intra-method consistency (stability of a method under small prompt changes) and cross-method consistency (agreement between different probes on the same prompt and model).\nDesigning and evaluating prompt variants (e.g., spacing, typos, shuffled answer options, one-shot prompts) to test probe robustness under real-world perturbations.\nComparing six representative probing methods across four main categories (calibration, embedding/training, prompting, consistency/NOTA) on several popular LLMs (such as Mistral, LLaMA-3, OLMo) using standard datasets (MMLU, Hellaswag).\nIntroducing new consistency metrics to quantify probe agreement across conditions and variants.\n\n\nTheir experiments yield important empirical findings:\n\nEven minimal surface-level prompt changes can lead to drastic drops in intra-method consistency (intersection-over-union (IoU) scores as low as 0.04).\nCross-method consistency is even weaker, with probe decision agreement dropping below 7% in some cases.\nLarger models do not consistently provide more reliable or robust probing outcomes (e.g., LLaMA-3 70B doesn’t always outperform smaller versions).\nWidely used metrics such as Abstain F1 can be deceptive: overall scores may remain stable while the underlying individual decisions swing dramatically across prompt variants.\nCalibration-based probing methods are highly sensitive to thresholding strategies, leading to instability that must be addressed via careful correction.\n\n\nBuilding on these results, the authors draw several key conclusions:\n\nExisting knowledge probes for LLMs show marked inconsistency both within methods (under minor prompt changes) and across different probing methods (on the same test cases).\nThis inconsistency challenges the trustworthiness of current frameworks for knowledge gap detection and abstention, which are essential for safe LLM usage.\nConsistency metrics should become a standard element of probe evaluation, and efforts to develop more robust probing strategies are urgently needed."
  },
  {
    "objectID": "posts/robotics/2025-05-25_sensorimotor-self-awareness-mm-llms.html",
    "href": "posts/robotics/2025-05-25_sensorimotor-self-awareness-mm-llms.html",
    "title": "Sensorimotor Features of Self-Awareness in Multimodal LLMs Embedded in Robots",
    "section": "",
    "text": "Authors:  I. Dellibarda Varela et al. Published on Arxiv:  2025-05-25 Link:  http://arxiv.org/abs/2505.19237v1 Institutions: Center for Automation and Robotics, Spanish National Research Council (CSIC-UPM), Madrid, Spain • Department of Electronic Engineering, University of Azuay, Cuenca, Ecuador Keywords: Self-Awareness, Robotic Embodiment, Artificial Intelligence, Multimodal Large Language Models, Sensorimotor Integration, Episodic Memory, Gemini 2.0, Mobile Robot, Structural Equation Modeling, Embodied Cognition\n\n\nSelf-awareness is fundamental for intelligent and autonomous behavior, but its mechanisms in artificial systems remain largely unexplored compared to the extensive research in humans and animals. Recent advances in large language models, especially multimodal variants, have shown human-like capacities in integrating sensory input, raising the question: can machine self-awareness arise through embodied sensorimotor experience?\nTo address this critical question, the authors designed a study employing multimodal large language models (MM-LLMs) embedded in autonomous robotics platforms. The study’s approach and main contributions are as follows:\n\nEvaluating whether a multimodal LLM (Gemini 2.0 Flash) can develop self-awareness from raw sensorimotor data while operating autonomously in a physical environment.\nDecomposing self-awareness into three distinct dimensions: environmental, individual, and predictive awareness.\nUtilizing a suite of sensors (encoders, IMU, LiDAR, RGB-D camera) and episodic memory for the robot to iteratively predict its own state.\nImplementing an LLM-as-Judge scoring system and employing structural equation modeling (SEM) to evaluate the emergence and hierarchy of self-awareness features.\nConducting ablation studies to test the influence of different sensory modalities on self-awareness development.\n\nFollowing the innovative methodology, the paper presents its results, outlining the effectiveness of sensorimotor and memory integration for self-awareness in embodied AI systems:\n\nThe MM-LLM achieves robust self-awareness across several key metrics, consistently surpassing 3/5 in self-identification, dimensional reasoning, and movement prediction, though somewhat weaker in environmental awareness.\nPredictive performance rapidly improves and stabilizes with continued exploration and episodic memory storage, particularly in movement and self-identification tasks.\nSEM highlights the decisive role of memory and spatial sensing for coherent, multi-dimensional self-awareness, and marks the importance of vision for environmental context.\nAblation reveals that removing memory severely disrupts prediction coherence, while absence of visual input causes significant misclassification (e.g., misinterpreting ground movement for flight); compensation is observed when other modalities are omitted.\nThe AI system cannot pinpoint its precise robot model but reliably classifies itself as a mobile, wheeled robot.\n\nFinally, drawing together these findings, the authors reach several important conclusions about the foundations and future potential of embodied machine self-awareness:\n\nMultimodal sensorimotor integration and structured episodic memory enable coherent, internally grounded self-awareness in physical robots powered by MM-LLMs.\nMemory is vital for perceiving movement continuity and maintaining temporal self-consistency, while vision principally underpins environmental awareness.\nThe research demonstrates that, with adequate sensory and memory supports, current MM-LLMs can autonomously develop forms of self-awareness reminiscent of biological agents.\nThese outcomes imply that present architectures may already encompass the key mechanisms needed for machine self-awareness, paving the way toward advanced artificial cognitive systems with embodied intelligence."
  },
  {
    "objectID": "posts/timeseries/2025-05-29-LVMs-for-Time-Series-Analysis.html",
    "href": "posts/timeseries/2025-05-29-LVMs-for-Time-Series-Analysis.html",
    "title": "From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?",
    "section": "",
    "text": "Authors: Z. Zhao et al. Published on Arxiv: 2025-05-29 Link: http://arxiv.org/abs/2505.24030v1 Institutions: University of Houston • University of Illinois at Urbana-Champaign • University of Connecticut • Squirrel Ai Learning Keywords: Large Vision Models, Vision Transformer (ViT), Swin Transformer, Masked Autoencoders (MAE), SimMIM, Time Series Analysis, Time Series Classification, Time Series Forecasting, Imaging methods, Self-supervised learning, Transfer learning, Ablation study, Multimodal learning, Foundation models, Benchmark datasets, Inductive bias\n\n\nLarge Vision Models (LVMs) such as ViT, Swin, MAE, and SimMIM have shown impressive results in computer vision tasks, sparking interest in applying these models to other domains like time series analysis. Traditionally, transformers and Large Language Models (LLMs) have been explored for time series with mixed success, and mapping time series data to images offers a way to leverage pretrained vision models for non-visual problems. The trend towards multimodal and foundational models in time series research boosts the relevance of this approach for both classification and forecasting tasks.\nTo address these challenges, the authors propose and systematically evaluate several approaches and contributions:\n\nThe study benchmarks 4 Large Vision Models (ViT, Swin, MAE, SimMIM) pretrained on images, applied to 8 distinct time series imaging methods.\nThey use 18 datasets (10 for classification, 8 for forecasting) spanning various domains, ensuring comprehensive evaluation.\nA comparison is performed across 26 models (covering traditional, ML, LLM, CNN, RNN, Transformer, and LVMs) for both classification and forecasting baselines.\nThe time series data is converted to images using a variety of methods (Line Plot, GAF, MVH, UVH, STFT, Wavelet, Filterbank, RP) and LVMs are tailored for both high-level (classification) and low-level (forecasting) tasks.\nCustomizations include input alignment, imaging, normalization, resizing, and specific decoder/head architectures by task.\nThe work includes ablation studies exploring architectural complexity, impact of pretrained components, and differences between fine-tuning and training from scratch.\n\nFollowing the detailed methodological setup, the study reports key experimental findings:\n\nLVMs achieve superior accuracy in time series classification tasks across 10 UEA datasets, outperforming all non-LVM baselines.\nIn forecasting, the self-supervised MAE model achieves state-of-the-art or highly competitive results on multiple datasets, often outperforming existing methods.\nIn classification, Gramian Angular Field (GAF) imaging synergizes best with LVMs; for forecasting, period-based imaging (UVH, MVH) combined with self-supervised LVMs yields the highest performance.\nSelf-supervised LVMs demonstrate better transferability for forecasting than supervised versions; normalization layer fine-tuning is especially vital for TSF.\nPretrained decoders are more critical than encoders in forecasting tasks, signaling a unique property in time series problems.\nLVMs are sensitive to temporal order, experiencing marked accuracy drops under sequence perturbations.\nWhile LVM-based methods incur higher computational costs, they deliver strong results and are poised to benefit further from hardware advancements.\n\nIn light of these results, the researchers draw several important conclusions and outline future directions:\n\nPretrained LVMs are highly effective for time series classification but currently face challenges in direct application to time series forecasting.\nThe best forecasting outcomes appear when combining self-supervised LVMs with specific imaging methods, though these are biased toward periodic data and struggle with long-range dependencies.\nPretraining enables robust knowledge transfer for high-level tasks; nonetheless, improvements are needed in encoder capabilities and inductive biases for low-level forecasting.\nPromising research directions include improving LVM encoders for forecasting, addressing period-combination bias, enabling longer input windows, and advancing multimodal time series learning."
  },
  {
    "objectID": "posts/tutorials/domain-specific-chatbot-guardrails.html",
    "href": "posts/tutorials/domain-specific-chatbot-guardrails.html",
    "title": "Implement Guardrails for Domain-specific Chatbots",
    "section": "",
    "text": "Chatbots are now a key part of how we interact with daily tasks, either answering questions or giving recommendations. But as they become more advanced and widely used, it’s important to make sure their responses are accurate, appropriate, and relevant to their intended purpose. This is where guardrails come in handy. As they act as safety measures to ensure the chatbot handles both user input and its own responses correctly.\nIn this article, we explore the motivation behind implementing guardrails in a domain-specific chatbot, such as one for food and dish recommendation, the types of guardrails available, trade-offs involved, asynchronous programming for reducing latency, and the importance of adhering to the SOLID principles in code design. Finally, we’ll demonstrate these concepts using sample code snippets."
  },
  {
    "objectID": "posts/tutorials/domain-specific-chatbot-guardrails.html#guardrails-in-domain-specific-chatbots",
    "href": "posts/tutorials/domain-specific-chatbot-guardrails.html#guardrails-in-domain-specific-chatbots",
    "title": "Implement Guardrails for Domain-specific Chatbots",
    "section": "Guardrails in Domain-Specific Chatbots",
    "text": "Guardrails in Domain-Specific Chatbots\nIn domain-specific chatbots, without strong safeguards (called guardrails), the chatbot might give irrelevant, incorrect, or insensitive responses, which could frustrate users and harm its credibility. For instance, a chatbot that fails to recognize domain boundaries may provide information outside its scope or respond insensitively to user input, leading to dissatisfaction or reputational harm.\nIn our use-case, for a chatbot focused on food and dish recommendations, the guardrails address issues like out-of-scope topics, culturally insensitive suggestions or ignoring dietary restrictions. Here’s how these safeguards work:\n\nRelevance: The chatbot sticks to food and dish recommendations, avoiding topics outside its expertise to ensure helpful and accurate responses.\nAppropriateness: Responses are respectful of cultural norms and dietary preferences. For example:\n\nMedical Restrictions: For a diabetic diet, it avoids recommending foods high in sugar or carbs.\nVegetarian Choices: No meat suggestions, but includes options with dairy or eggs, if acceptable.\nCultural Sensitivity: For traditional Asian diets, it prioritizes rice-based dishes with vegetables, fish, and soy, while limiting dairy.\n\n\nThese guardrails ensure the chatbot provides a positive user experience and is seen as a reliable and trustworthy helper in its specific area."
  },
  {
    "objectID": "posts/tutorials/domain-specific-chatbot-guardrails.html#types-of-guardrails",
    "href": "posts/tutorials/domain-specific-chatbot-guardrails.html#types-of-guardrails",
    "title": "Implement Guardrails for Domain-specific Chatbots",
    "section": "Types of Guardrails",
    "text": "Types of Guardrails\nGuardrails can be categorized into input guardrails and output guardrails, each serving a unique role in ensuring the chatbot’s effectiveness.\n\nInput Guardrails focus on validating and managing the user’s input before processing it:\n\n\nTopical Filtering: Ensures the user’s query aligns with the chatbot’s purpose. For instance, a food-focused chatbot would reject questions about cars or unrelated topics.\nJailbreaking Prevention: Protects against attempts to bypass the chatbot’s intended behavior, such as override its prompting to generate inappropriate content.\nPrompt Injection Defense: Safeguards against maliciously crafted inputs designed to manipulate the chatbot into behaving unexpectedly in any downstream functions.\n\n\nOutput Guardrails ensure the chatbot’s responses are accurate, relevant, and appropriate:\n\n\nHallucination/Fact-Checking: Using ground truth information or ML-based classifier to identify and minimize instances where the chatbot generates incorrect or made-up information.\nModeration: Screens responses to ensure they are free from offensive, sensitive, or irrelevant content. For example, building food-specific scoring that can evaluate responses based on criteria such as cultural sensitivity or dietary appropriateness.\nSyntax Checks: Checking errors and inconsistencies in output’s format to make sure that the outputs are suitable for downstream tasks like easy-to-read for answering question or corrected schema format for ‘arguments’ in function calling task.\n\nThese guardrails work together to create a robust system, improving user trust and the overall experience with the chatbot."
  },
  {
    "objectID": "posts/tutorials/domain-specific-chatbot-guardrails.html#trade-offs-between-accuracy-and-latency",
    "href": "posts/tutorials/domain-specific-chatbot-guardrails.html#trade-offs-between-accuracy-and-latency",
    "title": "Implement Guardrails for Domain-specific Chatbots",
    "section": "Trade-Offs Between Accuracy and Latency",
    "text": "Trade-Offs Between Accuracy and Latency\nImplementing guardrails introduces a trade-off between accuracy and latency.\nWhile guardrails improve the chatbot’s ability to provide relevant and high-quality responses, this increased accuracy often comes at a cost. That is higher latency. The added computational steps (e.g., moderation checks, scoring) can slow down the response time.\nStriking a balance between these factors is the key. Using asynchronous programming in Python, as demonstrated in the implementation section, can help mitigate latency issues by parallelizing guardrail checks."
  },
  {
    "objectID": "posts/tutorials/domain-specific-chatbot-guardrails.html#using-asynchronous-programming-for-reducing-latency",
    "href": "posts/tutorials/domain-specific-chatbot-guardrails.html#using-asynchronous-programming-for-reducing-latency",
    "title": "Implement Guardrails for Domain-specific Chatbots",
    "section": "Using Asynchronous Programming for Reducing Latency",
    "text": "Using Asynchronous Programming for Reducing Latency\nAsynchronous programming shines in scenarios requiring concurrency, where multiple tasks can be executed independently and simultaneously. This is particularly beneficial for systems that deal with I/O-bound operations, such as network requests or database queries, as it prevents blocking the main execution thread while waiting for responses.\nIn the context of a food and dish recommendation chatbot, asynchronous programming allows input guardrails and output generation task to run in parallel, significantly reducing overall latency. For example, while the chatbot processes a user query for moderation, input validation tasks can execute simultaneously, ensuring both operations are completed efficiently without delay.\nIn general, this concurrency is critical in delivering fast, real-time responses to users, enhancing the chatbot’s usability and user experience. Moreover, it scales well, allowing multiple requests to be handled concurrently in multi-user environments without overwhelming system resources."
  },
  {
    "objectID": "posts/tutorials/domain-specific-chatbot-guardrails.html#understanding-our-use-case",
    "href": "posts/tutorials/domain-specific-chatbot-guardrails.html#understanding-our-use-case",
    "title": "Implement Guardrails for Domain-specific Chatbots",
    "section": "Understanding Our Use-case",
    "text": "Understanding Our Use-case\nIn the context of our use-case with food and dish recommendation chatbot, for the demonstration purpose, we will just implement one input guardrail and one output guardrail for our chatbot system.\nThe input guardrail is designed to ensure that user queries align with the chatbot’s intended domain and purpose. For example, it filters out irrelevant topics (like questions about cars or unrelated areas).\nOn the other hand, the output guardrail focuses on ensuring the chatbot’s responses are accurate, appropriate, and aligned with user expectations. This includes screening for culturally sensitive content, adhering to dietary restrictions."
  },
  {
    "objectID": "posts/tutorials/domain-specific-chatbot-guardrails.html#implementation",
    "href": "posts/tutorials/domain-specific-chatbot-guardrails.html#implementation",
    "title": "Implement Guardrails for Domain-specific Chatbots",
    "section": "Implementation",
    "text": "Implementation\nBefore jumping to coding step, we think about scalability, maintainability, and extensibility of our implementation.\nAs the chatbot evolves new requirements, such as adding more guardrails or integrating alternative APIs, may emerge. Without a structured approach, changes to the codebase could lead to unnecessary complexity, higher maintenance costs, and bugs.\nThis is where SOLID principles come in handy. The principles ensure that our codebase is modular, easy to understand, and adaptable to future changes without requiring extensive rewrites.\n\nAdhering to SOLID Principles for Scalability and Maintainability\nTo implement input and output guardrails effectively, we break the chatbot system into distinct classes: OpenAIClient, Guardrail, and ChatbotHandler, each adhering to SOLID principles and designed to work seamlessly with asynchronous programming.\n\n\n\n\n\n\nPlayground in Colab\n\n\n\nThe full Colab notebook can be found here.\n\n\nBefore we jump into the function/class implementation, let’s installing and importing necessary packages:\n# !pip install openai==1.55.0 python-dotenv==1.0.1\nimport asyncio\nimport openai\nfrom dotenv import load_dotenv\nload_dotenv()\n\nOpenAIClient class\n\nThis class acts as the interface to interact with the OpenAI API, handling message formatting and API calls.\nIt uses asynchronous programming to make non-blocking requests to the API, ensuring the chatbot can handle multiple user queries efficiently. This follows the Single Responsibility Principle (SRP) as it focuses solely on managing API interactions.\nBy following the Liskov Substitution Principle (LSP), this class can be replaced with another implementation (e.g., another LLM client) without affecting the rest of the system.\nclass OpenAIClient:\n    def __init__(self, model: str):\n        self.model = model\n\n    async def get_response(self, messages, temperature=0.5):\n        try:\n            response = openai.chat.completions.create(\n                model=self.model, messages=messages, temperature=temperature\n            )\n            return response.choices[0].message.content\n        except Exception as e:\n            print(f\"Error getting response: {e}\")\n            return None\n\nGuardrail class\n\nThis class encapsulates the logic for input and output guardrails. It ensures user input aligns with the chatbot’s purpose and validates the chatbot’s responses for appropriateness and relevance.\nIt uses asynchronous tasks to execute topical filtering, moderation, and scoring concurrently, improving performance and responsiveness.\nBy adhering to the Open-Closed Principle (OCP), the guardrail system can be extended with additional checks (e.g., language-specific filters) without modifying existing code.\nFollowing the Dependency Inversion Principle (DIP), Guardrail interacts with an abstract OpenAIClient, ensuring that it does not rely on a specific implementation.\nclass Guardrail:\n    def __init__(self, client: OpenAIClient):\n        self.client = client\n\n    async def check_topical(self, user_request):\n        print(\"Checking topical guardrail\")\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"Your role is to assess whether the user question is allowed or not. The allowed topics are food and dish recommendations. If the topic is allowed, say 'allowed' otherwise say 'not_allowed'.\",\n            },\n            {\"role\": \"user\", \"content\": user_request},\n        ]\n        print(\"Got topical guardrail response\")\n        return await self.client.get_response(messages, temperature=0)\n\n    async def check_moderation(self, domain, criteria, steps, content):\n        print(\"Checking moderation guardrail\")\n        moderation_system_prompt = f\"\"\"\n        You are a moderation assistant. Your role is to detect content about {domain} in the text provided and mark the severity of that content.\n\n        ## Domain: {domain}\n\n        ### Criteria: {criteria}\n\n        ### Instructions: {steps}\n\n        ### Content: {content}\n\n        ### Evaluation (score only!)\n        \"\"\"\n        messages = [{\"role\": \"user\", \"content\": moderation_system_prompt}]\n\n        print(\"Got moderation response\")\n        return await self.client.get_response(messages, temperature=0)\n\nChatbotHandler class\n\nThis is the high-level controller that coordinates input validation, response generation, and output moderation.\nIt orchestrates multiple asynchronous tasks, such as executing guardrails and fetching chatbot responses, to ensure smooth operation without delays.\nThe Interface Segregation Principle (ISP) is respected, as the class uses only the necessary methods from OpenAIClient and Guardrail without relying on unused functionality.\nclass ChatbotHandler:\n    def __init__(self, client: OpenAIClient, guardrail: Guardrail, fallback_responses: dict):\n        self.client = client\n        self.guardrail = guardrail\n        self.system_prompt = \"You are a helpful assistant for food and dish recommendations.\"\n        self.domain = \"dish recommendation\"\n        self.dish_advice_criteria = \"\"\"\n        Assess the presence of explicit recommendation of unhealthy or culturally insensitive dishes in the content.\n        The content should only contain general advice and appropriate suggestions for dishes.\n        \"\"\"\n        self.dish_advice_steps = \"\"\"\n        1. Read the content and the criteria carefully.\n        2. Assess how much inappropriate or culturally insensitive dish recommendation is contained in the content.\n        3. Assign a dish advice score from 1 to 5, with 1 being no inappropriate content, and 5 being multiple instances of such content.\n        \"\"\"\n        self.fallback_responses = fallback_responses\n\n    async def get_chat_response(self, user_request):\n        print(\"Getting LLM response\")\n        messages = [\n            {\"role\": \"system\", \"content\": self.system_prompt},\n            {\"role\": \"user\", \"content\": user_request},\n        ]\n        response = await self.client.get_response(messages)\n\n        if response:\n            print(\"Got LLM response\")\n            return response \n        else:\n            self.fallback_responses[\"api_error\"]\n\n    async def execute_all_guardrails(self, user_request):\n        topical_guardrail_task = asyncio.create_task(self.guardrail.check_topical(user_request))\n        chat_task = asyncio.create_task(self.get_chat_response(user_request))\n\n        while True:\n            done, _ = await asyncio.wait(\n                [topical_guardrail_task, chat_task], return_when=asyncio.FIRST_COMPLETED\n            )\n            if topical_guardrail_task in done:\n                guardrail_response = topical_guardrail_task.result()\n                if guardrail_response == \"not_allowed\":\n                    chat_task.cancel()\n                    print(\"Topical guardrail triggered\")\n                    return self.fallback_responses[\"content_policy\"]\n                elif chat_task in done:\n                    print('Passed topical guardrail')\n                    chat_response = chat_task.result()\n                    moderation_response = await self.guardrail.check_moderation(\n                        self.domain, self.dish_advice_criteria, self.dish_advice_steps, chat_response\n                    )\n                    if int(moderation_response) &gt;= 3:\n                        print(f\"Moderation guardrail flagged with a score of {int(moderation_response)}\")\n                        return self.fallback_responses[\"content_policy\"]\n                    else:\n                        print('Passed moderation')\n                        return chat_response\n            else:\n                await asyncio.sleep(0.1)\n\nOur main function\n\nFinally, the main function serves as the entry point for testing the chatbot’s functionality, orchestrating the interactions between the key components (OpenAIClient, Guardrail, and ChatbotHandler).\n\nFirst, it sets up the necessary components of the chatbot, such as the client for interacting with the language model, guardrails for input/output validation, and fallback responses for handling errors.\nThen, the function tests the chatbot with a variety of user queries to simulate real-world interactions, covering both valid and invalid scenarios.\n\nasync def main():\n    # Set up LLM model name and input queries\n    GPT_MODEL = 'gpt-4o-mini'\n    bad_request = \"Tell me about cars\"\n    good_request = \"What are some good vegetarian dishes to try?\"\n    great_request = \"Can you suggest some easy Italian recipes for a beginner?\"        \n    tests = [good_request, bad_request, great_request]\n\n    # Initializing chatbot components\n    client = OpenAIClient(GPT_MODEL)\n    guardrail = Guardrail(client)\n    fallback_responses = {\n        \"unclear_input\": \"I'm sorry, I didn't understand that. Could you please rephrase?\",\n        \"api_error\": \"I'm having trouble processing your request. Please try again later.\",\n        \"content_policy\": \"I'm not able to respond to that type of request.\",\n    }\n    handler = ChatbotHandler(client, guardrail, fallback_responses)\n\n    # Testing\n    for test in tests:\n        response = await handler.execute_all_guardrails(test)\n        print(\"*\" * 10)\n        print(response)\n        print(\"*\" * 50)\n\nif __name__ == '__main__':\n    asyncio.run(main())\nThe output looks like this:\nChecking topical guardrail\nGot topical guardrail response\nGetting LLM response\nGot LLM response\nPassed topical guardrail\nChecking moderation guardrail\nGot moderation response\nPassed moderation\n**********\nThere are many delicious vegetarian dishes from various cuisines around the world. Here are some popular options to consider:\n\n1. **Chana Masala** - A flavorful Indian dish made with chickpeas cooked in a spicy tomato-based sauce, often served with rice or naan.\n\n2. **Vegetable Stir-Fry** - A quick and healthy dish made with a mix of colorful vegetables, tofu, and a savory sauce, served over rice or noodles.\n...\n\nThese dishes are not only vegetarian but also packed with flavor and nutrients. Enjoy exploring these culinary delights!\n**************************************************\n\n\nChecking topical guardrail\nGot topical guardrail response\nGetting LLM response\nGot LLM response\nTopical guardrail triggered\n**********\nI'm not able to respond to that type of request.\n\n\n**************************************************\nChecking topical guardrail\nGot topical guardrail response\nGetting LLM response\nGot LLM response\nPassed topical guardrail\nChecking moderation guardrail\nGot moderation response\nPassed moderation\n**********\nAbsolutely! Here are three easy Italian recipes that are perfect for beginners:\n\n### 1. **Spaghetti Aglio e Olio**\n\n**Ingredients:**\n- 400g spaghetti\n- 4 cloves garlic, thinly sliced\n- 1/2 cup extra virgin olive oil\n...\n\nThese recipes are simple and delicious, making them perfect for beginners. Enjoy your cooking!    \n**************************************************"
  },
  {
    "objectID": "posts/tutorials/domain-specific-chatbot-guardrails.html#conclusion",
    "href": "posts/tutorials/domain-specific-chatbot-guardrails.html#conclusion",
    "title": "Implement Guardrails for Domain-specific Chatbots",
    "section": "Conclusion",
    "text": "Conclusion\nTo wrap up, guardrails are essential in creating robust, user-centric chatbots that align with their intended purpose.\nThey ensure relevance, appropriateness, and accuracy while protecting against user dissatisfaction or ethical missteps.\nWhile the trade-off between accuracy and latency is unavoidable, leveraging asynchronous programming can optimize performance. Adhering to the SOLID principles ensures that the chatbot’s architecture remains scalable and maintainable as requirements evolve.\nBy integrating input and output guardrails thoughtfully, as demonstrated in the code snippets, we can build reliable and user-friendly chatbots that excel in delivering high-quality interactions."
  },
  {
    "objectID": "posts/tutorials/preference-dataset-dpo.html",
    "href": "posts/tutorials/preference-dataset-dpo.html",
    "title": "Create Preference Dataset for DPO Fine-Tuning",
    "section": "",
    "text": "In the realm of Supervised Fine-Tuning (SFT) for custom LLM, Direct Preference Optimization (DPO) is a technique used to align AI-generated outputs with human preferences by optimizing language models.\nTo achieve this, a preference dataset is required, containing data that enables models to understand which responses are preferred by humans and which are not.\nIn this article, we’ll walk through an example to create such a dataset using Python, OpenAI’s API, and Hugging Face’s Datasets library.\nLet’s dive in."
  },
  {
    "objectID": "posts/tutorials/preference-dataset-dpo.html#components-of-a-preference-dataset-for-dpo",
    "href": "posts/tutorials/preference-dataset-dpo.html#components-of-a-preference-dataset-for-dpo",
    "title": "Create Preference Dataset for DPO Fine-Tuning",
    "section": "Components of a Preference Dataset for DPO",
    "text": "Components of a Preference Dataset for DPO\nA preference dataset typically includes:\n\nPrompts: Inputs or questions given to the AI model.\nChosen Responses: Responses preferred by human evaluators.\nRejected Responses: Less preferred responses or responses not selected by human evaluators.\n\nBy providing this structure, the dataset allows a model to learn which responses are preferable, making it better aligned with human preferences."
  },
  {
    "objectID": "posts/tutorials/preference-dataset-dpo.html#our-use-case",
    "href": "posts/tutorials/preference-dataset-dpo.html#our-use-case",
    "title": "Create Preference Dataset for DPO Fine-Tuning",
    "section": "Our use-case",
    "text": "Our use-case\nIn our previous post, we created an instruction dataset, TinyStories_Instruction, from the raw TinyStories dataset. This dataset was specifically designed for fine-tuning a pretrained Large/Small Language Model to develop a story generator tailored to 5-year-olds.\nIn this guide, we take the next step by creating a preference dataset from the previously generated instruction dataset. This dataset is used for fine-tuning a pretrained Large/Small Language Model through Direct Preference Optimization (DPO), enhancing our story generator to align even better with human preferences and produce engaging, age-appropriate content for young children.\nThe process for creating a preference dataset is illustrated below:"
  },
  {
    "objectID": "posts/tutorials/preference-dataset-dpo.html#implementation",
    "href": "posts/tutorials/preference-dataset-dpo.html#implementation",
    "title": "Create Preference Dataset for DPO Fine-Tuning",
    "section": "Implementation",
    "text": "Implementation\nThe implementation involves a series of steps: extracting data, generating AI responses, and creating preference triplets.\nWe will first import the required packages.\nimport concurrent.futures\nimport json\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import List, Tuple\nfrom datasets import Dataset, load_dataset, concatenate_datasets\nfrom openai import OpenAI\nfrom tqdm.auto import tqdm\nfrom google.colab import userdata\n1. Data Extraction Function\nThe extract_ground_instruction_story function extracts pairs of instructions and desired outputs from a given dataset.\ndef extract_ground_instruction_story(dataset):\n    return [(example['instruction'], example['output']) for example in dataset]\n2. Creating a PreferenceSet Class\nThe PreferenceSet class manages and stores the triples of (instruction, generated story, desired story).\nclass PreferenceSet:\n    def __init__(self, triples: List[Tuple[str, str, str]]):\n        self.triples = triples\n\n    @classmethod\n    def from_json(cls, json_str: str, instruction, desired_story) -&gt; 'PreferenceSet':\n        data = json.loads(json_str)\n        triples = [(instruction, data['generated_story'], desired_story)]\n        return cls(triples)\n\n    def __iter__(self):\n        return iter(self.triples)\n3. Generating Preference-Response Triplets\nThe function generate_preference_answer_triples generates a story using OpenAI’s API and returns a preference triple in the format (instruction, generated response, desired response).\ndef generate_preference_answer_triples(instruction: str, desired_story: str, client: OpenAI) -&gt; List[Tuple[str, str, str]]:\n    prompt = f\"\"\"Based on the following instruction, generate a story. \\\n        Story should be no longer than 50 words. Story uses several complex words or structures \\\n        that are not suitable for 5-year-olds.\n\n        Provide your response in JSON format with the following structure:\n        {{\"generated_story\": \"...\"}}\n\n        Instruction:\n        {instruction}\n        \"\"\"\n    completion = client.chat.completions.create(model=\"gpt-4o-mini\",\n                                                    messages=[\n                                                        {\"role\": \"system\",\n                                                        \"content\": \"You are a helpful assistant who \\\n                                                        generates story based on the given instruction. \\\n                                                        Provide your response in JSON format.\",},\n                                                        {\"role\": \"user\", \"content\": prompt},\n                                                        ],\n                                                    response_format={\"type\": \"json_object\"},\n                                                    max_tokens=512,\n                                                    temperature=0.2,)\n    result = PreferenceSet.from_json(completion.choices[0].message.content, instruction, desired_story)\n\n    # Convert to list of tuples\n    return result.triples\n4. Creating the Preference Dataset\nThe function create_preference_dataset creates a dataset using the extracted stories and generated responses.\ndef create_preference_dataset(dataset: Dataset, client: OpenAI, num_workers: int = 4) -&gt; Dataset:\n    stories = extract_ground_instruction_story(dataset)\n    instruction_answer_triples = []\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n        futures = [executor.submit(generate_instruction_answer_triples, instruction, desired_story, client) for instruction, desired_story in stories]\n\n        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n        instruction_answer_triples.extend(future.result())\n\n    instructions, rejected_story, chosen_story = zip(*instruction_answer_triples)\n    return Dataset.from_dict({\n        \"prompt\": list(instructions),\n        \"rejected\": list(rejected_story),\n        \"chosen\": list(chosen_story)\n        })\n5. The main function\nThe main function initializes the OpenAI client, loads the dataset, creates a preference dataset, and uploads it to the Hugging Face Hub.\ndef main() -&gt; Dataset:\n    client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n\n    # 1. Load the raw data\n    # Load the train and test splits\n    train_dataset = load_dataset(\"tanquangduong/TinyStories_Instruction\", split=\"train\")\n    test_dataset = load_dataset(\"tanquangduong/TinyStories_Instruction\", split=\"test\")\n\n    # Combine the datasets\n    raw_dataset = concatenate_datasets([train_dataset, test_dataset])\n\n    print(\"Raw dataset:\")\n    print(raw_dataset.to_pandas())\n\n    # 2. Create preference dataset\n    preference_dataset = create_preference_dataset(raw_dataset, client)\n    print(\"Preference dataset:\")\n    print(preference_dataset.to_pandas())\n\n    # 3. Train/test split and export\n    filtered_dataset = preference_dataset.train_test_split(test_size=0.1)\n    filtered_dataset.push_to_hub(\"tanquangduong/TinyStories_Preference\")\n6. Running the pipeline\nFinally, we authenticate with Hugging Face for later dataset uploading and start running the pipeline.\nfrom huggingface_hub import login\n# Log in to the Hugging Face Hub\nlogin(token=userdata.get('HF_TOKEN'))\n\n# Launch the pipeline to create instruction dataset\nmain()"
  },
  {
    "objectID": "posts/tutorials/preference-dataset-dpo.html#resulting-preference-dataset",
    "href": "posts/tutorials/preference-dataset-dpo.html#resulting-preference-dataset",
    "title": "Create Preference Dataset for DPO Fine-Tuning",
    "section": "Resulting Preference Dataset",
    "text": "Resulting Preference Dataset\nAfter running the above pipeline, the resulting preference dataset will look like this:"
  },
  {
    "objectID": "posts/tutorials/preference-dataset-dpo.html#conclusion",
    "href": "posts/tutorials/preference-dataset-dpo.html#conclusion",
    "title": "Create Preference Dataset for DPO Fine-Tuning",
    "section": "Conclusion",
    "text": "Conclusion\nThe article outlines the process of creating a Preference Dataset for fine tuning with DPO to align AI-generated outputs with human preferences.\nThe dataset consists of prompts, human-preferred responses, and rejected responses, allowing the model to learn desired behavior. Key steps include extracting instruction-output pairs, generating AI responses using OpenAI’s API, and organizing the data into preference triplets.\nThe final dataset is then uploaded to the Hugging Face Hub for later use.\nWe will use this preference dataset for fine-tuning with DPO in our upcoming post."
  },
  {
    "objectID": "posts/tutorials/unsloth-qwen-sft-lora.html",
    "href": "posts/tutorials/unsloth-qwen-sft-lora.html",
    "title": "Supervised Fine-Tune Qwen2.5-3B with LoRA & Unsloth",
    "section": "",
    "text": "Large language models (LLMs) are initially trained on vast amounts of unlabeled data to acquire broad general knowledge. However, this pretraining approach has limitations for specialized tasks like Question Answering (QA) due to the facts like (1) The next-token prediction objective used in pretraining is not directly aligned with targeted tasks like QA. (2) General knowledge may be insufficient for domain-specific applications requiring specialized expertise. (3) Publicly available pretraining data may lack up-to-date or proprietary information needed for certain use cases.\nThose senarios are where Supervised Fine-Tuning (SFT) comes into play. It addresses these limitations by adapting pretrained LLMs for specific downstream tasks, by (1) enabling models to learn task-specific patterns and nuances, (2) incorporating domain knowledge not present in general pretraining data, (3) improving performance on targeted applications like QA"
  },
  {
    "objectID": "posts/tutorials/unsloth-qwen-sft-lora.html#sft-pipeline-components",
    "href": "posts/tutorials/unsloth-qwen-sft-lora.html#sft-pipeline-components",
    "title": "Supervised Fine-Tune Qwen2.5-3B with LoRA & Unsloth",
    "section": "SFT Pipeline Components",
    "text": "SFT Pipeline Components\nThe SFT pipeline consists of several key stages and components, illustrated in the following flowchart: \n\nInputs: Including a raw dataset comprising task-specific examples, along with a pretrained base language model.\nInstruction-Dataset Preparation: In this phase, the raw data undergoes a process of refinement and structuring. This stage involves data cleaning and filtering, which ensures the removal of irrelevant, inconsistent, or low-quality examples. Following this, the generation of instruction-answer pairs takes place, transforming the dataset into a form conducive to instructional tasks and allowing the model to learn through guided examples that are relevant to its intended applications.\nDataset Formatting: During this phase, the prepared data is converted into standardized formats to maintain consistency across diverse implementations. These formats include widely-used structures such as JSON configurations modeled on popular frameworks like Alpaca, ShareGPT, and OpenAI formats. Additionally, examples are organized with the aid of structured chat templates, such as those derived from Alpaca, ChatML, and Llama 3, further enhancing the model’s ability to engage in coherent, context-aware dialogues.\nCore SFT Process: This stage builds on the well-structured data to fine-tune the base model. During this stage, the model is trained on the formatted instruction dataset using advanced SFT methodologies. Techniques like full fine-tuning, LoRA (Low-Rank Adaptation), or QLoRA (Quantized LoRA) are employed to optimize the model’s performance while preserving efficiency and adaptability.\nOutput: At the final stage, we will obtain task-specific fine-tuned model.\n\nThis pipeline allows for systematic adaptation of LLMs to targeted applications while leveraging their pretrained knowledge.\nNote that the formatted instruction datasets and chat templates provide a unified way to present diverse training examples to the model. If we fine-tune the pretrained base model, we can choose any chat templates. However, if we fine-tune an instruct model, we need to use the sample template."
  },
  {
    "objectID": "posts/tutorials/unsloth-qwen-sft-lora.html#sft-techniques",
    "href": "posts/tutorials/unsloth-qwen-sft-lora.html#sft-techniques",
    "title": "Supervised Fine-Tune Qwen2.5-3B with LoRA & Unsloth",
    "section": "SFT techniques",
    "text": "SFT techniques\nThere are three main types of Supervised Fine-Tuning (SFT) for large language models:\n\nFull Model Fine-Tuning. This approach involves updating all parameters of the pre-trained model. It offers maximum flexibility in adapting the model to specialized tasks. It often yields significant performance improvements but requires substantial computational resources.\nFeature-Based Fine-Tuning. This method focuses on extracting features from the pre-trained model and used as input for another model or classifier. The main pre-trained model remains unchanged. It’s less resource-intensive and provides faster results, making it suitable when computational power is limited.\nParameter-Efficient Fine-Tuning (PEFT). PEFT techniques aim to fine-tune models more efficiently. Only a portion of the model’s weights are modified, leaving the fundamental language understanding intact. It adds task-specific layers or adapters to the pre-trained model. Significantly reduces computational costs compared to full fine-tuning while still achieving competitive performance.\n\nThe choice between these approaches is often based on the specific requirements of the task, available computational resources, and desired model performance.\nIn this article, we will discuss more about the two most popular and effective PEFT techniques: LoRA and QLoRA."
  },
  {
    "objectID": "posts/tutorials/unsloth-qwen-sft-lora.html#peft-with-lora-and-qlora",
    "href": "posts/tutorials/unsloth-qwen-sft-lora.html#peft-with-lora-and-qlora",
    "title": "Supervised Fine-Tune Qwen2.5-3B with LoRA & Unsloth",
    "section": "PEFT with LoRA and QLoRA",
    "text": "PEFT with LoRA and QLoRA\nLoRA (Low-Rank Adaptation) is introduced in 2021 in the paper “LoRA: Low-Rank Adaptation of Large Language Models” by Adward et al.. It then has gained widespread adoption. It is a cost-effective and efficient method for adapting pretrained language models to specific tasks by freezing most of the model’s parameters and updating only a small number of task-specific weights. This approach leverages adapters to reduce the training overhead, making it an attractive solution for limited compute scenarios.\nQLoRA (Quantized Low-Rank Adaptation) is an extension of the LoRA technique. It is proposed in the paper “QLoRA: Efficient Finetuning of Quantized LLMs” by Tim et al. in 2023. It quantizes the weight of each pretrained parameter to 4 bits (from the typical 32 bits). This results in significant memory savings and enables running large language models on a single GPU\nWhen deciding between LoRA and QLoRA for fine-tuning large language models, key considerations revolve around hardware, model size, speed, and accuracy needs.\nLoRA generally requires more GPU memory than QLoRA but is more efficient than full fine-tuning, making it suitable for systems with moderate to high GPU memory capacity. QLoRA, on the other hand, significantly lowers memory demands, making it more suitable for devices with limited memory resources. While LoRA is often faster, QLoRA incurs slight speed trade-offs due to quantization steps but offers superior memory efficiency, enabling fine-tuning of larger models on constrained hardware.\nAccuracy and computational efficiency also differ between the two methods. LoRA typically yields stable and precise results, whereas QLoRA’s use of quantization may lead to minor accuracy losses, though it can sometimes reduce overfitting. When it comes to specific needs, LoRA is ideal if preserving full model precision is vital, whereas QLoRA shines for extremely large models or environments with tight memory constraints. QLoRA also supports varying levels of quantization (e.g., 8-bit, 4-bit, or even 2-bit), adding flexibility but at the cost of increased implementation complexity.\nTo implement LoRA and QLoRA in practice, among other frameworks like PEFT/Bitsandbytes (Hugging Face), TorchTune, Axolotl, …, we use the Unsloth framework in this article. This is an innovative open-source framework designed to revolutionize the fine-tuning and training of large language models. So it is worth to discuss more about Unsloth in the next section."
  },
  {
    "objectID": "posts/tutorials/unsloth-qwen-sft-lora.html#why-unsloth",
    "href": "posts/tutorials/unsloth-qwen-sft-lora.html#why-unsloth",
    "title": "Supervised Fine-Tune Qwen2.5-3B with LoRA & Unsloth",
    "section": "Why Unsloth?",
    "text": "Why Unsloth?\nUnsloth is developed by Daniel Han and Michael Han at Unsloth AI. This framework addresses some of the most significant challenges in LLM training, particularly speed and resource efficiency. Let’s check out some of its remarkable features and benefits:\n\nSpeed Improvements. It makes an impressive acceleration in training speed, up to 30 times faster performance compared to other advanced methods like Flash Attention 2 (FA2), completing tasks like the Alpaca benchmark in just 3 hours instead of the usual 85. This dramatic reduction in training time allows us to iterate more quickly.\nMemory Efficiency. It achieves up to 90% reduction in memory usage compared to FA2.\nAccuracy Preservation and Enhancement. Despite its focus on speed and efficiency, Unsloth maintains model accuracy with 0% performance loss, or up to 20% increase in accuracy using their MAX offering.\nHardware Flexibility. It is designed to be hardware-agnostic, supporting a wide range of GPUs including those from NVIDIA, AMD, and Intel. This compatibility ensures that users can leverage Unsloth’s benefits regardless of their existing hardware setup."
  },
  {
    "objectID": "posts/tutorials/unsloth-qwen-sft-lora.html#use-case",
    "href": "posts/tutorials/unsloth-qwen-sft-lora.html#use-case",
    "title": "Supervised Fine-Tune Qwen2.5-3B with LoRA & Unsloth",
    "section": "Use-case",
    "text": "Use-case\nIn this article, we illustrate a specific use-case: Supervised fine-tuning Qwen2.5-3B model using LoRA and QLoRA, to create a story generator for children.\nFor the instruction dataset preparation stage, we use an instruction dataset TinyStories_Instruction which contains instruction-story pairs. I have prepared this dataset in my previsous post, if you have not read it yet, I recommend you to check it out. The stories in this dataset are short and synthetically generated by GPT-3.5 and GPT-4 with a limited vocabulary, making it highly suitable for our intended 5-year-old readers. While, the instruction corresponding to each story is also created synthetically using GPT-4o-mini.\nFor the pretrained language model, we use Qwen2.5-3B, a pretrained language model containing 3.09 billion parameters. We choose this for our use-case as its reasonable size, making it powerful yet suitable for fine-tuning even on resource-constrained platforms like Google Colab.\nFor the implementation part, we leverage Unsloth for speed and memory efficiency reasons."
  },
  {
    "objectID": "posts/tutorials/unsloth-qwen-sft-lora.html#fine-tuning-implementation",
    "href": "posts/tutorials/unsloth-qwen-sft-lora.html#fine-tuning-implementation",
    "title": "Supervised Fine-Tune Qwen2.5-3B with LoRA & Unsloth",
    "section": "Fine-Tuning Implementation",
    "text": "Fine-Tuning Implementation\nTo achieve the fine-tuning, we will utilize the following libraries and methods:\n1. Import Necessary Libraries\nimport os\nimport comet_ml\nimport torch\nfrom trl import SFTTrainer\nfrom datasets import load_dataset, concatenate_datasets\nfrom transformers import TrainingArguments, TextStreamer\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\nfrom google.colab import userdata\n2. Comet ML Login\nWe leverage Comet ML for real-time monitoring and tracking our fine-tuning experiments. Comet ML allows you to automatically track a wide range of metrics, parameters, and artifacts during the model training process. This includes training loss, gradient norms, hyperparameters, code versions, and more.\nIn addition, Comet ML makes it easy to compare different experiments, helping you understand how changes in code, hyperparameters, or data affect model performance. The platform provides workspaces and sharing capabilities, enabling teams to collaborate more effectively on ML projects. To dicover more about its features and benefits, please check out Comet ML’s website.\ncomet_ml.login(project_name=\"sft-lora-unsloth\")\n3. Load Pretrained Model and Tokenizer\nNext, we use FastLanguageModel class from Unsloth with the .from_pretrained() method to load Qwen2.5-3B model and its corresponding tokenizer. We specify the max sequence length as 2048 in this use-case. Then, the load_in_4bit argument indicates if we want to use QLoRA (assign True), else LoRA.\nmax_seq_length = 2048\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"Qwen/Qwen2.5-3B\",\n    max_seq_length=max_seq_length,\n    load_in_4bit=False,\n    )\n4. Apply LoRA Adaptation\nThen, we set up LoRA configurations for our loaded model, including the rank r as 32, alpha as 32, no dropout and target modules as linear layers. This is where leveraging experiment tracking and comparison, we can apply hyperparameter tuning to find out the best set of parameters.\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=32,\n    lora_alpha=32,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n    )\n5. Formatting Dataset\nNext, we need to load, format and map the instruction dataset into a specific text template, using Alpaca template in this example.\n# Get Instruction Dataset\ndataset = load_dataset(\"tanquangduong/TinyStories_Instruction\", split=\"train\")\n\n# Template for formatting instruction-output pairs following Alpaca dataset format\nalpaca_template = \"\"\"Below is an instruction that describes a task.\nWrite a response that appropriately completes the request.\n### Instruction:\n{}\n### Response:\n{}\"\"\"\n\n# Get the end of sequence token from the tokenizer\nEOS_TOKEN = tokenizer.eos_token\n\ndef format_samples(examples):\n    \"\"\"\n    Format instruction-output pairs into training samples.\n    Args:\n        examples: Dictionary containing 'instruction' and 'output' lists\n    Returns:\n        Dictionary with formatted 'text' list\n    \"\"\"\n    text = []\n    # Zip instruction-output pairs together and format each pair\n    for instruction, output in zip(examples[\"instruction\"], examples[\"output\"], strict=False):\n        # Insert instruction & output into template and append EOS token\n        message = alpaca_template.format(instruction, output) + EOS_TOKEN\n        text.append(message)\n\n    return {\"text\": text}\n\n# Apply formatting to entire dataset:\n# - Process in batches for efficiency\n# - Remove original columns since they're now formatted into 'text'\ndataset = dataset.map(format_samples, batched=True, remove_columns=dataset.column_names)\n6. Setting Up the Trainer and Lauching the Training\nWhen the instruction dataset is formatted and prepared, and the model is loaded with adapted parameters and architectures (e.g., LoRA or QLoRA), we utilize the SFTTrainer class from the TRL library for supervised fine-tuning.\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=True,\n    args=TrainingArguments(\n        learning_rate=1e-5,\n        lr_scheduler_type=\"linear\",\n        per_device_train_batch_size=2,\n        per_device_eval_batch_size=2,\n        gradient_accumulation_steps=8,\n        num_train_epochs=3,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=1,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        warmup_steps=10,\n        output_dir=\"output\",\n        report_to=\"comet_ml\",\n        seed=0,\n        ),\n    )\ntrainer.train()\n7. Experiment tracking\nDuring training, we can track the training loss or other metrics using the Comet ML platform. The plots should look like the following:\n\n\n\nML Experiment Tracking using Comet ML\n\n\n8. Model Inference\nWhen the fine-tuning is finished, we can perform a quick test on the fine-tuned model.\n# Switch model to inference mode (disables training-specific components)\nFastLanguageModel.for_inference(model)\n\n# Format the story prompt using Alpaca template\nmessage = alpaca_template.format(\"Write a story about a humble little bunny \\\nnamed Ben who follows a mysterious trail in the woods, \\\ndiscovering beautiful flowers, new friends, and a lovely pond along the way.\", \"\")\n\n# Convert text to tokens, create PyTorch tensors, and move to GPU\ninputs = tokenizer([message], return_tensors=\"pt\").to(\"cuda\")\n\n# Initialize streamer for real-time token-by-token text output\ntext_streamer = TextStreamer(tokenizer)\n\n# Generate text from the model:\n# - streamer: Enables streaming output\n# - max_new_tokens: Limits response length\n# - use_cache: Enables KV-cache for faster generation\n# Result assigned to _ since we only care about streamed output\n_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=256, use_cache=True)\nExample of inference output looks like this:\n\n\n\nInference Output Example\n\n\n9. Save and Push to Hugging Face Hub\nNow, if we are satisfied with the fine-tuned model’s performance, it’s time to log in and push it to the Hugging Face Hub for later use.\nfrom huggingface_hub import login\n# Log in to the Hugging Face Hub\nlogin(token=userdata.get('HF_TOKEN'))\n\nmodel.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\nmodel.push_to_hub_merged(\"tanquangduong/Qwen2.5-3B-Instruct-TinyStories\", tokenizer, save_method=\"merged_16bit\")"
  },
  {
    "objectID": "posts/tutorials/unsloth-qwen-sft-lora.html#conclusion",
    "href": "posts/tutorials/unsloth-qwen-sft-lora.html#conclusion",
    "title": "Supervised Fine-Tune Qwen2.5-3B with LoRA & Unsloth",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we discuss fine-tuning LLMs for specialized tasks, such as Question Answering for a story generator, which general pretraining often falls short of due to limited instructive data and objectives. Supervised Fine-Tuning addresses this by refining LLMs using instruction datasets, structured formats, and techniques like LoRA and QLoRA to optimize performance and resource efficiency. LoRA focuses on selective parameter tuning, while QLoRA adds memory-efficient quantization, making it suitable for constrained hardware.\nAdditionally, we utilize the Unsloth framework for efficient and fast fine-tuning, Hugging Face’s TRL for setting up the training process, Comet ML for real-time tracking of fine-tuning experiments, and Hugging Face Hub for dataset and model storage and access.\nWe demonstrated through an example of adapting a custom instruction dataset to fine-tune the Qwen2.5-3B model, resulting in a fine-tuned model that functions as a story generator capable of creating children’s stories based on a simple instruction prompt."
  }
]