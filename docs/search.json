[
  {
    "objectID": "posts/unsloth-qwen-sft-lora.html",
    "href": "posts/unsloth-qwen-sft-lora.html",
    "title": "Supervised Fine-Tune Qwen2.5-3B with LoRA & Unsloth",
    "section": "",
    "text": "Large language models (LLMs) are initially trained on vast amounts of unlabeled data to acquire broad general knowledge. However, this pretraining approach has limitations for specialized tasks like Question Answering (QA) due to the facts like (1) The next-token prediction objective used in pretraining is not directly aligned with targeted tasks like QA. (2) General knowledge may be insufficient for domain-specific applications requiring specialized expertise. (3) Publicly available pretraining data may lack up-to-date or proprietary information needed for certain use cases.\nThose senarios are where Supervised Fine-Tuning (SFT) comes into play. It addresses these limitations by adapting pretrained LLMs for specific downstream tasks, by (1) enabling models to learn task-specific patterns and nuances, (2) incorporating domain knowledge not present in general pretraining data, (3) improving performance on targeted applications like QA",
    "crumbs": [
      "üìùPosts",
      "‚ö° **Fine-Tuning**",
      "1. Supervised Fine-Tuning"
    ]
  },
  {
    "objectID": "posts/unsloth-qwen-sft-lora.html#sft-pipeline-components",
    "href": "posts/unsloth-qwen-sft-lora.html#sft-pipeline-components",
    "title": "Supervised Fine-Tune Qwen2.5-3B with LoRA & Unsloth",
    "section": "SFT Pipeline Components",
    "text": "SFT Pipeline Components\nThe SFT pipeline consists of several key stages and components, illustrated in the following flowchart: \n\nInputs: Including a raw dataset comprising task-specific examples, along with a pretrained base language model.\nInstruction-Dataset Preparation: In this phase, the raw data undergoes a process of refinement and structuring. This stage involves data cleaning and filtering, which ensures the removal of irrelevant, inconsistent, or low-quality examples. Following this, the generation of instruction-answer pairs takes place, transforming the dataset into a form conducive to instructional tasks and allowing the model to learn through guided examples that are relevant to its intended applications.\nDataset Formatting: During this phase, the prepared data is converted into standardized formats to maintain consistency across diverse implementations. These formats include widely-used structures such as JSON configurations modeled on popular frameworks like Alpaca, ShareGPT, and OpenAI formats. Additionally, examples are organized with the aid of structured chat templates, such as those derived from Alpaca, ChatML, and Llama 3, further enhancing the model‚Äôs ability to engage in coherent, context-aware dialogues.\nCore SFT Process: This stage builds on the well-structured data to fine-tune the base model. During this stage, the model is trained on the formatted instruction dataset using advanced SFT methodologies. Techniques like full fine-tuning, LoRA (Low-Rank Adaptation), or QLoRA (Quantized LoRA) are employed to optimize the model‚Äôs performance while preserving efficiency and adaptability.\nOutput: At the final stage, we will obtain task-specific fine-tuned model.\n\nThis pipeline allows for systematic adaptation of LLMs to targeted applications while leveraging their pretrained knowledge.\nNote that the formatted instruction datasets and chat templates provide a unified way to present diverse training examples to the model. If we fine-tune the pretrained base model, we can choose any chat templates. However, if we fine-tune an instruct model, we need to use the sample template.",
    "crumbs": [
      "üìùPosts",
      "‚ö° **Fine-Tuning**",
      "1. Supervised Fine-Tuning"
    ]
  },
  {
    "objectID": "posts/unsloth-qwen-sft-lora.html#sft-techniques",
    "href": "posts/unsloth-qwen-sft-lora.html#sft-techniques",
    "title": "Supervised Fine-Tune Qwen2.5-3B with LoRA & Unsloth",
    "section": "SFT techniques",
    "text": "SFT techniques\nThere are three main types of Supervised Fine-Tuning (SFT) for large language models:\n\nFull Model Fine-Tuning. This approach involves updating all parameters of the pre-trained model. It offers maximum flexibility in adapting the model to specialized tasks. It often yields significant performance improvements but requires substantial computational resources.\nFeature-Based Fine-Tuning. This method focuses on extracting features from the pre-trained model and used as input for another model or classifier. The main pre-trained model remains unchanged. It‚Äôs less resource-intensive and provides faster results, making it suitable when computational power is limited.\nParameter-Efficient Fine-Tuning (PEFT). PEFT techniques aim to fine-tune models more efficiently. Only a portion of the model‚Äôs weights are modified, leaving the fundamental language understanding intact. It adds task-specific layers or adapters to the pre-trained model. Significantly reduces computational costs compared to full fine-tuning while still achieving competitive performance.\n\nThe choice between these approaches is often based on the specific requirements of the task, available computational resources, and desired model performance.\nIn this article, we will discuss more about the two most popular and effective PEFT techniques: LoRA and QLoRA.",
    "crumbs": [
      "üìùPosts",
      "‚ö° **Fine-Tuning**",
      "1. Supervised Fine-Tuning"
    ]
  },
  {
    "objectID": "posts/unsloth-qwen-sft-lora.html#peft-with-lora-and-qlora",
    "href": "posts/unsloth-qwen-sft-lora.html#peft-with-lora-and-qlora",
    "title": "Supervised Fine-Tune Qwen2.5-3B with LoRA & Unsloth",
    "section": "PEFT with LoRA and QLoRA",
    "text": "PEFT with LoRA and QLoRA\nLoRA (Low-Rank Adaptation) is introduced in 2021 in the paper ‚ÄúLoRA: Low-Rank Adaptation of Large Language Models‚Äù by Adward et al.. It then has gained widespread adoption. It is a cost-effective and efficient method for adapting pretrained language models to specific tasks by freezing most of the model‚Äôs parameters and updating only a small number of task-specific weights. This approach leverages adapters to reduce the training overhead, making it an attractive solution for limited compute scenarios.\nQLoRA (Quantized Low-Rank Adaptation) is an extension of the LoRA technique. It is proposed in the paper ‚ÄúQLoRA: Efficient Finetuning of Quantized LLMs‚Äù by Tim et al.¬†in 2023. It quantizes the weight of each pretrained parameter to 4 bits (from the typical 32 bits). This results in significant memory savings and enables running large language models on a single GPU\nWhen deciding between LoRA and QLoRA for fine-tuning large language models, key considerations revolve around hardware, model size, speed, and accuracy needs.\nLoRA generally requires more GPU memory than QLoRA but is more efficient than full fine-tuning, making it suitable for systems with moderate to high GPU memory capacity. QLoRA, on the other hand, significantly lowers memory demands, making it more suitable for devices with limited memory resources. While LoRA is often faster, QLoRA incurs slight speed trade-offs due to quantization steps but offers superior memory efficiency, enabling fine-tuning of larger models on constrained hardware.\nAccuracy and computational efficiency also differ between the two methods. LoRA typically yields stable and precise results, whereas QLoRA‚Äôs use of quantization may lead to minor accuracy losses, though it can sometimes reduce overfitting. When it comes to specific needs, LoRA is ideal if preserving full model precision is vital, whereas QLoRA shines for extremely large models or environments with tight memory constraints. QLoRA also supports varying levels of quantization (e.g., 8-bit, 4-bit, or even 2-bit), adding flexibility but at the cost of increased implementation complexity.\nTo implement LoRA and QLoRA in practice, among other frameworks like PEFT/Bitsandbytes (Hugging Face), TorchTune, Axolotl, ‚Ä¶, we use the Unsloth framework in this article. This is an innovative open-source framework designed to revolutionize the fine-tuning and training of large language models. So it is worth to discuss more about Unsloth in the next section.",
    "crumbs": [
      "üìùPosts",
      "‚ö° **Fine-Tuning**",
      "1. Supervised Fine-Tuning"
    ]
  },
  {
    "objectID": "posts/unsloth-qwen-sft-lora.html#why-unsloth",
    "href": "posts/unsloth-qwen-sft-lora.html#why-unsloth",
    "title": "Supervised Fine-Tune Qwen2.5-3B with LoRA & Unsloth",
    "section": "Why Unsloth?",
    "text": "Why Unsloth?\nUnsloth is developed by Daniel Han and Michael Han at Unsloth AI. This framework addresses some of the most significant challenges in LLM training, particularly speed and resource efficiency. Let‚Äôs check out some of its remarkable features and benefits:\n\nSpeed Improvements. It makes an impressive acceleration in training speed, up to 30 times faster performance compared to other advanced methods like Flash Attention 2 (FA2), completing tasks like the Alpaca benchmark in just 3 hours instead of the usual 85. This dramatic reduction in training time allows us to iterate more quickly.\nMemory Efficiency. It achieves up to 90% reduction in memory usage compared to FA2.\nAccuracy Preservation and Enhancement. Despite its focus on speed and efficiency, Unsloth maintains model accuracy with 0% performance loss, or up to 20% increase in accuracy using their MAX offering.\nHardware Flexibility. It is designed to be hardware-agnostic, supporting a wide range of GPUs including those from NVIDIA, AMD, and Intel. This compatibility ensures that users can leverage Unsloth‚Äôs benefits regardless of their existing hardware setup.",
    "crumbs": [
      "üìùPosts",
      "‚ö° **Fine-Tuning**",
      "1. Supervised Fine-Tuning"
    ]
  },
  {
    "objectID": "posts/unsloth-qwen-sft-lora.html#use-case",
    "href": "posts/unsloth-qwen-sft-lora.html#use-case",
    "title": "Supervised Fine-Tune Qwen2.5-3B with LoRA & Unsloth",
    "section": "Use-case",
    "text": "Use-case\nIn this article, we illustrate a specific use-case: Supervised fine-tuning Qwen2.5-3B model using LoRA and QLoRA, to create a story generator for children.\nFor the instruction dataset preparation stage, we use an instruction dataset TinyStories_Instruction which contains instruction-story pairs. I have prepared this dataset in my previsous post, if you have not read it yet, I recommend you to check it out. The stories in this dataset are short and synthetically generated by GPT-3.5 and GPT-4 with a limited vocabulary, making it highly suitable for our intended 5-year-old readers. While, the instruction corresponding to each story is also created synthetically using GPT-4o-mini.\nFor the pretrained language model, we use Qwen2.5-3B, a pretrained language model containing 3.09 billion parameters. We choose this for our use-case as its reasonable size, making it powerful yet suitable for fine-tuning even on resource-constrained platforms like Google Colab.\nFor the implementation part, we leverage Unsloth for speed and memory efficiency reasons.",
    "crumbs": [
      "üìùPosts",
      "‚ö° **Fine-Tuning**",
      "1. Supervised Fine-Tuning"
    ]
  },
  {
    "objectID": "posts/unsloth-qwen-sft-lora.html#fine-tuning-implementation",
    "href": "posts/unsloth-qwen-sft-lora.html#fine-tuning-implementation",
    "title": "Supervised Fine-Tune Qwen2.5-3B with LoRA & Unsloth",
    "section": "Fine-Tuning Implementation",
    "text": "Fine-Tuning Implementation\nTo achieve the fine-tuning, we will utilize the following libraries and methods:\n1. Import Necessary Libraries\nimport os\nimport comet_ml\nimport torch\nfrom trl import SFTTrainer\nfrom datasets import load_dataset, concatenate_datasets\nfrom transformers import TrainingArguments, TextStreamer\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\nfrom google.colab import userdata\n2. Comet ML Login\nWe leverage Comet ML for real-time monitoring and tracking our fine-tuning experiments. Comet ML allows you to automatically track a wide range of metrics, parameters, and artifacts during the model training process. This includes training loss, gradient norms, hyperparameters, code versions, and more.\nIn addition, Comet ML makes it easy to compare different experiments, helping you understand how changes in code, hyperparameters, or data affect model performance. The platform provides workspaces and sharing capabilities, enabling teams to collaborate more effectively on ML projects. To dicover more about its features and benefits, please check out Comet ML‚Äôs website.\ncomet_ml.login(project_name=\"sft-lora-unsloth\")\n3. Load Pretrained Model and Tokenizer\nNext, we use FastLanguageModel class from Unsloth with the .from_pretrained() method to load Qwen2.5-3B model and its corresponding tokenizer. We specify the max sequence length as 2048 in this use-case. Then, the load_in_4bit argument indicates if we want to use QLoRA (assign True), else LoRA.\nmax_seq_length = 2048\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"Qwen/Qwen2.5-3B\",\n    max_seq_length=max_seq_length,\n    load_in_4bit=False,\n    )\n4. Apply LoRA Adaptation\nThen, we set up LoRA configurations for our loaded model, including the rank r as 32, alpha as 32, no dropout and target modules as linear layers. This is where leveraging experiment tracking and comparison, we can apply hyperparameter tuning to find out the best set of parameters.\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=32,\n    lora_alpha=32,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n    )\n5. Formatting Dataset\nNext, we need to load, format and map the instruction dataset into a specific text template, using Alpaca template in this example.\n# Get Instruction Dataset\ndataset = load_dataset(\"tanquangduong/TinyStories_Instruction\", split=\"train\")\n\n# Template for formatting instruction-output pairs following Alpaca dataset format\nalpaca_template = \"\"\"Below is an instruction that describes a task.\nWrite a response that appropriately completes the request.\n### Instruction:\n{}\n### Response:\n{}\"\"\"\n\n# Get the end of sequence token from the tokenizer\nEOS_TOKEN = tokenizer.eos_token\n\ndef format_samples(examples):\n    \"\"\"\n    Format instruction-output pairs into training samples.\n    Args:\n        examples: Dictionary containing 'instruction' and 'output' lists\n    Returns:\n        Dictionary with formatted 'text' list\n    \"\"\"\n    text = []\n    # Zip instruction-output pairs together and format each pair\n    for instruction, output in zip(examples[\"instruction\"], examples[\"output\"], strict=False):\n        # Insert instruction & output into template and append EOS token\n        message = alpaca_template.format(instruction, output) + EOS_TOKEN\n        text.append(message)\n\n    return {\"text\": text}\n\n# Apply formatting to entire dataset:\n# - Process in batches for efficiency\n# - Remove original columns since they're now formatted into 'text'\ndataset = dataset.map(format_samples, batched=True, remove_columns=dataset.column_names)\n6. Setting Up the Trainer and Lauching the Training\nWhen the instruction dataset is formatted and prepared, and the model is loaded with adapted parameters and architectures (e.g., LoRA or QLoRA), we utilize the SFTTrainer class from the TRL library for supervised fine-tuning.\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=True,\n    args=TrainingArguments(\n        learning_rate=1e-5,\n        lr_scheduler_type=\"linear\",\n        per_device_train_batch_size=2,\n        per_device_eval_batch_size=2,\n        gradient_accumulation_steps=8,\n        num_train_epochs=3,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=1,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        warmup_steps=10,\n        output_dir=\"output\",\n        report_to=\"comet_ml\",\n        seed=0,\n        ),\n    )\ntrainer.train()\n7. Experiment tracking\nDuring training, we can track the training loss or other metrics using the Comet ML platform. The plots should look like the following:\n\n\n\nML Experiment Tracking using Comet ML\n\n\n8. Model Inference\nWhen the fine-tuning is finished, we can perform a quick test on the fine-tuned model.\n# Switch model to inference mode (disables training-specific components)\nFastLanguageModel.for_inference(model)\n\n# Format the story prompt using Alpaca template\nmessage = alpaca_template.format(\"Write a story about a humble little bunny \\\nnamed Ben who follows a mysterious trail in the woods, \\\ndiscovering beautiful flowers, new friends, and a lovely pond along the way.\", \"\")\n\n# Convert text to tokens, create PyTorch tensors, and move to GPU\ninputs = tokenizer([message], return_tensors=\"pt\").to(\"cuda\")\n\n# Initialize streamer for real-time token-by-token text output\ntext_streamer = TextStreamer(tokenizer)\n\n# Generate text from the model:\n# - streamer: Enables streaming output\n# - max_new_tokens: Limits response length\n# - use_cache: Enables KV-cache for faster generation\n# Result assigned to _ since we only care about streamed output\n_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=256, use_cache=True)\nExample of inference output looks like this:\n\n\n\nInference Output Example\n\n\n9. Save and Push to Hugging Face Hub\nNow, if we are satisfied with the fine-tuned model‚Äôs performance, it‚Äôs time to log in and push it to the Hugging Face Hub for later use.\nfrom huggingface_hub import login\n# Log in to the Hugging Face Hub\nlogin(token=userdata.get('HF_TOKEN'))\n\nmodel.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\nmodel.push_to_hub_merged(\"tanquangduong/Qwen2.5-3B-Instruct-TinyStories\", tokenizer, save_method=\"merged_16bit\")",
    "crumbs": [
      "üìùPosts",
      "‚ö° **Fine-Tuning**",
      "1. Supervised Fine-Tuning"
    ]
  },
  {
    "objectID": "posts/unsloth-qwen-sft-lora.html#conclusion",
    "href": "posts/unsloth-qwen-sft-lora.html#conclusion",
    "title": "Supervised Fine-Tune Qwen2.5-3B with LoRA & Unsloth",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we discuss fine-tuning LLMs for specialized tasks, such as Question Answering for a story generator, which general pretraining often falls short of due to limited instructive data and objectives. Supervised Fine-Tuning addresses this by refining LLMs using instruction datasets, structured formats, and techniques like LoRA and QLoRA to optimize performance and resource efficiency. LoRA focuses on selective parameter tuning, while QLoRA adds memory-efficient quantization, making it suitable for constrained hardware.\nAdditionally, we utilize the Unsloth framework for efficient and fast fine-tuning, Hugging Face‚Äôs TRL for setting up the training process, Comet ML for real-time tracking of fine-tuning experiments, and Hugging Face Hub for dataset and model storage and access.\nWe demonstrated through an example of adapting a custom instruction dataset to fine-tune the Qwen2.5-3B model, resulting in a fine-tuned model that functions as a story generator capable of creating children‚Äôs stories based on a simple instruction prompt.",
    "crumbs": [
      "üìùPosts",
      "‚ö° **Fine-Tuning**",
      "1. Supervised Fine-Tuning"
    ]
  },
  {
    "objectID": "posts/preference-dataset-dpo.html",
    "href": "posts/preference-dataset-dpo.html",
    "title": "Create Preference Dataset for DPO Fine-Tuning",
    "section": "",
    "text": "In the realm of Supervised Fine-Tuning (SFT) for custom LLM, Direct Preference Optimization (DPO) is a technique used to align AI-generated outputs with human preferences by optimizing language models.\nTo achieve this, a preference dataset is required, containing data that enables models to understand which responses are preferred by humans and which are not.\nIn this article, we‚Äôll walk through an example to create such a dataset using Python, OpenAI‚Äôs API, and Hugging Face‚Äôs Datasets library.\nLet‚Äôs dive in.",
    "crumbs": [
      "üìùPosts",
      "üß™ **Data Processing**",
      "2. Preference Dataset"
    ]
  },
  {
    "objectID": "posts/preference-dataset-dpo.html#components-of-a-preference-dataset-for-dpo",
    "href": "posts/preference-dataset-dpo.html#components-of-a-preference-dataset-for-dpo",
    "title": "Create Preference Dataset for DPO Fine-Tuning",
    "section": "Components of a Preference Dataset for DPO",
    "text": "Components of a Preference Dataset for DPO\nA preference dataset typically includes:\n\nPrompts: Inputs or questions given to the AI model.\nChosen Responses: Responses preferred by human evaluators.\nRejected Responses: Less preferred responses or responses not selected by human evaluators.\n\nBy providing this structure, the dataset allows a model to learn which responses are preferable, making it better aligned with human preferences.",
    "crumbs": [
      "üìùPosts",
      "üß™ **Data Processing**",
      "2. Preference Dataset"
    ]
  },
  {
    "objectID": "posts/preference-dataset-dpo.html#our-use-case",
    "href": "posts/preference-dataset-dpo.html#our-use-case",
    "title": "Create Preference Dataset for DPO Fine-Tuning",
    "section": "Our use-case",
    "text": "Our use-case\nIn our previous post, we created an instruction dataset, TinyStories_Instruction, from the raw TinyStories dataset. This dataset was specifically designed for fine-tuning a pretrained Large/Small Language Model to develop a story generator tailored to 5-year-olds.\nIn this guide, we take the next step by creating a preference dataset from the previously generated instruction dataset. This dataset is used for fine-tuning a pretrained Large/Small Language Model through Direct Preference Optimization (DPO), enhancing our story generator to align even better with human preferences and produce engaging, age-appropriate content for young children.\nThe process for creating a preference dataset is illustrated below:",
    "crumbs": [
      "üìùPosts",
      "üß™ **Data Processing**",
      "2. Preference Dataset"
    ]
  },
  {
    "objectID": "posts/preference-dataset-dpo.html#implementation",
    "href": "posts/preference-dataset-dpo.html#implementation",
    "title": "Create Preference Dataset for DPO Fine-Tuning",
    "section": "Implementation",
    "text": "Implementation\nThe implementation involves a series of steps: extracting data, generating AI responses, and creating preference triplets.\nWe will first import the required packages.\nimport concurrent.futures\nimport json\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import List, Tuple\nfrom datasets import Dataset, load_dataset, concatenate_datasets\nfrom openai import OpenAI\nfrom tqdm.auto import tqdm\nfrom google.colab import userdata\n1. Data Extraction Function\nThe extract_ground_instruction_story function extracts pairs of instructions and desired outputs from a given dataset.\ndef extract_ground_instruction_story(dataset):\n    return [(example['instruction'], example['output']) for example in dataset]\n2. Creating a PreferenceSet Class\nThe PreferenceSet class manages and stores the triples of (instruction, generated story, desired story).\nclass PreferenceSet:\n    def __init__(self, triples: List[Tuple[str, str, str]]):\n        self.triples = triples\n\n    @classmethod\n    def from_json(cls, json_str: str, instruction, desired_story) -&gt; 'PreferenceSet':\n        data = json.loads(json_str)\n        triples = [(instruction, data['generated_story'], desired_story)]\n        return cls(triples)\n\n    def __iter__(self):\n        return iter(self.triples)\n3. Generating Preference-Response Triplets\nThe function generate_preference_answer_triples generates a story using OpenAI‚Äôs API and returns a preference triple in the format (instruction, generated response, desired response).\ndef generate_preference_answer_triples(instruction: str, desired_story: str, client: OpenAI) -&gt; List[Tuple[str, str, str]]:\n    prompt = f\"\"\"Based on the following instruction, generate a story. \\\n        Story should be no longer than 50 words. Story uses several complex words or structures \\\n        that are not suitable for 5-year-olds.\n\n        Provide your response in JSON format with the following structure:\n        {{\"generated_story\": \"...\"}}\n\n        Instruction:\n        {instruction}\n        \"\"\"\n    completion = client.chat.completions.create(model=\"gpt-4o-mini\",\n                                                    messages=[\n                                                        {\"role\": \"system\",\n                                                        \"content\": \"You are a helpful assistant who \\\n                                                        generates story based on the given instruction. \\\n                                                        Provide your response in JSON format.\",},\n                                                        {\"role\": \"user\", \"content\": prompt},\n                                                        ],\n                                                    response_format={\"type\": \"json_object\"},\n                                                    max_tokens=512,\n                                                    temperature=0.2,)\n    result = PreferenceSet.from_json(completion.choices[0].message.content, instruction, desired_story)\n\n    # Convert to list of tuples\n    return result.triples\n4. Creating the Preference Dataset\nThe function create_preference_dataset creates a dataset using the extracted stories and generated responses.\ndef create_preference_dataset(dataset: Dataset, client: OpenAI, num_workers: int = 4) -&gt; Dataset:\n    stories = extract_ground_instruction_story(dataset)\n    instruction_answer_triples = []\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n        futures = [executor.submit(generate_instruction_answer_triples, instruction, desired_story, client) for instruction, desired_story in stories]\n\n        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n        instruction_answer_triples.extend(future.result())\n\n    instructions, rejected_story, chosen_story = zip(*instruction_answer_triples)\n    return Dataset.from_dict({\n        \"prompt\": list(instructions),\n        \"rejected\": list(rejected_story),\n        \"chosen\": list(chosen_story)\n        })\n5. The main function\nThe main function initializes the OpenAI client, loads the dataset, creates a preference dataset, and uploads it to the Hugging Face Hub.\ndef main() -&gt; Dataset:\n    client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n\n    # 1. Load the raw data\n    # Load the train and test splits\n    train_dataset = load_dataset(\"tanquangduong/TinyStories_Instruction\", split=\"train\")\n    test_dataset = load_dataset(\"tanquangduong/TinyStories_Instruction\", split=\"test\")\n\n    # Combine the datasets\n    raw_dataset = concatenate_datasets([train_dataset, test_dataset])\n\n    print(\"Raw dataset:\")\n    print(raw_dataset.to_pandas())\n\n    # 2. Create preference dataset\n    preference_dataset = create_preference_dataset(raw_dataset, client)\n    print(\"Preference dataset:\")\n    print(preference_dataset.to_pandas())\n\n    # 3. Train/test split and export\n    filtered_dataset = preference_dataset.train_test_split(test_size=0.1)\n    filtered_dataset.push_to_hub(\"tanquangduong/TinyStories_Preference\")\n6. Running the pipeline\nFinally, we authenticate with Hugging Face for later dataset uploading and start running the pipeline.\nfrom huggingface_hub import login\n# Log in to the Hugging Face Hub\nlogin(token=userdata.get('HF_TOKEN'))\n\n# Launch the pipeline to create instruction dataset\nmain()",
    "crumbs": [
      "üìùPosts",
      "üß™ **Data Processing**",
      "2. Preference Dataset"
    ]
  },
  {
    "objectID": "posts/preference-dataset-dpo.html#resulting-preference-dataset",
    "href": "posts/preference-dataset-dpo.html#resulting-preference-dataset",
    "title": "Create Preference Dataset for DPO Fine-Tuning",
    "section": "Resulting Preference Dataset",
    "text": "Resulting Preference Dataset\nAfter running the above pipeline, the resulting preference dataset will look like this:",
    "crumbs": [
      "üìùPosts",
      "üß™ **Data Processing**",
      "2. Preference Dataset"
    ]
  },
  {
    "objectID": "posts/preference-dataset-dpo.html#conclusion",
    "href": "posts/preference-dataset-dpo.html#conclusion",
    "title": "Create Preference Dataset for DPO Fine-Tuning",
    "section": "Conclusion",
    "text": "Conclusion\nThe article outlines the process of creating a Preference Dataset for fine tuning with DPO to align AI-generated outputs with human preferences.\nThe dataset consists of prompts, human-preferred responses, and rejected responses, allowing the model to learn desired behavior. Key steps include extracting instruction-output pairs, generating AI responses using OpenAI‚Äôs API, and organizing the data into preference triplets.\nThe final dataset is then uploaded to the Hugging Face Hub for later use.\nWe will use this preference dataset for fine-tuning with DPO in our upcoming post.",
    "crumbs": [
      "üìùPosts",
      "üß™ **Data Processing**",
      "2. Preference Dataset"
    ]
  },
  {
    "objectID": "posts/domain-specific-chatbot-guardrails.html",
    "href": "posts/domain-specific-chatbot-guardrails.html",
    "title": "Implement Guardrails for Domain-specific Chatbots",
    "section": "",
    "text": "Chatbots are now a key part of how we interact with daily tasks, either answering questions or giving recommendations. But as they become more advanced and widely used, it‚Äôs important to make sure their responses are accurate, appropriate, and relevant to their intended purpose. This is where guardrails come in handy. As they act as safety measures to ensure the chatbot handles both user input and its own responses correctly.\nIn this article, we explore the motivation behind implementing guardrails in a domain-specific chatbot, such as one for food and dish recommendation, the types of guardrails available, trade-offs involved, asynchronous programming for reducing latency, and the importance of adhering to the SOLID principles in code design. Finally, we‚Äôll demonstrate these concepts using sample code snippets.",
    "crumbs": [
      "üìùPosts",
      "üí¨ **AI Chatbot**",
      "2. üõ°Ô∏è Guardrails for Domain-specific Chatbot"
    ]
  },
  {
    "objectID": "posts/domain-specific-chatbot-guardrails.html#guardrails-in-domain-specific-chatbots",
    "href": "posts/domain-specific-chatbot-guardrails.html#guardrails-in-domain-specific-chatbots",
    "title": "Implement Guardrails for Domain-specific Chatbots",
    "section": "Guardrails in Domain-Specific Chatbots",
    "text": "Guardrails in Domain-Specific Chatbots\nIn domain-specific chatbots, without strong safeguards (called guardrails), the chatbot might give irrelevant, incorrect, or insensitive responses, which could frustrate users and harm its credibility. For instance, a chatbot that fails to recognize domain boundaries may provide information outside its scope or respond insensitively to user input, leading to dissatisfaction or reputational harm.\nIn our use-case, for a chatbot focused on food and dish recommendations, the guardrails address issues like out-of-scope topics, culturally insensitive suggestions or ignoring dietary restrictions. Here‚Äôs how these safeguards work:\n\nRelevance: The chatbot sticks to food and dish recommendations, avoiding topics outside its expertise to ensure helpful and accurate responses.\nAppropriateness: Responses are respectful of cultural norms and dietary preferences. For example:\n\nMedical Restrictions: For a diabetic diet, it avoids recommending foods high in sugar or carbs.\nVegetarian Choices: No meat suggestions, but includes options with dairy or eggs, if acceptable.\nCultural Sensitivity: For traditional Asian diets, it prioritizes rice-based dishes with vegetables, fish, and soy, while limiting dairy.\n\n\nThese guardrails ensure the chatbot provides a positive user experience and is seen as a reliable and trustworthy helper in its specific area.",
    "crumbs": [
      "üìùPosts",
      "üí¨ **AI Chatbot**",
      "2. üõ°Ô∏è Guardrails for Domain-specific Chatbot"
    ]
  },
  {
    "objectID": "posts/domain-specific-chatbot-guardrails.html#types-of-guardrails",
    "href": "posts/domain-specific-chatbot-guardrails.html#types-of-guardrails",
    "title": "Implement Guardrails for Domain-specific Chatbots",
    "section": "Types of Guardrails",
    "text": "Types of Guardrails\nGuardrails can be categorized into input guardrails and output guardrails, each serving a unique role in ensuring the chatbot‚Äôs effectiveness.\n\nInput Guardrails focus on validating and managing the user‚Äôs input before processing it:\n\n\nTopical Filtering: Ensures the user‚Äôs query aligns with the chatbot‚Äôs purpose. For instance, a food-focused chatbot would reject questions about cars or unrelated topics.\nJailbreaking Prevention: Protects against attempts to bypass the chatbot‚Äôs intended behavior, such as override its prompting to generate inappropriate content.\nPrompt Injection Defense: Safeguards against maliciously crafted inputs designed to manipulate the chatbot into behaving unexpectedly in any downstream functions.\n\n\nOutput Guardrails ensure the chatbot‚Äôs responses are accurate, relevant, and appropriate:\n\n\nHallucination/Fact-Checking: Using ground truth information or ML-based classifier to identify and minimize instances where the chatbot generates incorrect or made-up information.\nModeration: Screens responses to ensure they are free from offensive, sensitive, or irrelevant content. For example, building food-specific scoring that can evaluate responses based on criteria such as cultural sensitivity or dietary appropriateness.\nSyntax Checks: Checking errors and inconsistencies in output‚Äôs format to make sure that the outputs are suitable for downstream tasks like easy-to-read for answering question or corrected schema format for ‚Äòarguments‚Äô in function calling task.\n\nThese guardrails work together to create a robust system, improving user trust and the overall experience with the chatbot.",
    "crumbs": [
      "üìùPosts",
      "üí¨ **AI Chatbot**",
      "2. üõ°Ô∏è Guardrails for Domain-specific Chatbot"
    ]
  },
  {
    "objectID": "posts/domain-specific-chatbot-guardrails.html#trade-offs-between-accuracy-and-latency",
    "href": "posts/domain-specific-chatbot-guardrails.html#trade-offs-between-accuracy-and-latency",
    "title": "Implement Guardrails for Domain-specific Chatbots",
    "section": "Trade-Offs Between Accuracy and Latency",
    "text": "Trade-Offs Between Accuracy and Latency\nImplementing guardrails introduces a trade-off between accuracy and latency.\nWhile guardrails improve the chatbot‚Äôs ability to provide relevant and high-quality responses, this increased accuracy often comes at a cost. That is higher latency. The added computational steps (e.g., moderation checks, scoring) can slow down the response time.\nStriking a balance between these factors is the key. Using asynchronous programming in Python, as demonstrated in the implementation section, can help mitigate latency issues by parallelizing guardrail checks.",
    "crumbs": [
      "üìùPosts",
      "üí¨ **AI Chatbot**",
      "2. üõ°Ô∏è Guardrails for Domain-specific Chatbot"
    ]
  },
  {
    "objectID": "posts/domain-specific-chatbot-guardrails.html#using-asynchronous-programming-for-reducing-latency",
    "href": "posts/domain-specific-chatbot-guardrails.html#using-asynchronous-programming-for-reducing-latency",
    "title": "Implement Guardrails for Domain-specific Chatbots",
    "section": "Using Asynchronous Programming for Reducing Latency",
    "text": "Using Asynchronous Programming for Reducing Latency\nAsynchronous programming shines in scenarios requiring concurrency, where multiple tasks can be executed independently and simultaneously. This is particularly beneficial for systems that deal with I/O-bound operations, such as network requests or database queries, as it prevents blocking the main execution thread while waiting for responses.\nIn the context of a food and dish recommendation chatbot, asynchronous programming allows input guardrails and output generation task to run in parallel, significantly reducing overall latency. For example, while the chatbot processes a user query for moderation, input validation tasks can execute simultaneously, ensuring both operations are completed efficiently without delay.\nIn general, this concurrency is critical in delivering fast, real-time responses to users, enhancing the chatbot‚Äôs usability and user experience. Moreover, it scales well, allowing multiple requests to be handled concurrently in multi-user environments without overwhelming system resources.",
    "crumbs": [
      "üìùPosts",
      "üí¨ **AI Chatbot**",
      "2. üõ°Ô∏è Guardrails for Domain-specific Chatbot"
    ]
  },
  {
    "objectID": "posts/domain-specific-chatbot-guardrails.html#understanding-our-use-case",
    "href": "posts/domain-specific-chatbot-guardrails.html#understanding-our-use-case",
    "title": "Implement Guardrails for Domain-specific Chatbots",
    "section": "Understanding Our Use-case",
    "text": "Understanding Our Use-case\nIn the context of our use-case with food and dish recommendation chatbot, for the demonstration purpose, we will just implement one input guardrail and one output guardrail for our chatbot system.\nThe input guardrail is designed to ensure that user queries align with the chatbot‚Äôs intended domain and purpose. For example, it filters out irrelevant topics (like questions about cars or unrelated areas).\nOn the other hand, the output guardrail focuses on ensuring the chatbot‚Äôs responses are accurate, appropriate, and aligned with user expectations. This includes screening for culturally sensitive content, adhering to dietary restrictions.",
    "crumbs": [
      "üìùPosts",
      "üí¨ **AI Chatbot**",
      "2. üõ°Ô∏è Guardrails for Domain-specific Chatbot"
    ]
  },
  {
    "objectID": "posts/domain-specific-chatbot-guardrails.html#implementation",
    "href": "posts/domain-specific-chatbot-guardrails.html#implementation",
    "title": "Implement Guardrails for Domain-specific Chatbots",
    "section": "Implementation",
    "text": "Implementation\nBefore jumping to coding step, we think about scalability, maintainability, and extensibility of our implementation.\nAs the chatbot evolves new requirements, such as adding more guardrails or integrating alternative APIs, may emerge. Without a structured approach, changes to the codebase could lead to unnecessary complexity, higher maintenance costs, and bugs.\nThis is where SOLID principles come in handy. The principles ensure that our codebase is modular, easy to understand, and adaptable to future changes without requiring extensive rewrites.\n\nAdhering to SOLID Principles for Scalability and Maintainability\nTo implement input and output guardrails effectively, we break the chatbot system into distinct classes: OpenAIClient, Guardrail, and ChatbotHandler, each adhering to SOLID principles and designed to work seamlessly with asynchronous programming.\n\n\n\n\n\n\nPlayground in Colab\n\n\n\nThe full Colab notebook can be found here.\n\n\nBefore we jump into the function/class implementation, let‚Äôs installing and importing necessary packages:\n# !pip install openai==1.55.0 python-dotenv==1.0.1\nimport asyncio\nimport openai\nfrom dotenv import load_dotenv\nload_dotenv()\n\nOpenAIClient class\n\nThis class acts as the interface to interact with the OpenAI API, handling message formatting and API calls.\nIt uses asynchronous programming to make non-blocking requests to the API, ensuring the chatbot can handle multiple user queries efficiently. This follows the Single Responsibility Principle (SRP) as it focuses solely on managing API interactions.\nBy following the Liskov Substitution Principle (LSP), this class can be replaced with another implementation (e.g., another LLM client) without affecting the rest of the system.\nclass OpenAIClient:\n    def __init__(self, model: str):\n        self.model = model\n\n    async def get_response(self, messages, temperature=0.5):\n        try:\n            response = openai.chat.completions.create(\n                model=self.model, messages=messages, temperature=temperature\n            )\n            return response.choices[0].message.content\n        except Exception as e:\n            print(f\"Error getting response: {e}\")\n            return None\n\nGuardrail class\n\nThis class encapsulates the logic for input and output guardrails. It ensures user input aligns with the chatbot‚Äôs purpose and validates the chatbot‚Äôs responses for appropriateness and relevance.\nIt uses asynchronous tasks to execute topical filtering, moderation, and scoring concurrently, improving performance and responsiveness.\nBy adhering to the Open-Closed Principle (OCP), the guardrail system can be extended with additional checks (e.g., language-specific filters) without modifying existing code.\nFollowing the Dependency Inversion Principle (DIP), Guardrail interacts with an abstract OpenAIClient, ensuring that it does not rely on a specific implementation.\nclass Guardrail:\n    def __init__(self, client: OpenAIClient):\n        self.client = client\n\n    async def check_topical(self, user_request):\n        print(\"Checking topical guardrail\")\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"Your role is to assess whether the user question is allowed or not. The allowed topics are food and dish recommendations. If the topic is allowed, say 'allowed' otherwise say 'not_allowed'.\",\n            },\n            {\"role\": \"user\", \"content\": user_request},\n        ]\n        print(\"Got topical guardrail response\")\n        return await self.client.get_response(messages, temperature=0)\n\n    async def check_moderation(self, domain, criteria, steps, content):\n        print(\"Checking moderation guardrail\")\n        moderation_system_prompt = f\"\"\"\n        You are a moderation assistant. Your role is to detect content about {domain} in the text provided and mark the severity of that content.\n\n        ## Domain: {domain}\n\n        ### Criteria: {criteria}\n\n        ### Instructions: {steps}\n\n        ### Content: {content}\n\n        ### Evaluation (score only!)\n        \"\"\"\n        messages = [{\"role\": \"user\", \"content\": moderation_system_prompt}]\n\n        print(\"Got moderation response\")\n        return await self.client.get_response(messages, temperature=0)\n\nChatbotHandler class\n\nThis is the high-level controller that coordinates input validation, response generation, and output moderation.\nIt orchestrates multiple asynchronous tasks, such as executing guardrails and fetching chatbot responses, to ensure smooth operation without delays.\nThe Interface Segregation Principle (ISP) is respected, as the class uses only the necessary methods from OpenAIClient and Guardrail without relying on unused functionality.\nclass ChatbotHandler:\n    def __init__(self, client: OpenAIClient, guardrail: Guardrail, fallback_responses: dict):\n        self.client = client\n        self.guardrail = guardrail\n        self.system_prompt = \"You are a helpful assistant for food and dish recommendations.\"\n        self.domain = \"dish recommendation\"\n        self.dish_advice_criteria = \"\"\"\n        Assess the presence of explicit recommendation of unhealthy or culturally insensitive dishes in the content.\n        The content should only contain general advice and appropriate suggestions for dishes.\n        \"\"\"\n        self.dish_advice_steps = \"\"\"\n        1. Read the content and the criteria carefully.\n        2. Assess how much inappropriate or culturally insensitive dish recommendation is contained in the content.\n        3. Assign a dish advice score from 1 to 5, with 1 being no inappropriate content, and 5 being multiple instances of such content.\n        \"\"\"\n        self.fallback_responses = fallback_responses\n\n    async def get_chat_response(self, user_request):\n        print(\"Getting LLM response\")\n        messages = [\n            {\"role\": \"system\", \"content\": self.system_prompt},\n            {\"role\": \"user\", \"content\": user_request},\n        ]\n        response = await self.client.get_response(messages)\n\n        if response:\n            print(\"Got LLM response\")\n            return response \n        else:\n            self.fallback_responses[\"api_error\"]\n\n    async def execute_all_guardrails(self, user_request):\n        topical_guardrail_task = asyncio.create_task(self.guardrail.check_topical(user_request))\n        chat_task = asyncio.create_task(self.get_chat_response(user_request))\n\n        while True:\n            done, _ = await asyncio.wait(\n                [topical_guardrail_task, chat_task], return_when=asyncio.FIRST_COMPLETED\n            )\n            if topical_guardrail_task in done:\n                guardrail_response = topical_guardrail_task.result()\n                if guardrail_response == \"not_allowed\":\n                    chat_task.cancel()\n                    print(\"Topical guardrail triggered\")\n                    return self.fallback_responses[\"content_policy\"]\n                elif chat_task in done:\n                    print('Passed topical guardrail')\n                    chat_response = chat_task.result()\n                    moderation_response = await self.guardrail.check_moderation(\n                        self.domain, self.dish_advice_criteria, self.dish_advice_steps, chat_response\n                    )\n                    if int(moderation_response) &gt;= 3:\n                        print(f\"Moderation guardrail flagged with a score of {int(moderation_response)}\")\n                        return self.fallback_responses[\"content_policy\"]\n                    else:\n                        print('Passed moderation')\n                        return chat_response\n            else:\n                await asyncio.sleep(0.1)\n\nOur main function\n\nFinally, the main function serves as the entry point for testing the chatbot‚Äôs functionality, orchestrating the interactions between the key components (OpenAIClient, Guardrail, and ChatbotHandler).\n\nFirst, it sets up the necessary components of the chatbot, such as the client for interacting with the language model, guardrails for input/output validation, and fallback responses for handling errors.\nThen, the function tests the chatbot with a variety of user queries to simulate real-world interactions, covering both valid and invalid scenarios.\n\nasync def main():\n    # Set up LLM model name and input queries\n    GPT_MODEL = 'gpt-4o-mini'\n    bad_request = \"Tell me about cars\"\n    good_request = \"What are some good vegetarian dishes to try?\"\n    great_request = \"Can you suggest some easy Italian recipes for a beginner?\"        \n    tests = [good_request, bad_request, great_request]\n\n    # Initializing chatbot components\n    client = OpenAIClient(GPT_MODEL)\n    guardrail = Guardrail(client)\n    fallback_responses = {\n        \"unclear_input\": \"I'm sorry, I didn't understand that. Could you please rephrase?\",\n        \"api_error\": \"I'm having trouble processing your request. Please try again later.\",\n        \"content_policy\": \"I'm not able to respond to that type of request.\",\n    }\n    handler = ChatbotHandler(client, guardrail, fallback_responses)\n\n    # Testing\n    for test in tests:\n        response = await handler.execute_all_guardrails(test)\n        print(\"*\" * 10)\n        print(response)\n        print(\"*\" * 50)\n\nif __name__ == '__main__':\n    asyncio.run(main())\nThe output looks like this:\nChecking topical guardrail\nGot topical guardrail response\nGetting LLM response\nGot LLM response\nPassed topical guardrail\nChecking moderation guardrail\nGot moderation response\nPassed moderation\n**********\nThere are many delicious vegetarian dishes from various cuisines around the world. Here are some popular options to consider:\n\n1. **Chana Masala** - A flavorful Indian dish made with chickpeas cooked in a spicy tomato-based sauce, often served with rice or naan.\n\n2. **Vegetable Stir-Fry** - A quick and healthy dish made with a mix of colorful vegetables, tofu, and a savory sauce, served over rice or noodles.\n...\n\nThese dishes are not only vegetarian but also packed with flavor and nutrients. Enjoy exploring these culinary delights!\n**************************************************\n\n\nChecking topical guardrail\nGot topical guardrail response\nGetting LLM response\nGot LLM response\nTopical guardrail triggered\n**********\nI'm not able to respond to that type of request.\n\n\n**************************************************\nChecking topical guardrail\nGot topical guardrail response\nGetting LLM response\nGot LLM response\nPassed topical guardrail\nChecking moderation guardrail\nGot moderation response\nPassed moderation\n**********\nAbsolutely! Here are three easy Italian recipes that are perfect for beginners:\n\n### 1. **Spaghetti Aglio e Olio**\n\n**Ingredients:**\n- 400g spaghetti\n- 4 cloves garlic, thinly sliced\n- 1/2 cup extra virgin olive oil\n...\n\nThese recipes are simple and delicious, making them perfect for beginners. Enjoy your cooking!    \n**************************************************",
    "crumbs": [
      "üìùPosts",
      "üí¨ **AI Chatbot**",
      "2. üõ°Ô∏è Guardrails for Domain-specific Chatbot"
    ]
  },
  {
    "objectID": "posts/domain-specific-chatbot-guardrails.html#conclusion",
    "href": "posts/domain-specific-chatbot-guardrails.html#conclusion",
    "title": "Implement Guardrails for Domain-specific Chatbots",
    "section": "Conclusion",
    "text": "Conclusion\nTo wrap up, guardrails are essential in creating robust, user-centric chatbots that align with their intended purpose.\nThey ensure relevance, appropriateness, and accuracy while protecting against user dissatisfaction or ethical missteps.\nWhile the trade-off between accuracy and latency is unavoidable, leveraging asynchronous programming can optimize performance. Adhering to the SOLID principles ensures that the chatbot‚Äôs architecture remains scalable and maintainable as requirements evolve.\nBy integrating input and output guardrails thoughtfully, as demonstrated in the code snippets, we can build reliable and user-friendly chatbots that excel in delivering high-quality interactions.",
    "crumbs": [
      "üìùPosts",
      "üí¨ **AI Chatbot**",
      "2. üõ°Ô∏è Guardrails for Domain-specific Chatbot"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "‚ú® AI/ML, MLOps, LLMOps",
    "section": "",
    "text": "Implement Guardrails for Domain-specific Chatbots\n\n\nInput & Output Guardrails for Food and Dish Recommendation Chatbot | Asynchronous Programming | SOLID Principles\n\n\n\nChatbot Guardrail\n\n\nAsynchronous Programming\n\n\nSOLID Principles\n\n\n\n\n\n\nDec 1, 2024\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuild Conversational Agentic Chatbot\n\n\nBuilding conversational chatbot using agent (tool/function calling) to interact with database\n\n\n\nAgentic Chatbot\n\n\nConversational Chatbot\n\n\nFunction Calling\n\n\n\n\n\n\nNov 23, 2024\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFine-Tune Qwen2.5-3B with DPO & Unsloth\n\n\nFine-Tuning Qwen2.5-3B with DPO using Unsloth on Preference TinyStories Dataset\n\n\n\nFinetuning\n\n\nDPO\n\n\nUnsloth\n\n\nQwen\n\n\n\n\n\n\nAug 26, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSupervised Fine-Tune Qwen2.5-3B with LoRA & Unsloth\n\n\nFinetuning Qwen2.5-3B with SFT-Lora using Unsloth on TinyStories instruction dataset\n\n\n\nFinetuning\n\n\nLORA\n\n\nUnsloth\n\n\n\n\n\n\nAug 25, 2024\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate Preference Dataset for DPO Fine-Tuning\n\n\nLeveraging LLMs for creating preference dataset for DPO Fine-Tuning\n\n\n\npreference-dataset\n\n\n\n\n\n\nAug 18, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate Instruction Dataset for Supervised Fine-Tuning\n\n\nLeveraging LLMs for creating instruction dataset for Supervised Fine-Tuning\n\n\n\ninstruction-dataset\n\n\n\n\n\n\nAug 11, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "draft/course.html",
    "href": "draft/course.html",
    "title": "Hands-On Generative AI Engineering with Large Language Model",
    "section": "",
    "text": "I am thrilled to introduce my Udemy course Hands-On Generative AI Engineering with Large Language Model.\nThis course equips you with a wide range of tools, frameworks, and techniques to create your GenAI applications using Large Language Models, including Python, PyTorch, LangChain, LlamaIndex, Hugging Face, FAISS, Chroma, Tavily, Streamlit, Gradio, FastAPI, Docker, and more.\nThis hands-on course covers essential topics such as implementing Transformers from scratch, fine-tuning Transformer models for downstream tasks, prompt engineering, vector embeddings, vector stores, and creating cutting-edge AI applications like AI Assistants, Chatbots, Retrieval-Augmented Generation (RAG) systems, autonomous agents, and serving your GenAI applications from scratch using REST APIs and Docker containerization.\nBy the end of this course, you will have the practical skills and theoretical knowledge needed to develop and serving your own LLM-based applications.\nIf you are interested in these above topics, checking it out."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I‚Äôm Quang. I am an AI/ML engineer. I live and work in Paris, France."
  },
  {
    "objectID": "draft/design_pattern_ai_system_production.html",
    "href": "draft/design_pattern_ai_system_production.html",
    "title": "Quang Duong",
    "section": "",
    "text": "When building an LLM-based agentic chatbot for production, several design patterns and elements should be considered: Guardrails: Implement safety controls to monitor and dictate user interactions with the LLM application. This ensures the AI model operates within defined principles and organizational guidelines2. Multi-Agent Systems (MAS): Utilize a system of specialized LLM agents for different tasks, such as a reflector, document checker, web searcher, critic, and coder. This approach can improve performance and accuracy compared to a single chatbot3. Agentic Design Patterns: Supervision: Use a Supervisor (Router) agent to manage Worker/Specialist agents4. Reflection: Prompt the LLM to critique its past actions for improvement4. Collaboration: Enable agents to share common memories and work together as specialists4. Agent Components: Planner: Creates step-by-step plans by decomposing complex tasks4. Memory: Stores conversation history and learned context4. Tools: Enables the agent to call external APIs or functions4. Specialization: Focus on building specialized agents for specific tasks rather than general-purpose ones. This approach often leads to better performance and reliability4. Evaluation Pipeline: Implement a proper evaluation system with clearly defined goals to assess and improve agent performance4. Rate Limiting: As you mentioned, implement rate limiting to manage API calls and prevent overuse or abuse of the system. Composability: Design agents as functions that can be combined and reused in various configurations5. Error Handling and Fallbacks: Implement robust error handling mechanisms and fallback options for when the LLM fails to provide satisfactory responses. Scalability: Design the system to handle increasing loads and user interactions efficiently. Monitoring and Logging: Implement comprehensive monitoring and logging systems to track performance, errors, and user interactions. Privacy and Security: Ensure proper data handling, encryption, and compliance with relevant regulations. By considering these design patterns and elements, you can create a more robust, efficient, and reliable LLM-based agentic chatbot for production use."
  },
  {
    "objectID": "posts/conversational-agent-chatbot.html",
    "href": "posts/conversational-agent-chatbot.html",
    "title": "Build Conversational Agentic Chatbot",
    "section": "",
    "text": "Conversational Agentic Chatbots leveraging Large Language Models (LLMs) represent a significant leap in AI-powered communication. These advanced chatbots go beyond simple query responses, actively engaging in complex tasks and decision-making processes.\nThey can interact with databases to answer user questions, schedule appointments, or compose and send emails, to name but a few. Unlike traditional chatbots, these AI agents understand context, adapt to user needs in real time, and can perform actions autonomously.\nThe increasing importance of agentic chatbots in the AI world stems from their ability to provide more personalized, efficient, and human-like interactions.\nSome benefits include reduced workload for human teams, and the capacity to handle complex inquiries with greater accuracy and contextual awareness.\nAs businesses seek to enhance customer engagement and streamline operations, agentic chatbots powered by LLMs are becoming indispensable tools for delivering superior user experiences and driving operational efficiency",
    "crumbs": [
      "üìùPosts",
      "üí¨ **AI Chatbot**",
      "1. ü§ñ Build First Conversational Agentic Chatbot"
    ]
  },
  {
    "objectID": "posts/conversational-agent-chatbot.html#conversational-agentic-chatbot",
    "href": "posts/conversational-agent-chatbot.html#conversational-agentic-chatbot",
    "title": "Build Conversational Agentic Chatbot",
    "section": "Conversational Agentic Chatbot",
    "text": "Conversational Agentic Chatbot\nIn this guide, we‚Äôll explore the development of a conversational agentic chatbot through a real-world use case.\nOur objective is to create an LLM-powered chatbot capable of interacting with an unstructured database of food images and generating detailed descriptions of dishes presented in these images.\nFor instance:\nHuman: Can you describe images presented famous dishes?\nAI: Please provide the name or details of the image you would like me to describe.\nHuman: I think it is Figure 1\nAI: This image showcases a traditional Vietnamese B√°nh M√¨, a popular street food sandwich ...\nHuman: Now describe Figure 2\nAI: This dish is a classic Italian spaghetti with tomato sauce, ...\nOur technology stack for this project includes:\n\nLLM model: OpenAI‚Äôs GPT-4 or GPT-4-mini\nOrchestration framework: LangGraph & LangChain for creating and orchestrating the conversational agent-based chatbot\nUnstructured database: MongoDB for efficient storage and retrieval of image data\nLLM & prompt monitoring: Opik by Comet ML for performance tracking and artifact management\n\nNow, let‚Äôs dive into the exciting implementation phase of our project!",
    "crumbs": [
      "üìùPosts",
      "üí¨ **AI Chatbot**",
      "1. ü§ñ Build First Conversational Agentic Chatbot"
    ]
  },
  {
    "objectID": "posts/conversational-agent-chatbot.html#implementation-with-langgraph-openai-mongodb-opik-comet-ml",
    "href": "posts/conversational-agent-chatbot.html#implementation-with-langgraph-openai-mongodb-opik-comet-ml",
    "title": "Build Conversational Agentic Chatbot",
    "section": "Implementation with LangGraph, OpenAI, MongoDB, Opik (Comet ML)",
    "text": "Implementation with LangGraph, OpenAI, MongoDB, Opik (Comet ML)\n\nImporting packages & Loading environment variables\nFirst, we import necessary packages.\n# Import required packages\nimport os\nimport json\nimport requests\nimport opik\nfrom opik import track\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.tools import tool\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.prebuilt import create_react_agent\nfrom pymongo import MongoClient\nfrom IPython.display import Image, display\nfrom dotenv import load_dotenv\nNext, we need to load environment variables, such as API keys, to enable the use of various services. For example: OPENAI_API_KEY for accessing OpenAI GPT models, COMET_API_KEY for utilizing Comet ML‚Äôs Opik monitoring service, and CONNECTION_STRING for connecting to MongoDB.\n\n\n\nExample of .env file\n\n\nWe use the load_dotenv function from the dotenv package to securely load our confidential API keys.\nload_dotenv()\n\n\nConnect, Add and Query MongoDB\nLet‚Äôs first connect with your MongoDB cluster.\n# Get MongoDB Connection String, database name and collection name\nMONGO_CONNECTION_STRING = os.getenv('CONNECTION_STRING')\nFOOD_DATABASE_NAME = os.getenv('FOOD_DATABASE_NAME')\nFOOD_COLLECTION_NAME = os.getenv('FOOD_COLLECTION_NAME')\n\n# Connect to Mongo\nclient = MongoClient(MONGO_CONNECTION_STRING)\nfood_db = client[FOOD_DATABASE_NAME]\nfood_collection = food_db[FOOD_COLLECTION_NAME]\nImagine you have raw data in JSON format containing image names and their corresponding URLs. We load this data into a dictionary.\n\nGet the raw data\ndata_path = './data/food_db.json'\nwith open(data_path, 'r') as file:\n    food_dict = json.load(file)\nprint(food_dict)\nThe raw data is structured as follows:\n{'Figure 1': 'https://drive.usercontent.google.com/download?id=1ODozOIYAjChN_oZoSFxUqGaAlNMeKYya&export=view&authuser=0',\n 'Figure 2': 'https://drive.usercontent.google.com/download?id=1bxNoQ0ORvvA1Ywnijq3mCmlH4kp5Az96&export=view&authuser=0'}\n\n\nAdd item to Mongo DB\nNext, we insert each item from the above data dictionary into our food_photos data collection.\n# Iterate and upload to MongoDB\nfor image_name, image_uri in food_dict.items():\n    food_collection.insert_one({'image_name': image_name, 'image_uri': image_uri})\n\n\n\nFood Photo DB on MongoDB\n\n\n\n\nQuery an item from Mongo DB\nThe following example demonstrates how to retrieve an image URI from our MongoDB collection using an image name, and then display the image.\nimport requests\nfrom IPython.display import Image, display\n\n# Querying image_uri by image_name\nimage_name = \"Figure 1\"\nitem = food_collection.find_one({'image_name': image_name})\nimage_url = item['image_uri']\n\n# Plot image\nresponse = requests.get(image_url)\nimage_data = response.content\ndisplay(Image(data=image_data))\n\n\n\nExample of querying image_uri\n\n\n\n\n\nCreate OpenAI LLM and Opik clients\nIn the next step, we initialize the OpenAI LLM model and the Opik client, which serve as the core AI model and the AI monitoring service, respectively.\n# Initial OpenAI model, i.e. gpt-4o or gpt-4o-mini\nllm = ChatOpenAI(model=\"gpt-4o\")\n\n# Initiate Opik client for LLM and prompt monitoring and management\nopik_client = opik.Opik()\n\n\nPromp tracking and management with Opik (Comet ML)\nPrompt engineering significantly impacts LLM performance. Therefore, a dedicated tool for prompt monitoring, versioning, and management is crucial. This practice is a cornerstone of LLMOps best practices. A centralized platform is essential for streamlining prompt monitoring and management while enabling seamless team collaboration.\nFor this vital task, we utilize Comet ML‚Äôs Opik to ensure easy-to-use and effectiveness.\nLet‚Äôs create our prompt for the agent task on the Opik platform. Alternatively, you can also create it programmatically. See this tutorial from Opik for more information. \nThe prompt for our agent is as bellow.\nYou are a renowned master chef with expertise in analyzing dishes and crafting exquisite recipes. \n\nUpon receiving a photo of a food item, you will provide a detailed description of the dish, including its key characteristics, ingredients, and cultural context if applicable. \n\nThen, you will create a high-quality, easy-to-follow recipe to prepare this dish, ensuring both flavor and presentation are exceptional.\nWhen you create different prompts for various purposes, the Opik‚Äôs Prompt Library page should look like this: \n\n\nPrepare prompt for AI Tool\nAfter creating the agent prompt on the Opik platform, we use the Opik Python SDK to retrieve this prompt directly in our source code. We then integrate it into a HumanMessage template from LangChain, enabling its use in the main function of our agentic tool.\nThe prompt and human message preparation are achieved using the following get_prompt function.\nWe use a decorator for this function:\n\n@track: for Opik to track the function‚Äôs inputs and outputs.\n\n@track\ndef get_prompt(prompt_name, image_url):\n    # Get prompt that is created on Opik platform\n    prompt_template = opik_client.get_prompt(name=prompt_name)\n    formatted_prompt_template = prompt_template.format()\n\n    # Create Human message\n    human_message = HumanMessage(\n        content=[\n            {\"type\": \"text\", \"text\": formatted_prompt_template},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n        ],\n    )\n    return human_message\nNext, we create the main function, generate_ip_image_description, for our agentic tool. Following the LangGraph agent convention, we include a docstring that describes the task the agent should perform. This function performs several steps:\n\nSpecify the name of the prompt we want to use.\nQuery the image_url from the MongoDB collection using the image_name.\nCreate a human message using the get_prompt function with two input arguments: prompt_name and image_url.\nInvoke the LLM model with the message to generate the response as the image description.\n\nWe apply two decorators to this function:\n\n@tool from LangChain_Core to define this function as an agentic tool.\n@track for Opik to enable tracking of inputs and outputs.\n\n\n\nCreate main function for agentic tool\n@tool\n@track\ndef generate_ip_image_description(image_name):\n    \"\"\"Generate description for an image about food. \n    Call this whenever you need to provide an image description, \n    for example when a customer asks 'Can you describe the Figure 1?'\"\"\"\n    \n    # prompt_name in created Opik prompt library\n    prompt_name = \"food_image_description\"\n\n    # get image_url from mongodb based on image_name\n    item = food_collection.find_one({'image_name': image_name})\n    image_url = item['image_uri']\n\n    # Get human message\n    message = get_prompt(prompt_name, image_url)\n\n    # Get image description from llm\n    response = llm.invoke([message])\n    return response.content\n\n\nAgent {Tools + Prompt + LLM + Memory}\nOnce the tools, LLM, and prompt are ready, we combine these agent components using the create_react_agent function from LangGraph to build our LLM agent. Importantly, to enable our agent chatbot to handle conversations effectively, we integrate memory by using MemorySaver from LangGraph.\n# Use`generate_ip_image_description` as tools for function calling\ntools = [generate_ip_image_description]\n\n# Global prompt for agent\nprompt = (\n    \"You are a helpful assistant. \"\n    \"You may not need to use tools for every query - the user may just want to chat!\"\n)\n\n# Agent memory\nmemory = MemorySaver()\n\n# Create Agent\nagent = create_react_agent(llm, tools, state_modifier=prompt, checkpointer=memory)",
    "crumbs": [
      "üìùPosts",
      "üí¨ **AI Chatbot**",
      "1. ü§ñ Build First Conversational Agentic Chatbot"
    ]
  },
  {
    "objectID": "posts/conversational-agent-chatbot.html#agent-inference",
    "href": "posts/conversational-agent-chatbot.html#agent-inference",
    "title": "Build Conversational Agentic Chatbot",
    "section": "Agent Inference",
    "text": "Agent Inference\nNow, let‚Äôs move on to the final and exciting step: testing our conversational agentic chatbot.\nBefore testing, we need to create the agent_inference function, which performs two key tasks:\n\nInvoke the agent‚Äôs invoke method using the user‚Äôs query.\nRetrieve the desired answer, handling two possible scenarios:\n\nIf no ToolMessage is returned, retrieve the AIMessage content to guide the user in providing more information.\nIf a ToolMessage is returned, extract and return its content.\n\n\ndef agent_inference(query):\n    # Response \n    response = agent.invoke(\n        {\"messages\": [HumanMessage(query)]},\n        config={\"configurable\": {\"thread_id\": \"1\"}},\n    )\n    \n    # Get ToolMessage if exists, else get AIMessage\n    if response['messages'][-2].__class__.__name__ != 'ToolMessage':\n        return response['messages'][-1].content # AIMessage content\n    else:\n        return response['messages'][-2].content # ToolMessage content\nNow, let‚Äôs test our conversational agentic chatbot with three scenarios:\n\nQA #1: The user asks a general question about describing an image. The chatbot responds by requesting more specific information.\nQA #2: The user specifies the image name (e.g., ‚ÄúFigure 1‚Äù), and the chatbot generates a description for this image.\nQA #3: The user asks about a new image (e.g., ‚ÄúFigure 2‚Äù), and the chatbot processes the request and generates a description for the new image.\n\n\nQA #1\n# Question-Response 1\nquery = \"Can you describe for me an image?\"\nresponse = agent_inference(query)\nprint(response)\nOf course! Please provide me with the image name or more details, so I can generate a description for you.\n\n\nQA #2\n# Question-Response 2\nquery = \"I think it is Figure 1\"\nresponse = agent_inference(query)\nprint(response)\n\n\n\nFigure 1: Vietnamese Sandwich\n\n\nThis image showcases a B√°nh M√¨, a popular Vietnamese sandwich known for its delightful combination of flavors and textures. The B√°nh M√¨ is a fusion of French and Vietnamese culinary traditions, originating from the period of French colonial rule in Vietnam. It typically features a crusty baguette filled with a variety of ingredients that create a harmonious balance of savory, spicy, sweet, and tangy flavors.\n\n### Key Characteristics:\n- **Bread:** A light, crispy baguette with a soft interior.\n- **Protein:** Often includes grilled or roasted meats such as pork or chicken.\n- **Vegetables:** Fresh cucumber slices, pickled carrots, and daikon radish.\n- **Herbs:** Fresh cilantro adds a fragrant note.\n- **Condiments:** Mayonnaise, p√¢t√©, and a hint of spicy chili.\n\n### Recipe for a Classic B√°nh M√¨\n\n#### Ingredients:\n- **Baguette:**\n    - 1 fresh baguette\n\n- **Protein:**\n    - 200g pork tenderloin or chicken breast\n    - 2 tablespoons soy sauce\n    - 1 tablespoon fish sauce\n    - 1 tablespoon sugar\n    - 1 garlic clove, minced\n    - 1 tablespoon vegetable oil\n\n- **Pickled Vegetables:**\n    - 1 carrot, julienned\n    - 1 daikon radish, julienned\n    - 1/4 cup white vinegar\n    - 1/4 cup sugar\n    - 1/2 cup water\n    - Pinch of salt\n\n- **Additional Toppings:**\n    - 1 cucumber, thinly sliced\n    - Fresh cilantro leaves\n    - 2 tablespoons mayonnaise\n    - Fresh chili slices (optional)\n\n#### Instructions:\n\n1. **Marinate the Protein:**\n    - In a bowl, combine soy sauce, fish sauce, sugar, garlic, and oil. Add the pork or chicken, ensuring it's well-coated. Marinate for at least 30 minutes.\n\n2. **Prepare Pickled Vegetables:**\n    - In a small saucepan, combine vinegar, sugar, water, and salt. Heat until sugar dissolves, then let cool.\n    - Place carrot and daikon in a jar, pour the pickling liquid over, and let sit for at least 30 minutes.\n\n3. **Cook the Protein:**\n    - Heat a grill pan or skillet over medium-high heat. Cook the marinated meat until it's cooked through and slightly caramelized. Let rest, then slice thinly.\n\n4. **Assemble the B√°nh M√¨:**\n    - Slice the baguette lengthwise, spread mayonnaise on both sides.\n    - Layer the cooked protein, pickled vegetables, cucumber slices, cilantro, and chili (if using) inside the baguette.\n\n5. **Serve:**\n    - Press the sandwich slightly to hold it together, and serve immediately to enjoy the contrasting textures and flavors.\n\nThis B√°nh M√¨ recipe offers a delightful balance of flavors and can be customized with your preferred proteins and additional toppings. Enjoy this Vietnamese classic as a flavorful and satisfying meal.\n\n\nQA #3\n# Question-Response 3\nquery = \"Now describe Figure 2\"\nresponse = agent_inference(query)\nprint(response)\n\n\n\nFigure 2: Italian Spaghetti\n\n\nThis dish is a classic Italian Spaghetti al Pomodoro. It features a beautifully arranged mound of spaghetti topped with a rich tomato sauce, adorned with fresh basil leaves and a sprinkle of grated Parmesan cheese. The vibrant red of the sauce contrasts with the golden pasta, and the garnishing adds a touch of green, making it visually appealing.\n\n### Key Characteristics and Cultural Context:\n- **Main Ingredients**: Spaghetti, tomatoes, basil, Parmesan cheese.\n- **Cultural Context**: Spaghetti al Pomodoro is a staple in Italian cuisine, celebrated for its simplicity and fresh ingredients. It embodies the Italian philosophy of letting high-quality ingredients shine without overwhelming them.\n\n### Recipe for Spaghetti al Pomodoro\n\n#### Ingredients:\n- 400g spaghetti\n- 800g ripe tomatoes (or canned whole tomatoes)\n- 3 tablespoons olive oil\n- 3 cloves garlic, minced\n- Salt and freshly ground black pepper to taste\n- A pinch of red pepper flakes (optional)\n- Fresh basil leaves\n- 100g Parmesan cheese, grated\n\n#### Instructions:\n\n1. **Prepare the Tomatoes:**\n    - If using fresh tomatoes, blanch them in boiling water for 30 seconds, then transfer to ice water. Peel and chop them.\n\n2. **Cook the Pasta:**\n    - Bring a large pot of salted water to a boil. Add the spaghetti and cook according to package instructions until al dente. Reserve 1 cup of pasta water, then drain the pasta.\n\n3. **Make the Sauce:**\n    - In a large skillet, heat the olive oil over medium heat. Add the minced garlic and saut√© until fragrant, about 1 minute.\n    - Add the tomatoes (fresh or canned) and crush them with a spoon. Season with salt, black pepper, and red pepper flakes if using.\n    - Simmer the sauce for about 15 minutes, stirring occasionally. Adjust the seasoning as needed.\n\n4. **Combine Pasta and Sauce:**\n    - Add the cooked spaghetti to the sauce. Toss to coat the pasta, adding reserved pasta water if the sauce is too thick.\n\n5. **Serve:**\n    - Divide the pasta among serving plates. Top with grated Parmesan cheese and fresh basil leaves.\n    - Drizzle a little olive oil over the top for extra flavor.\n\n### Presentation Tips:\n- Use a fork to twist the pasta into nests on each plate.\n- Place basil leaves strategically for a pop of color.\n- Serve with extra Parmesan cheese on the side.\n\nEnjoy your homemade Spaghetti al Pomodoro with a glass of red wine for an authentic Italian dining experience!\nGreat! Amazing! Our conversational agentic chatbot has successfully provided descriptions for each photo we requested!",
    "crumbs": [
      "üìùPosts",
      "üí¨ **AI Chatbot**",
      "1. ü§ñ Build First Conversational Agentic Chatbot"
    ]
  },
  {
    "objectID": "posts/conversational-agent-chatbot.html#monitoring-qa-prompt-execution-time",
    "href": "posts/conversational-agent-chatbot.html#monitoring-qa-prompt-execution-time",
    "title": "Build Conversational Agentic Chatbot",
    "section": "Monitoring QA, Prompt, Execution Time",
    "text": "Monitoring QA, Prompt, Execution Time\nAs discussed earlier, monitoring prompts and the LLM‚Äôs inputs and outputs is crucial for enhancing traceability, which helps us analyze the performance of the LLM agent effectively.\nOpik platform ensures tracking for each query, including:\n\nThe prompt used.\nInputs and outputs for each function, sub-function, and agent tool.\nExecution time for each step and function.\n\n\nMonitoring QA #1\n\n\n\nQA tracking #1 on Opik platform\n\n\n\n\nMonitoring QA #2\n\n\n\nQA tracking #2 on Opik platform\n\n\n\n\nMonitoring QA #3\n\n\n\nQA tracking #3 on Opik platform\n\n\nWhen numerous experiments are conducted, analyzing these tracked data points can help us optimize the system effectively.",
    "crumbs": [
      "üìùPosts",
      "üí¨ **AI Chatbot**",
      "1. ü§ñ Build First Conversational Agentic Chatbot"
    ]
  },
  {
    "objectID": "posts/conversational-agent-chatbot.html#conclusion",
    "href": "posts/conversational-agent-chatbot.html#conclusion",
    "title": "Build Conversational Agentic Chatbot",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, Conversational Agentic Chatbots leveraging Large Language Models (LLMs) are innovating AI-driven interactions by combining contextual understanding, adaptability, and autonomous decision-making.\nAs demonstrated in the development of a food image description chatbot, these systems can seamlessly integrate with unstructured databases like MongoDB, advanced LLM like OpenAI GPT-4o, advanced orchestration frameworks like LangGraph, and monitoring tools like Comet ML‚Äôs Opik to deliver highly personalized and context-aware user experiences.\nBy enabling enhanced customer engagement, streamlined operations, and creative problem-solving, agentic chatbots are transforming industries and setting new standards for intelligent communication tools. As businesses increasingly adopt this cutting-edge technology, the potential for innovation and efficiency in human-AI collaboration continues to grow.",
    "crumbs": [
      "üìùPosts",
      "üí¨ **AI Chatbot**",
      "1. ü§ñ Build First Conversational Agentic Chatbot"
    ]
  },
  {
    "objectID": "posts/conversational-agent-chatbot.html#reference",
    "href": "posts/conversational-agent-chatbot.html#reference",
    "title": "Build Conversational Agentic Chatbot",
    "section": "Reference",
    "text": "Reference\n[1] https://cookbook.openai.com/examples/how_to_use_guardrails\n[2] https://medium.com/velotio-perspectives/an-introduction-to-asynchronous-programming-in-python-af0189a88bbb\n[3] https://realpython.com/solid-principles-python/",
    "crumbs": [
      "üìùPosts",
      "üí¨ **AI Chatbot**",
      "1. ü§ñ Build First Conversational Agentic Chatbot"
    ]
  },
  {
    "objectID": "posts/instruction-dataset-sft.html",
    "href": "posts/instruction-dataset-sft.html",
    "title": "Create Instruction Dataset for Supervised Fine-Tuning",
    "section": "",
    "text": "Creating a tailored instruction dataset for fine-tuning a language model is a critical step in enhancing the model‚Äôs capabilities for specialized tasks. This guide provides a step-by-step example of how to create an instruction dataset.\nBefore starting, it is essential to define the dataset‚Äôs intended purpose. Are you developing a chatbot, a story generator, or a question-answering system? Clearly understanding the desired model behavior will guide the type and structure of the data you prepare.\nIn this guide, our goal is to create an instruction dataset suitable for fine-tuning a pretrained Large/Small Language Model to produce a story generator dedicated for 5-year-olds.",
    "crumbs": [
      "üìùPosts",
      "üß™ **Data Processing**",
      "1. Instruction Dataset"
    ]
  },
  {
    "objectID": "posts/instruction-dataset-sft.html#use-case",
    "href": "posts/instruction-dataset-sft.html#use-case",
    "title": "Create Instruction Dataset for Supervised Fine-Tuning",
    "section": "Use Case",
    "text": "Use Case\nLet‚Äôs examine our use-case for this guide.\nThe raw dataset TinyStories, introduced in the paper TinyStories: How Small Can Language Models Be and Still Speak Coherent English? by Ronen Eldan and Yuanzhi Li. This dataset consists of short, synthetically generated stories created by GPT-3.5 and GPT-4 with a limited vocabulary, making it highly suitable for our intended 5-year-old readers. The dataset is divided into two splits: train (2.12M rows) and validation (22K rows). For this use case, we will use the train split with 10K rows.\nBelow is a sample view of the TinyStories dataset on the Hugging Face Dataset Hub: \nTo create the instruction dataset, we will generate synthetic instruction sentences that correspond to each story in the TinyStories dataset.",
    "crumbs": [
      "üìùPosts",
      "üß™ **Data Processing**",
      "1. Instruction Dataset"
    ]
  },
  {
    "objectID": "posts/instruction-dataset-sft.html#implementation",
    "href": "posts/instruction-dataset-sft.html#implementation",
    "title": "Create Instruction Dataset for Supervised Fine-Tuning",
    "section": "Implementation",
    "text": "Implementation\n\n1. Load Required Packages\nLet‚Äôs begin by loading the necessary packages.\nimport concurrent.futures\nimport json\nimport re\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import List, Tuple\nfrom datasets import Dataset, load_dataset, concatenate_datasets\nfrom openai import OpenAI\nfrom tqdm.auto import tqdm\nfrom google.colab import userdata\n\n\n2. Define Modular Functions\nNext, we define key functions to structure our pipeline.\nExtracting Stories The get_story_list function creates a list of stories from the raw dataset:\ndef get_story_list(dataset):\n    return [example['text'] for example in dataset]\nManaging Instruction-Answer Pairs\nThe InstructionAnswerSet class defines a structure to store and manage instruction-answer pairs, with methods to create instances from JSON and iterate over pairs:\nclass InstructionAnswerSet:\n    def __init__(self, pairs: List[Tuple[str, str]]):\n        self.pairs = pairs\n\n    @classmethod\n    def from_json(cls, json_str: str, story: str) -&gt; 'InstructionAnswerSet':\n        data = json.loads(json_str)\n        pairs = [(data['instruction_answer'], story)]\n        return cls(pairs)\n\n    def __iter__(self):\n        return iter(self.pairs)\nGenerating Instruction-Answer Pairs\nThe generate_instruction_answer_pairs function takes a story and an OpenAI client as inputs to generate instruction-answer pairs using GPT-4. The function crafts a prompt to create relevant instructions while adhering to specific formatting requirements:\ndef generate_instruction_answer_pairs(story: str, client: OpenAI) -&gt; List[Tuple[str, str]]:\n    prompt = f\"\"\"Based on the following story, generate an one-sentence instruction. Instruction \\\n        must ask to write about a content the story.\n        Only use content from the story to generate the instruction. \\\n        Instruction must never explicitly mention a story. \\\n        Instruction must be self-contained and general. \\\n\n        Example story: Once upon a time, there was a little girl named Lily. \\\n        Lily liked to pretend she was a popular princess. She lived in a big castle \\\n        with her best friends, a cat and a dog. One day, while playing in the castle, \\\n        Lily found a big cobweb. The cobweb was in the way of her fun game. \\\n        She wanted to get rid of it, but she was scared of the spider that lived there. \\\n        Lily asked her friends, the cat and the dog, to help her. They all worked together to clean the cobweb. \\\n        The spider was sad, but it found a new home outside. Lily, the cat, and \\\n        the dog were happy they could play without the cobweb in the way. \\\n        And they all lived happily ever after.\n        \n        Example instruction: Write a story about a little girl named Lily who, \\\n        with the help of her cat and dog friends, overcomes her fear of a spider to \\\n        clean a cobweb in their castle, allowing everyone to play happily ever after. \\\n\n        Provide your response in JSON format with the following structure:\n        {{\"instruction_answer\": \"...\"}}\n        Story:\n        {story}\n        \"\"\"\n    completion = client.chat.completions.create(model=\"gpt-4o-mini\",\n                                                messages=[\n                                                    {\"role\": \"system\",\n                                                    \"content\": \"You are a helpful assistant who \\\n                                                    generates instruction based on the given story. \\\n                                                    Provide your response in JSON format.\",},\n                                                    {\"role\": \"user\", \"content\": prompt},\n                                                    ],\n                                                response_format={\"type\": \"json_object\"},\n                                                max_tokens=1200,\n                                                temperature=0.7,)\n    result = InstructionAnswerSet.from_json(completion.choices[0].message.content, story)\n    # Convert to list of tuples\n    return result.pairs\nCreating the Instruction Dataset\nWe now wrap the previous functions into a final function create_instruction_dataset:\ndef create_instruction_dataset(dataset: Dataset, client: OpenAI, num_workers: int = 4) -&gt; Dataset:\n    stories = extract_substory(dataset)\n    instruction_answer_pairs = []\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n        futures = [executor.submit(generate_instruction_answer_pairs, story, client) for story in stories]\n\n        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n            instruction_answer_pairs.extend(future.result())\n\n    instructions, answers = zip(*instruction_answer_pairs)\n    return Dataset.from_dict({\"instruction\": list(instructions), \"output\": list(answers)})\n\n\n3. Orchestrating the Pipeline\nNest, the main function orchestrates the entire pipeline, which includes:\n+ Initialize the OpenAI client\n+ Load the raw TinyStories dataset\n+ Create instruction dataset\n+ Perform train/test split\n+ Push the processed dataset to Hugging Face Hub\ndef main() -&gt; Dataset:\n    # Initializes the OpenAI client\n    client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n\n    # Load the raw data\n    raw_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:10000]\")\n\n    # Create instructiondataset\n    instruction_dataset = create_instruction_dataset(raw_dataset, client)\n\n    # Train/test split and export\n    filtered_dataset = instruction_dataset.train_test_split(test_size=0.1)\n\n    # Push the processed dataset to Hugging Face Hub\n    filtered_dataset.push_to_hub(\"tanquangduong/TinyStories_Instruction\")\n\n\n4. Authenticating and Running the Pipeline\nThen, we authenticate with the Hugging Face Hub for later dataset uploading and execute the pipeline.\nfrom huggingface_hub import login\n# Log in to the Hugging Face Hub\nlogin(token=userdata.get('HF_TOKEN'))\n\n# Launch the pipeline to create instruction dataset\nmain()",
    "crumbs": [
      "üìùPosts",
      "üß™ **Data Processing**",
      "1. Instruction Dataset"
    ]
  },
  {
    "objectID": "posts/instruction-dataset-sft.html#resulting-instruction-dataset",
    "href": "posts/instruction-dataset-sft.html#resulting-instruction-dataset",
    "title": "Create Instruction Dataset for Supervised Fine-Tuning",
    "section": "Resulting Instruction Dataset",
    "text": "Resulting Instruction Dataset\nAfter running the above pipeline, the resulting instruction dataset will look like this:",
    "crumbs": [
      "üìùPosts",
      "üß™ **Data Processing**",
      "1. Instruction Dataset"
    ]
  },
  {
    "objectID": "posts/instruction-dataset-sft.html#conclusion",
    "href": "posts/instruction-dataset-sft.html#conclusion",
    "title": "Create Instruction Dataset for Supervised Fine-Tuning",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, this guide demonstrated the creation of an instruction dataset tailored for fine-tuning. We first defined the purpose of fine-tuning and structured the dataset accordingly. By leveraging GPT-4, we generated instructions for each story using best practices in prompt engineering, including precise instructions, a one-shot example, and a specified output format. Finally, the processed dataset was uploaded to the Hugging Face Hub for future use.",
    "crumbs": [
      "üìùPosts",
      "üß™ **Data Processing**",
      "1. Instruction Dataset"
    ]
  },
  {
    "objectID": "posts/unsloth-qwen-dpo.html",
    "href": "posts/unsloth-qwen-dpo.html",
    "title": "Fine-Tune Qwen2.5-3B with DPO & Unsloth",
    "section": "",
    "text": "Supervised fine-tuning for large language models (LLMs) enables precise adaptation of a model to specific tasks or domains, significantly improving its performance by providing labeled data tailored to desired outputs. This technique is particularly beneficial in scenarios where off-the-shelf LLMs fail to meet domain-specific or task-specific requirements, such as legal document summarization or medical diagnostics, where accuracy and relevance are critical.\nHowever, supervised fine-tuning alone may not address alignment with human preferences, especially in open-ended or subjective tasks. This is where Reinforcement Learning from Human Feedback (RLHF) techniques come in handy. As they align the model‚Äôs outputs with user preferences by incorporating feedback on what users value most, ensuring that the LLM generates responses that are not only accurate but also contextually aligned with human expectations and ethical considerations. These fine-tuning techniques are often named as fine-tuning with preference alignmnent.\nThis post focuses on Direct Preference Optimization (DPO), a fine-tuning technique that aligns LLMs with human preferences by directly optimizing the model‚Äôs outputs based on human feedback.",
    "crumbs": [
      "üìùPosts",
      "‚ö° **Fine-Tuning**",
      "2. Preference Alignment Fine-Tuning with DPO"
    ]
  },
  {
    "objectID": "posts/unsloth-qwen-dpo.html#preference-alignmnent-with-dpo",
    "href": "posts/unsloth-qwen-dpo.html#preference-alignmnent-with-dpo",
    "title": "Fine-Tune Qwen2.5-3B with DPO & Unsloth",
    "section": "Preference Alignmnent with DPO",
    "text": "Preference Alignmnent with DPO\nDPO is introduced in the paper Direct Preference Optimization: Your Language Model is Secretly a Reward Model by R. Rafailov et al in 2023.\nUnlike Proximal Policy Optimization (PPO), a reinforcement learning algorithm that requires training a separate reward model and iterative sampling, DPO simplifies the process by using a supervised learning framework to adjust the model‚Äôs behavior according to ranked human preferences.\nThe principle of DPO involves collecting human preferences on model outputs and using a binary cross-entropy objective to steer the model towards producing desired responses.\nThis method offers several benefits: it simplifies the training process, reduces computational requirements, and potentially leads to faster and more effective alignment with human values. Additionally, it can be more efficient in mitigating the risk of inheriting biases from training data.\nIn the next session, we will work on a use-case where we implement DPO to fine-tune a base LLM for a specific question-answering task that aligns with human preference.",
    "crumbs": [
      "üìùPosts",
      "‚ö° **Fine-Tuning**",
      "2. Preference Alignment Fine-Tuning with DPO"
    ]
  },
  {
    "objectID": "posts/unsloth-qwen-dpo.html#use-case",
    "href": "posts/unsloth-qwen-dpo.html#use-case",
    "title": "Fine-Tune Qwen2.5-3B with DPO & Unsloth",
    "section": "Use-case",
    "text": "Use-case\nIn the previous post, Fine-tune Qwen2.5-3B using Lora with Unsloth, we have fine-tuned the base model Qwen2.5-3B on the instruction dataset TinyStories_Instruction using Parameter-Efficient Fine-Tuning (PEFT) technique like LoRA to obtain a story generator for children given a story instruction request.\nContinue this use-case, in this post we will also fine-tune the base model Qwen2.5-3B, but using DPO. For this end, we need to use a preference dataset TinyStories_Preference, that we have created and discussed in the previous post Create Preference Dataset for DPO Fine-Tuning.\nEventually we will compare stories generated by the two above methods to evaluate their performance.\nLet‚Äôs jump into the implementation part.",
    "crumbs": [
      "üìùPosts",
      "‚ö° **Fine-Tuning**",
      "2. Preference Alignment Fine-Tuning with DPO"
    ]
  },
  {
    "objectID": "posts/unsloth-qwen-dpo.html#implementation",
    "href": "posts/unsloth-qwen-dpo.html#implementation",
    "title": "Fine-Tune Qwen2.5-3B with DPO & Unsloth",
    "section": "Implementation",
    "text": "Implementation\n\nInstall and import required packages\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps xformers trl peft accelerate bitsandbytes\n!pip install -q comet_ml\nfrom unsloth import PatchDPOTrainer\nPatchDPOTrainer()\n\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, TextStreamer\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\nfrom trl import DPOConfig, DPOTrainer\nfrom google.colab import userdata\nimport comet_ml\n\n\nInitialize Comet ML for Experiment Tracking\nSet up Comet ML to log experiments, track training metrics, and monitor performance:\ncomet_ml.login(project_name=\"dpo-lora-unsloth\")\n\n\nLoad Pretrained Model and Tokenizer\nLoad the pretrained Qwen model and tokenizer from Hugging Face. Specify the maximum sequence length and determine whether to load the model in 4-bit precision for efficiency: True means using QLoRA, False means using LoRA.\nmax_seq_length = 2048\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"Qwen/Qwen2.5-3B\",\n    max_seq_length=max_seq_length,\n    load_in_4bit=False,\n    )\n\n\nApply LoRA Adaptation\nLoRA efficiently fine-tunes specific layers, i.e.¬†‚Äúq_proj‚Äù, ‚Äúk_proj‚Äù, ‚Äúv_proj‚Äù, etc, by introducing additional trainable parameters, significantly reducing memory and computation requirements:\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=32,\n    lora_alpha=32,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n    )\n\n\nDataset Preparation\nFormat the dataset with a specific Alpaca-like template and append the EOS token to chosen and rejected samples. Then split the dataset into training and testing sets:\nalpaca_template = \"\"\"Below is an instruction that describes a task.\nWrite a response that appropriately completes the request.\n### Instruction:\n{}\n### Response:\n\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token\ndef format_samples(example):\n    example[\"prompt\"] = alpaca_template.format(example[\"prompt\"])\n    example[\"chosen\"] = example['chosen'] + EOS_TOKEN\n    example[\"rejected\"] = example['rejected'] + EOS_TOKEN\n    return {\n        \"prompt\": example[\"prompt\"],\n        \"chosen\": example[\"chosen\"],\n        \"rejected\": example[\"rejected\"]\n    }\ndataset = dataset.map(format_samples)\ndataset = dataset.train_test_split(test_size=0.05)\n\n\nTraining Using DPOTrainer\nConfigure the training process with the DPOTrainer class. Define hyperparameters such as learning rate, batch size, gradient accumulation steps, and number of epochs. Enable Comet ML for logging:\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=None,\n    tokenizer=tokenizer,\n    beta=0.5,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    max_length=max_seq_length//2,\n    max_prompt_length=max_seq_length//2,\n    args=DPOConfig(\n        learning_rate=2e-6,\n        lr_scheduler_type=\"linear\",\n        per_device_train_batch_size=2,\n        per_device_eval_batch_size=2,\n        gradient_accumulation_steps=8,\n        num_train_epochs=1,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        warmup_steps=10,\n        output_dir=\"output\",\n        eval_strategy=\"steps\",\n        eval_steps=0.2,\n        logging_steps=1,\n        report_to=\"comet_ml\",\n        seed=0,\n        ),\n)\n\n\ntrainer.train()\n\n\nModel Inference\nWhen the training has finished, let‚Äôs test our fine-tuned model. The inference includes formatting the input prompt and using a text streamer for real-time text generation:\nFastLanguageModel.for_inference(model)\nmessage = alpaca_template.format(\"Write a story about a humble little bunny \\\nnamed Ben who follows a mysterious trail in the woods, \\\ndiscovering beautiful flowers, new friends, and a lovely pond along the way.\", \"\")\ninputs = tokenizer([message], return_tensors=\"pt\").to(\"cuda\")\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=2048, use_cache=True)\nThe generated output is as below:\nOnce upon a time, there was a little bunny named Ben. Ben lived in a cozy little house in the woods, surrounded by tall trees and a beautiful meadow. He loved to play and explore, but he always felt a little bit lost and alone.\n\nOne day, Ben decided to take a walk in the woods. He was feeling a little adventurous and wanted to see what he could find. As he walked, he noticed a trail that led deeper into the woods. He followed the trail, and soon he found himself in a beautiful meadow filled with colorful flowers.\n\nAs Ben walked through the meadow, he met a little bird named Lily. Lily was a cheerful bird who loved to sing and dance. She told Ben about all the different flowers she had seen in the meadow, and they spent the rest of the day playing and having fun together.\n\nAfter a while, Ben and Lily decided to take a break and rest in a cozy little clearing. As they sat there, they heard a gentle sound coming from the pond. They followed the sound and found a lovely pond with clear, blue water. They sat by the pond and watched the fish swim around, and they even saw a little frog hop by.\n\nAs the sun began to set, Ben and Lily decided to head back home. They said goodbye to each other and promised to meet again soon. Ben felt a little bit sad, but he knew that he had made some new friends and had discovered some beautiful things in the woods.\n\nFrom that day on, Ben made a habit of exploring the woods every day. He would follow the mysterious trail and discover new things, new friends, and new adventures. And he knew that no matter where he went, he would always have a little bit of magic in his heart.\nLet‚Äôs compare it with the story generated by the instructed model:\nOnce upon a time, there was a humble little bunny named Ben. Ben loved to hop around in the meadow, eating carrots and playing with his friends. One day, Ben saw a mysterious trail in the woods. He was curious and wanted to follow it.\n\nBen hopped along the trail, hopping faster and faster. He saw beautiful flowers and new friends along the way. Suddenly, he came to a big pond. He hopped into the water and splashed around. It was so much fun!\n\nBen continued to follow the mysterious trail, hopping and splashing until he couldn't see the end. He was happy to have had such a wonderful adventure. From that day on, Ben knew that he could always follow a mysterious trail and have fun.\n\nIt can be seen that the story generated by preference alignment using DPO is more attractive and having more interesting content than the one generated by supervised fine-tune with LoRA on instruction dataset.\n\n\nSave and Push to Hugging Face Hub\nWhen we are satisfied with our training process, let‚Äôs save the fine-tuned model in 16-bit merged format and push it to the Hugging Face Hub for easy sharing and deployment:\nfrom huggingface_hub import login\n# Log in to the Hugging Face Hub\nlogin(token=userdata.get('HF_TOKEN'))\n\nmodel.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\nmodel.push_to_hub_merged(\"tanquangduong/Qwen2.5-3B-DPO-TinyStories\", tokenizer, save_method=\"merged_16bit\")",
    "crumbs": [
      "üìùPosts",
      "‚ö° **Fine-Tuning**",
      "2. Preference Alignment Fine-Tuning with DPO"
    ]
  },
  {
    "objectID": "posts/unsloth-qwen-dpo.html#conclusion",
    "href": "posts/unsloth-qwen-dpo.html#conclusion",
    "title": "Fine-Tune Qwen2.5-3B with DPO & Unsloth",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we have discussed the need for preference alignment in fine-tuning LLMs. While traditional fine-tuning enhances model accuracy for structured tasks, preference alignment techniques like DPO extend the capabilities of LLMs to align with human values and preferences.\nThe implementation provided in this post demonstrates how to apply DPO for fine-tuning the Qwen2.5-3B model using a preference dataset. This workflow enables the model to generate outputs that are not only accurate but also contextually relevant and ethically aligned with user expectations.",
    "crumbs": [
      "üìùPosts",
      "‚ö° **Fine-Tuning**",
      "2. Preference Alignment Fine-Tuning with DPO"
    ]
  }
]