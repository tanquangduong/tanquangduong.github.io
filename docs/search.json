[
  {
    "objectID": "posts/unsloth-qwen-sft-lora.html#what-is",
    "href": "posts/unsloth-qwen-sft-lora.html#what-is",
    "title": "Finetuning Qwen2.5-3B with Unscloth",
    "section": "ü§ù What is",
    "text": "ü§ù What is\nExample of configuration:",
    "crumbs": [
      "üöÄ AI Station",
      "‚ö° **Supervised Fine Tuning (SFT)**",
      "1. Supervised Finetuning Qwen2.5-3B using LORA with Unscloth"
    ]
  },
  {
    "objectID": "posts/preference-dataset-dpo.html#what-is",
    "href": "posts/preference-dataset-dpo.html#what-is",
    "title": "Preference Dataset Creation for DPO Fine-Tuning",
    "section": "ü§ù What is",
    "text": "ü§ù What is\nExample of configuration:",
    "crumbs": [
      "üöÄ AI Station",
      "üß™ **Dataset for LLM**",
      "2. Preference Dataset Creation for DPO Fine-Tuning"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "‚ú® AI/ML, MLOps, LLMOps",
    "section": "",
    "text": "Instruction Dataset Creation for Supervised Fine-Tuning\n\n\nLeveraging LLMs for creating instruction dataset for Supervised Fine-Tuning\n\n\n\ninstruction-dataset\n\n\n\n\n\n\nAug 27, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreference Dataset Creation for DPO Fine-Tuning\n\n\nLeveraging LLMs for creating preference dataset for DPO Fine-Tuning\n\n\n\npreference-dataset\n\n\n\n\n\n\nAug 27, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinetuning Qwen2.5-3B using DPO with Unsloth\n\n\nFinetuning Qwen2.5-3B with DPO using Unsloth on TinyStories prefrence dataset\n\n\n\nFinetuning\n\n\nDPO\n\n\nUnsloth\n\n\nQwen\n\n\n\n\n\n\nAug 24, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nFinetuning Qwen2.5-3B with Unscloth\n\n\nFinetuning Qwen2.5-3B with SFT-Lora using Unsloth on TinyStories instruction dataset\n\n\n\nFinetuning\n\n\nLORA\n\n\nUnsloth\n\n\n\n\n\n\nAug 24, 2024\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I‚Äôm Quang. I am an AI/ML engineer. I live and work in Paris, France."
  },
  {
    "objectID": "posts/instruction-dataset-sft.html",
    "href": "posts/instruction-dataset-sft.html",
    "title": "Instruction Dataset Creation for Supervised Fine-Tuning",
    "section": "",
    "text": "Getting started GenAI & LLM with my Udemy course, Hands-on Generative AI Engineering with Large Language Model üëá \nCreating an instruction dataset tailored for fine-tuning a language model is a critical step in enhancing the model‚Äôs capabilities for specialized tasks. Fine-tuning refers to training a pre-trained model further on a custom dataset to improve its performance on specific tasks. This guide walks through an example of creating an instruction dataset.\nBefore creating your dataset, it‚Äôs essential to define the intended purpose. Are you building a chatbot, a story generator, or a question-answering system? Understanding the desired behavior of the model will guide the type and structure of data you include.\nOur objective is to\n\nCreate an instruction dataset, that is ready for fine-tunining a pretrained Large/Small Language Model to obtain a story generator dedicated for the 5-year-olds.\n\nTo do so, for the sake of demonstration, we choose\n\nQwen2.5-3B as a pretraine language model to be finetuned. The model has 3.09B parameters that are not too big to fine-tune with Colab. This fine-tuning part will be presented in another post.\nFor raw dataset, we choose TinyStory, presented along with the paper TinyStories: How Small Can Language Models Be and Still Speak Coherent English? proposed by Romen Eldan and Yuanzhi Li. This dataset contains short stories that are synthetically generated by GPT-3.5 and GPT-4, only using a small vocabulary. That is very suitable for our intended 5-year-old readers. This dataset has 2 splits: train(2.12M rows) and validation(22K rows). In our use-case, I just use train split with 10K rows.\n\nLet‚Äôs take a look at how the TinyStories dataset looks like. \nTo obtain the instruction dataset, for each story in the TinyStories dataset, we need to generate synthetically an instruction sentence that corresponds to the story. The result should look like this. \nHow can we obtain this?\nLet‚Äôs jump into the code!\nFirst, we load required packages.\nimport concurrent.futures\nimport json\nimport re\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import List, Tuple\nfrom datasets import Dataset, load_dataset, concatenate_datasets\nfrom openai import OpenAI\nfrom tqdm.auto import tqdm\nfrom google.colab import userdata\nNext, we write extract_substory function to create a list of story texts\ndef extract_substory(dataset):\n    return [example['text'] for example in dataset]\nThe InstructionAnswerSet class defines a data structure to store and manage instruction-answer pairs, with methods to create instances from JSON and iterate over pairs.\nclass InstructionAnswerSet:\n    def __init__(self, pairs: List[Tuple[str, str]]):\n        self.pairs = pairs\n\n    @classmethod\n    def from_json(cls, json_str: str, story: str) -&gt; 'InstructionAnswerSet':\n        data = json.loads(json_str)\n        pairs = [(data['instruction_answer'], story)]\n        return cls(pairs)\n\n    def __iter__(self):\n        return iter(self.pairs)\nThis function takes a story and OpenAI client as input to generate instruction-answer pairs using GPT-4. It processes the story through a carefully crafted prompt to create relevant instructions while following specific formatting requirements.\ndef generate_instruction_answer_pairs(story: str, client: OpenAI) -&gt; List[Tuple[str, str]]:\n    prompt = f\"\"\"Based on the following story, generate an one-sentence instruction. Instruction \\\n        must ask to write about a content the story.\n        Only use content from the story to generate the instruction. \\\n        Instruction must never explicitly mention a story. \\\n        Instruction must be self-contained and general. \\\n\n        Example story: Once upon a time, there was a little girl named Lily. \\\n        Lily liked to pretend she was a popular princess. She lived in a big castle \\\n        with her best friends, a cat and a dog. One day, while playing in the castle, \\\n        Lily found a big cobweb. The cobweb was in the way of her fun game. \\\n        She wanted to get rid of it, but she was scared of the spider that lived there. \\\n        Lily asked her friends, the cat and the dog, to help her. They all worked together to clean the cobweb. \\\n        The spider was sad, but it found a new home outside. Lily, the cat, and \\\n        the dog were happy they could play without the cobweb in the way. \\\n        And they all lived happily ever after.\n        \n        Example instruction: Write a story about a little girl named Lily who, \\\n        with the help of her cat and dog friends, overcomes her fear of a spider to \\\n        clean a cobweb in their castle, allowing everyone to play happily ever after. \\\n\n        Provide your response in JSON format with the following structure:\n        {{\"instruction_answer\": \"...\"}}\n        Story:\n        {story}\n        \"\"\"\n    completion = client.chat.completions.create(model=\"gpt-4o-mini\",\n                                                messages=[\n                                                    {\"role\": \"system\",\n                                                    \"content\": \"You are a helpful assistant who \\\n                                                    generates instruction based on the given story. \\\n                                                    Provide your response in JSON format.\",},\n                                                    {\"role\": \"user\", \"content\": prompt},\n                                                    ],\n                                                response_format={\"type\": \"json_object\"},\n                                                max_tokens=1200,\n                                                temperature=0.7,)\n    result = InstructionAnswerSet.from_json(completion.choices[0].message.content, story)\n    # Convert to list of tuples\n    return result.pairs\nNext, we wrap all the above atomic functions into a final function create_instruction_dataset to create the instruction dataset.\ndef create_instruction_dataset(dataset: Dataset, client: OpenAI, num_workers: int = 4) -&gt; Dataset:\n    stories = extract_substory(dataset)\n    instruction_answer_pairs = []\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n        futures = [executor.submit(generate_instruction_answer_pairs, story, client) for story in stories]\n\n        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n            instruction_answer_pairs.extend(future.result())\n\n    instructions, answers = zip(*instruction_answer_pairs)\n    return Dataset.from_dict({\"instruction\": list(instructions), \"output\": list(answers)})\nThe main function orchestrates the entire pipeline:\n\nInitializes the OpenAI client\nLoads the raw TinyStories dataset\nCreates instruction dataset\nPerforms train/test split\nExports the processed dataset to Hugging Face Hub\n\ndef main() -&gt; Dataset:\n    client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n\n    # 1. Load the raw data\n    raw_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:10000]\")\n    print(\"Raw dataset:\")\n    print(raw_dataset.to_pandas())\n\n    # 2. Create instructiondataset\n    instruction_dataset = create_instruction_dataset(raw_dataset, client)\n    print(\"Instruction dataset:\")\n    print(instruction_dataset.to_pandas())\n\n    # 3. Train/test split and export\n    filtered_dataset = instruction_dataset.train_test_split(test_size=0.1)\n    filtered_dataset.push_to_hub(\"tanquangduong/TinyStories_Instruction\")\nFinally, we authenticate with Hugging Face Hub and execute the main pipeline to generate and upload our instruction dataset.\nfrom huggingface_hub import login\n# Log in to the Hugging Face Hub\nlogin(token=userdata.get('HF_TOKEN'))\n\n# Launch the pipeline to create instruction dataset\nmain()",
    "crumbs": [
      "üöÄ AI Station",
      "üß™ **Dataset for LLM**",
      "1. Instruction Dataset for Supervised Fine Tuning"
    ]
  },
  {
    "objectID": "posts/unsloth-qwen-dpo.html#what-is",
    "href": "posts/unsloth-qwen-dpo.html#what-is",
    "title": "Finetuning Qwen2.5-3B using DPO with Unsloth",
    "section": "ü§ù What is",
    "text": "ü§ù What is\nExample of configuration:",
    "crumbs": [
      "üöÄ AI Station",
      "‚ö° **Supervised Fine Tuning (SFT)**",
      "2. DPO Finetuning Qwen2.5-3B with Unsloth"
    ]
  }
]