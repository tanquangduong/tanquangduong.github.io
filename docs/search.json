[
  {
    "objectID": "posts/unsloth-qwen-sft-lora.html",
    "href": "posts/unsloth-qwen-sft-lora.html",
    "title": "Finetuning Qwen2.5-3B with Unscloth",
    "section": "",
    "text": "Large language models (LLMs) are initially trained on vast amounts of unlabeled data to acquire broad general knowledge. However, this pretraining approach has limitations for specialized tasks like Question Answering (QA) due to the facts like (1) The next-token prediction objective used in pretraining is not directly aligned with targeted tasks like QA. (2) General knowledge may be insufficient for domain-specific applications requiring specialized expertise. (3) Publicly available pretraining data may lack up-to-date or proprietary information needed for certain use cases.\nThose senarios are where Supervised Fine-Tuning (SFT) comes into play. It addresses these limitations by adapting pretrained LLMs for specific downstream tasks, by (1) enabling models to learn task-specific patterns and nuances, (2) incorporating domain knowledge not present in general pretraining data, (3) improving performance on targeted applications like QA",
    "crumbs": [
      "üìùPosts",
      "‚ö° **Supervised Fine Tuning**",
      "1. Fine-tune Qwen2.5-3B with LORA"
    ]
  },
  {
    "objectID": "posts/unsloth-qwen-sft-lora.html#sft-pipeline-components",
    "href": "posts/unsloth-qwen-sft-lora.html#sft-pipeline-components",
    "title": "Finetuning Qwen2.5-3B with Unscloth",
    "section": "SFT Pipeline Components",
    "text": "SFT Pipeline Components\nThe SFT pipeline consists of several key stages and components, illustrated in the follwing flowchart: \n\nInputs: Including a raw dataset comprising task-specific examples, along with a pretrained base language model.\nInstruction-Dataset Preparation: In this phase, the raw data undergoes a process of refinement and structuring. This stage involves data cleaning and filtering, which ensures the removal of irrelevant, inconsistent, or low-quality examples. Following this, the generation of instruction-answer pairs takes place, transforming the dataset into a form conducive to instructional tasks and allowing the model to learn through guided examples that are relevant to its intended applications.\nDataset Formatting: During this phase, the prepared data is converted into standardized formats to maintain consistency across diverse implementations. These formats include widely-used structures such as JSON configurations modeled on popular frameworks like Alpaca, ShareGPT, and OpenAI formats. Additionally, examples are organized with the aid of structured chat templates, such as those derived from Alpaca, ChatML, and Llama 3, further enhancing the model‚Äôs ability to engage in coherent, context-aware dialogues.\nCore SFT Process: This stage builds on the well-structured data to fine-tune the base model. During this stage, the model is trained on the formatted instruction dataset using advanced SFT methodologies. Techniques like full fine-tuning, LoRA (Low-Rank Adaptation), or QLoRA (Quantized LoRA) are employed to optimize the model‚Äôs performance while preserving efficiency and adaptability.\nOutput: At the final stage, we will obtain task-specific fine-tuned model.\n\nThis pipeline allows for systematic adaptation of LLMs to targeted applications while leveraging their pretrained knowledge.\nNote that the formatted instruction datasets and chat templates provide a unified way to present diverse training examples to the model. If we fine-tune the pretrained base model, we can choose any chat templates. However, if we fine-tune an instruct model, we need to use the sample template.",
    "crumbs": [
      "üìùPosts",
      "‚ö° **Supervised Fine Tuning**",
      "1. Fine-tune Qwen2.5-3B with LORA"
    ]
  },
  {
    "objectID": "posts/unsloth-qwen-sft-lora.html#sft-techniques",
    "href": "posts/unsloth-qwen-sft-lora.html#sft-techniques",
    "title": "Finetuning Qwen2.5-3B with Unscloth",
    "section": "SFT techniques",
    "text": "SFT techniques\nThere are three main types of Supervised Fine-Tuning (SFT) for large language models:\n\nFull Model Fine-Tuning. This approach involves updating all parameters of the pre-trained model. It offers maximum flexibility in adapting the model to specialized tasks. It often yields significant performance improvements but requires substantial computational resources.\nFeature-Based Fine-Tuning. This method focuses on extracting features from the pre-trained model and used as input for another model or classifier. The main pre-trained model remains unchanged. It‚Äôs less resource-intensive and provides faster results, making it suitable when computational power is limited.\nParameter-Efficient Fine-Tuning (PEFT). PEFT techniques aim to fine-tune models more efficiently. Only a portion of the model‚Äôs weights are modified, leaving the fundamental language understanding intact. It adds task-specific layers or adapters to the pre-trained model. Significantly reduces computational costs compared to full fine-tuning while still achieving competitive performance.\n\nThe choice between these approaches is often based on the specific requirements of the task, available computational resources, and desired model performance.\nIn this article, we will discuss more about the two most popular and effective PEFT techniques: LoRA and QLoRA.",
    "crumbs": [
      "üìùPosts",
      "‚ö° **Supervised Fine Tuning**",
      "1. Fine-tune Qwen2.5-3B with LORA"
    ]
  },
  {
    "objectID": "posts/unsloth-qwen-sft-lora.html#peft-with-lora-and-qlora",
    "href": "posts/unsloth-qwen-sft-lora.html#peft-with-lora-and-qlora",
    "title": "Finetuning Qwen2.5-3B with Unscloth",
    "section": "PEFT with LoRA and QLoRA",
    "text": "PEFT with LoRA and QLoRA\nLoRA (Low-Rank Adaptation) is introduced in 2021 in the paper ‚ÄúLoRA: Low-Rank Adaptation of Large Language Models‚Äù by Adward et al.. It then has gained widespread adoption. It is a cost-effective and efficient method for adapting pretrained language models to specific tasks by freezing most of the model‚Äôs parameters and updating only a small number of task-specific weights. This approach leverages adapters to reduce the training overhead, making it an attractive solution for limited compute scenarios.\nQLoRA (Quantized Low-Rank Adaptation) is an extension of the LoRA technique. It is proposed in the paper ‚ÄúQLoRA: Efficient Finetuning of Quantized LLMs‚Äù by Tim et al.¬†in 2023. It quantizes the weight of each pretrained parameter to 4 bits (from the typical 32 bits). This results in significant memory savings and enables running large language models on a single GPU\nWhen deciding between LoRA and QLoRA for fine-tuning large language models, key considerations revolve around hardware, model size, speed, and accuracy needs.\nLoRA generally requires more GPU memory than QLoRA but is more efficient than full fine-tuning, making it suitable for systems with moderate to high GPU memory capacity. QLoRA, on the other hand, significantly lowers memory demands, making it more suitable for devices with limited memory resources. While LoRA is often faster, QLoRA incurs slight speed trade-offs due to quantization steps but offers superior memory efficiency, enabling fine-tuning of larger models on constrained hardware.\nAccuracy and computational efficiency also differ between the two methods. LoRA typically yields stable and precise results, whereas QLoRA‚Äôs use of quantization may lead to minor accuracy losses, though it can sometimes reduce overfitting. When it comes to specific needs, LoRA is ideal if preserving full model precision is vital, whereas QLoRA shines for extremely large models or environments with tight memory constraints. QLoRA also supports varying levels of quantization (e.g., 8-bit, 4-bit, or even 2-bit), adding flexibility but at the cost of increased implementation complexity.\nTo implement LoRA and QLoRA in practice, among other frameworks like PEFT/Bitsandbytes (Hugging Face), TorchTune, Axolotl, ‚Ä¶, we use the Unscloth framework in this article. This is an innovative open-source framework designed to revolutionize the fine-tuning and training of large language models. So it is worth to discuss more about Unsloth in the next section.",
    "crumbs": [
      "üìùPosts",
      "‚ö° **Supervised Fine Tuning**",
      "1. Fine-tune Qwen2.5-3B with LORA"
    ]
  },
  {
    "objectID": "posts/unsloth-qwen-sft-lora.html#why-unsloth",
    "href": "posts/unsloth-qwen-sft-lora.html#why-unsloth",
    "title": "Finetuning Qwen2.5-3B with Unscloth",
    "section": "Why Unsloth?",
    "text": "Why Unsloth?\nUnsloth is developed by Daniel Han and Michael Han at Unsloth AI. This framework addresses some of the most significant challenges in LLM training, particularly speed and resource efficiency. Let‚Äôs check out some of its remarkable features and benefits:\n\nSpeed Improvements. It makes an impressive acceleration in training speed, up to 30 times faster performance compared to other advanced methods like Flash Attention 2 (FA2), completing tasks like the Alpaca benchmark in just 3 hours instead of the usual 85. This dramatic reduction in training time allows us to iterate more quickly.\nMemory Efficiency. It achieves up to 90% reduction in memory usage compared to FA2.\nAccuracy Preservation and Enhancement. Despite its focus on speed and efficiency, Unsloth maintains model accuracy with 0% performance loss, or up to 20% increase in accuracy using their MAX offering.\nHardware Flexibility. It is designed to be hardware-agnostic, supporting a wide range of GPUs including those from NVIDIA, AMD, and Intel. This compatibility ensures that users can leverage Unsloth‚Äôs benefits regardless of their existing hardware setup.",
    "crumbs": [
      "üìùPosts",
      "‚ö° **Supervised Fine Tuning**",
      "1. Fine-tune Qwen2.5-3B with LORA"
    ]
  },
  {
    "objectID": "posts/unsloth-qwen-sft-lora.html#use-case",
    "href": "posts/unsloth-qwen-sft-lora.html#use-case",
    "title": "Finetuning Qwen2.5-3B with Unscloth",
    "section": "Use-case",
    "text": "Use-case\nIn this article, we illustrate a specific use-case: Supervised fine-tuning Qwen2.5-3B model using LoRA and QLoRA, to create a story generator for children.\nFor the instruction dataset preparation stage, we use an instruction dataset TinyStories_Instruction which contains instruction-story pairs. I have prepared this dataset in my previsous post, if you have not read it yet, I recommend you to check it out. The stories in this dataset are short and synthetically generated by GPT-3.5 and GPT-4 with a limited vocabulary, making it highly suitable for our intended 5-year-old readers. While, the instruction corresponding to each story is also created synthetically using GPT-4o-mini.\nFor the pretrained language model, we use Qwen2.5-3B, a pretrained language model containing 3.09 billion parameters. We choose this for our use-case as its reasonable size, making it powerful yet suitable for fine-tuning even on resource-constrained platforms like Google Colab.\nFor the implementation part, we leverage Unsloth for speed and memory efficiency reasons.",
    "crumbs": [
      "üìùPosts",
      "‚ö° **Supervised Fine Tuning**",
      "1. Fine-tune Qwen2.5-3B with LORA"
    ]
  },
  {
    "objectID": "posts/unsloth-qwen-sft-lora.html#fine-tuning-implementation",
    "href": "posts/unsloth-qwen-sft-lora.html#fine-tuning-implementation",
    "title": "Finetuning Qwen2.5-3B with Unscloth",
    "section": "Fine-Tuning Implementation",
    "text": "Fine-Tuning Implementation\nTo achieve the fine-tuning, we will utilize the following libraries and methods:\nStep 1: Import Necessary Libraries\nimport os\nimport comet_ml\nimport torch\nfrom trl import SFTTrainer\nfrom datasets import load_dataset, concatenate_datasets\nfrom transformers import TrainingArguments, TextStreamer\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\nfrom google.colab import userdata\nStep 2: Comet ML Login\nWe leverage Comet ML for real-time monitoring and tracking our fine-tuning experiments. Comet ML allows you to automatically track a wide range of metrics, parameters, and artifacts during the model training process. This includes training loss, gradient norms, hyperparameters, code versions, and more.\nIn addition, Comet ML makes it easy to compare different experiments, helping you understand how changes in code, hyperparameters, or data affect model performance. The platform provides workspaces and sharing capabilities, enabling teams to collaborate more effectively on ML projects. To dicover more about its features and benefits, please check out Comet ML‚Äôs website.\ncomet_ml.login(project_name=\"sft-lora-unsloth\")\nStep 3: Load Pretrained Model and Tokenizer\nNext, we use FastLanguageModel class from Unsloth with the .from_pretrained() method to load Qwen2.5-3B model and its corresponding tokenizer. We specify the max sequence length as 2048 in this use-case. Then, the load_in_4bit argument indicates if we want to use QLoRA (assign True), else LoRA.\nmax_seq_length = 2048\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"Qwen/Qwen2.5-0.5B\",\n    max_seq_length=max_seq_length,\n    load_in_4bit=False,\n    )\nStep 4: Apply LoRA Adaptation\nThen, we set up LoRA configurations for our loaded model, including the rank r as 32, alpha as 32, no dropout and target modules as linear layers. This is where leveraging experiment tracking and comparison, we can apply hyperparameter tuning to find out the best set of parameters.\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=32,\n    lora_alpha=32,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n    )\nStep 5: Formatting Dataset\nNext, we need to format and map the instruction dataset into a specific text template, using Alpaca template in this example.\n# Template for formatting instruction-output pairs following Alpaca dataset format\nalpaca_template = \"\"\"Below is an instruction that describes a task.\nWrite a response that appropriately completes the request.\n### Instruction:\n{}\n### Response:\n{}\"\"\"\n\n# Get the end of sequence token from the tokenizer\nEOS_TOKEN = tokenizer.eos_token\n\ndef format_samples(examples):\n    \"\"\"\n    Format instruction-output pairs into training samples.\n    Args:\n        examples: Dictionary containing 'instruction' and 'output' lists\n    Returns:\n        Dictionary with formatted 'text' list\n    \"\"\"\n    text = []\n    # Zip instruction-output pairs together and format each pair\n    for instruction, output in zip(examples[\"instruction\"], examples[\"output\"], strict=False):\n        # Insert instruction & output into template and append EOS token\n        message = alpaca_template.format(instruction, output) + EOS_TOKEN\n        text.append(message)\n\n    return {\"text\": text}\n\n# Apply formatting to entire dataset:\n# - Process in batches for efficiency\n# - Remove original columns since they're now formatted into 'text'\ndataset = dataset.map(format_samples, batched=True, remove_columns=dataset.column_names)\nStep 6: Setting Up the Trainer and Lauching the Training\nWhen the instruction dataset is formatted and prepared, and the model is loaded with adapted parameters and architectures (e.g., LoRA or QLoRA), we utilize the SFTTrainer class from the TRL library for supervised fine-tuning.\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=True,\n    args=TrainingArguments(\n        learning_rate=1e-5,\n        lr_scheduler_type=\"linear\",\n        per_device_train_batch_size=2,\n        per_device_eval_batch_size=2,\n        gradient_accumulation_steps=8,\n        num_train_epochs=3,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=1,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        warmup_steps=10,\n        output_dir=\"output\",\n        report_to=\"comet_ml\",\n        seed=0,\n        ),\n    )\ntrainer.train()\nStep 7: Experiment tracking\nDuring training, we can track the training loss or other metrics using the Comet ML platform. The plots should look like the following:\n\n\n\nML Experiment Tracking using Comet ML\n\n\nStep 8: Model Inference\nWhen the fine-tuning is finished, we can perform a quick test on the fine-tuned model.\n# Switch model to inference mode (disables training-specific components)\nFastLanguageModel.for_inference(model)\n\n# Format the story prompt using Alpaca template\nmessage = alpaca_template.format(\"Write a story about a humble little bunny \\\nnamed Ben who follows a mysterious trail in the woods, \\\ndiscovering beautiful flowers, new friends, and a lovely pond along the way.\", \"\")\n\n# Convert text to tokens, create PyTorch tensors, and move to GPU\ninputs = tokenizer([message], return_tensors=\"pt\").to(\"cuda\")\n\n# Initialize streamer for real-time token-by-token text output\ntext_streamer = TextStreamer(tokenizer)\n\n# Generate text from the model:\n# - streamer: Enables streaming output\n# - max_new_tokens: Limits response length\n# - use_cache: Enables KV-cache for faster generation\n# Result assigned to _ since we only care about streamed output\n_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=256, use_cache=True)\nExample of inference output looks like this:\n\n\n\nInference Output Example\n\n\nStep 9: Save and Push to Hugging Face Hub\nNow, if we are satisfied with the fine-tuned model‚Äôs performance, it‚Äôs time to log in and push it to the Hugging Face Hub for later use.\nfrom huggingface_hub import login\n# Log in to the Hugging Face Hub\nlogin(token=userdata.get('HF_TOKEN'))\n\nmodel.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\nmodel.push_to_hub_merged(\"tanquangduong/Qwen2.5-0.5B-Instruct-TinyStories\", tokenizer, save_method=\"merged_16bit\")",
    "crumbs": [
      "üìùPosts",
      "‚ö° **Supervised Fine Tuning**",
      "1. Fine-tune Qwen2.5-3B with LORA"
    ]
  },
  {
    "objectID": "posts/unsloth-qwen-sft-lora.html#conclusion",
    "href": "posts/unsloth-qwen-sft-lora.html#conclusion",
    "title": "Finetuning Qwen2.5-3B with Unscloth",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we discuss fine-tuning LLMs for specialized tasks, such as Question Answering for a story generator, which general pretraining often falls short of due to limited instructive data and objectives. Supervised Fine-Tuning addresses this by refining LLMs using instruction datasets, structured formats, and techniques like LoRA and QLoRA to optimize performance and resource efficiency. LoRA focuses on selective parameter tuning, while QLoRA adds memory-efficient quantization, making it suitable for constrained hardware.\nAdditionally, we utilize the Unsloth framework for efficient and fast fine-tuning, Hugging Face‚Äôs TRL for setting up the training process, Comet ML for real-time tracking of fine-tuning experiments, and Hugging Face Hub for dataset and model storage and access.\nWe demonstrated through an example of adapting a custom instruction dataset to fine-tune the Qwen2.5-3B model, resulting in a fine-tuned model that functions as a story generator capable of creating children‚Äôs stories based on a simple instruction prompt.",
    "crumbs": [
      "üìùPosts",
      "‚ö° **Supervised Fine Tuning**",
      "1. Fine-tune Qwen2.5-3B with LORA"
    ]
  },
  {
    "objectID": "posts/instruction-dataset-sft.html",
    "href": "posts/instruction-dataset-sft.html",
    "title": "Instruction Dataset Creation for Supervised Fine-Tuning",
    "section": "",
    "text": "Creating a tailored instruction dataset for fine-tuning a language model is a critical step in enhancing the model‚Äôs capabilities for specialized tasks. This guide provides a step-by-step example of how to create an instruction dataset.\nBefore starting, it is essential to define the dataset‚Äôs intended purpose. Are you developing a chatbot, a story generator, or a question-answering system? Clearly understanding the desired model behavior will guide the type and structure of the data you prepare.\nIn this guide, our goal is to create an instruction dataset suitable for fine-tuning a pretrained Large/Small Language Model to produce a story generator dedicated for 5-year-olds.",
    "crumbs": [
      "üìùPosts",
      "üß™ **Data Processing**",
      "1. Instruction Dataset"
    ]
  },
  {
    "objectID": "posts/instruction-dataset-sft.html#use-case",
    "href": "posts/instruction-dataset-sft.html#use-case",
    "title": "Instruction Dataset Creation for Supervised Fine-Tuning",
    "section": "Use Case",
    "text": "Use Case\nLet‚Äôs examine our use-case for this guide.\nThe raw dataset TinyStories, introduced in the paper TinyStories: How Small Can Language Models Be and Still Speak Coherent English? by Ronen Eldan and Yuanzhi Li. This dataset consists of short, synthetically generated stories created by GPT-3.5 and GPT-4 with a limited vocabulary, making it highly suitable for our intended 5-year-old readers. The dataset is divided into two splits: train (2.12M rows) and validation (22K rows). For this use case, we will use the train split with 10K rows.\nBelow is a sample view of the TinyStories dataset on the Hugging Face Dataset Hub: \nTo create the instruction dataset, we will generate synthetic instruction sentences that correspond to each story in the TinyStories dataset.",
    "crumbs": [
      "üìùPosts",
      "üß™ **Data Processing**",
      "1. Instruction Dataset"
    ]
  },
  {
    "objectID": "posts/instruction-dataset-sft.html#implementation",
    "href": "posts/instruction-dataset-sft.html#implementation",
    "title": "Instruction Dataset Creation for Supervised Fine-Tuning",
    "section": "Implementation",
    "text": "Implementation\n\n1. Load Required Packages\nLet‚Äôs begin by loading the necessary packages.\nimport concurrent.futures\nimport json\nimport re\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import List, Tuple\nfrom datasets import Dataset, load_dataset, concatenate_datasets\nfrom openai import OpenAI\nfrom tqdm.auto import tqdm\nfrom google.colab import userdata\n\n\n2. Define Modular Functions\nNext, we define key functions to structure our pipeline.\nExtracting Stories The get_story_list function creates a list of stories from the raw dataset:\ndef get_story_list(dataset):\n    return [example['text'] for example in dataset]\nManaging Instruction-Answer Pairs\nThe InstructionAnswerSet class defines a structure to store and manage instruction-answer pairs, with methods to create instances from JSON and iterate over pairs:\nclass InstructionAnswerSet:\n    def __init__(self, pairs: List[Tuple[str, str]]):\n        self.pairs = pairs\n\n    @classmethod\n    def from_json(cls, json_str: str, story: str) -&gt; 'InstructionAnswerSet':\n        data = json.loads(json_str)\n        pairs = [(data['instruction_answer'], story)]\n        return cls(pairs)\n\n    def __iter__(self):\n        return iter(self.pairs)\nGenerating Instruction-Answer Pairs\nThe generate_instruction_answer_pairs function takes a story and an OpenAI client as inputs to generate instruction-answer pairs using GPT-4. The function crafts a prompt to create relevant instructions while adhering to specific formatting requirements:\ndef generate_instruction_answer_pairs(story: str, client: OpenAI) -&gt; List[Tuple[str, str]]:\n    prompt = f\"\"\"Based on the following story, generate an one-sentence instruction. Instruction \\\n        must ask to write about a content the story.\n        Only use content from the story to generate the instruction. \\\n        Instruction must never explicitly mention a story. \\\n        Instruction must be self-contained and general. \\\n\n        Example story: Once upon a time, there was a little girl named Lily. \\\n        Lily liked to pretend she was a popular princess. She lived in a big castle \\\n        with her best friends, a cat and a dog. One day, while playing in the castle, \\\n        Lily found a big cobweb. The cobweb was in the way of her fun game. \\\n        She wanted to get rid of it, but she was scared of the spider that lived there. \\\n        Lily asked her friends, the cat and the dog, to help her. They all worked together to clean the cobweb. \\\n        The spider was sad, but it found a new home outside. Lily, the cat, and \\\n        the dog were happy they could play without the cobweb in the way. \\\n        And they all lived happily ever after.\n        \n        Example instruction: Write a story about a little girl named Lily who, \\\n        with the help of her cat and dog friends, overcomes her fear of a spider to \\\n        clean a cobweb in their castle, allowing everyone to play happily ever after. \\\n\n        Provide your response in JSON format with the following structure:\n        {{\"instruction_answer\": \"...\"}}\n        Story:\n        {story}\n        \"\"\"\n    completion = client.chat.completions.create(model=\"gpt-4o-mini\",\n                                                messages=[\n                                                    {\"role\": \"system\",\n                                                    \"content\": \"You are a helpful assistant who \\\n                                                    generates instruction based on the given story. \\\n                                                    Provide your response in JSON format.\",},\n                                                    {\"role\": \"user\", \"content\": prompt},\n                                                    ],\n                                                response_format={\"type\": \"json_object\"},\n                                                max_tokens=1200,\n                                                temperature=0.7,)\n    result = InstructionAnswerSet.from_json(completion.choices[0].message.content, story)\n    # Convert to list of tuples\n    return result.pairs\nCreating the Instruction Dataset\nWe now wrap the previous functions into a final function create_instruction_dataset:\ndef create_instruction_dataset(dataset: Dataset, client: OpenAI, num_workers: int = 4) -&gt; Dataset:\n    stories = extract_substory(dataset)\n    instruction_answer_pairs = []\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n        futures = [executor.submit(generate_instruction_answer_pairs, story, client) for story in stories]\n\n        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n            instruction_answer_pairs.extend(future.result())\n\n    instructions, answers = zip(*instruction_answer_pairs)\n    return Dataset.from_dict({\"instruction\": list(instructions), \"output\": list(answers)})\n\n\n3. Orchestrating the Pipeline\nNest, the main function orchestrates the entire pipeline, which includes:\n+ Initialize the OpenAI client\n+ Load the raw TinyStories dataset\n+ Create instruction dataset\n+ Perform train/test split\n+ Push the processed dataset to Hugging Face Hub\ndef main() -&gt; Dataset:\n    # Initializes the OpenAI client\n    client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n\n    # Load the raw data\n    raw_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:10000]\")\n\n    # Create instructiondataset\n    instruction_dataset = create_instruction_dataset(raw_dataset, client)\n\n    # Train/test split and export\n    filtered_dataset = instruction_dataset.train_test_split(test_size=0.1)\n\n    # Push the processed dataset to Hugging Face Hub\n    filtered_dataset.push_to_hub(\"tanquangduong/TinyStories_Instruction\")\n\n\n4. Authenticating and Running the Pipeline\nThen, we authenticate with the Hugging Face Hub for later dataset uploading and execute the pipeline.\nfrom huggingface_hub import login\n# Log in to the Hugging Face Hub\nlogin(token=userdata.get('HF_TOKEN'))\n\n# Launch the pipeline to create instruction dataset\nmain()",
    "crumbs": [
      "üìùPosts",
      "üß™ **Data Processing**",
      "1. Instruction Dataset"
    ]
  },
  {
    "objectID": "posts/instruction-dataset-sft.html#resulting-instruction-dataset",
    "href": "posts/instruction-dataset-sft.html#resulting-instruction-dataset",
    "title": "Instruction Dataset Creation for Supervised Fine-Tuning",
    "section": "Resulting Instruction Dataset",
    "text": "Resulting Instruction Dataset\nAfter running the above pipeline, the resulting instruction dataset will look like this:",
    "crumbs": [
      "üìùPosts",
      "üß™ **Data Processing**",
      "1. Instruction Dataset"
    ]
  },
  {
    "objectID": "posts/instruction-dataset-sft.html#conclusion",
    "href": "posts/instruction-dataset-sft.html#conclusion",
    "title": "Instruction Dataset Creation for Supervised Fine-Tuning",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, this guide demonstrated the creation of an instruction dataset tailored for fine-tuning. We first defined the purpose of fine-tuning and structured the dataset accordingly. By leveraging GPT-4, we generated instructions for each story using best practices in prompt engineering, including precise instructions, a one-shot example, and a specified output format. Finally, the processed dataset was uploaded to the Hugging Face Hub for future use.",
    "crumbs": [
      "üìùPosts",
      "üß™ **Data Processing**",
      "1. Instruction Dataset"
    ]
  },
  {
    "objectID": "draft/unsloth-qwen-dpo.html#fine-tuning-with-preference-alignment",
    "href": "draft/unsloth-qwen-dpo.html#fine-tuning-with-preference-alignment",
    "title": "Finetuning Qwen2.5-3B using DPO with Unsloth",
    "section": "Fine-Tuning with Preference Alignment",
    "text": "Fine-Tuning with Preference Alignment"
  },
  {
    "objectID": "draft/unsloth-qwen-dpo.html#fine-tuning-with-dpo",
    "href": "draft/unsloth-qwen-dpo.html#fine-tuning-with-dpo",
    "title": "Finetuning Qwen2.5-3B using DPO with Unsloth",
    "section": "Fine Tuning with DPO",
    "text": "Fine Tuning with DPO\nDirect Preference Optimization (DPO) is a technique used for fine-tuning language models in scenarios where it is critical to optimize for specific outputs based on preferences, such as ranking user responses. By using DPO with the LoRA (Low-Rank Adaptation) technique, we can leverage efficient finetuning by only modifying a small subset of the model‚Äôs parameters. This keeps training costs low while maintaining flexibility in the model‚Äôs outputs."
  },
  {
    "objectID": "draft/unsloth-qwen-dpo.html#use-case",
    "href": "draft/unsloth-qwen-dpo.html#use-case",
    "title": "Finetuning Qwen2.5-3B using DPO with Unsloth",
    "section": "Use Case",
    "text": "Use Case\nFor this example, our use-case involves using DPO to fine-tune Qwen2.5-3B to generate engaging Tiny Stories tailored for children. By providing prompts and desired responses (and contrasting rejected responses), we can better shape the model to deliver coherent, engaging, and task-appropriate stories for various instructions.\nQwen2.5-3B is a large-scale, pretrained language model comprising 3.09 billion parameters. This model provides a balance between expressiveness and computational feasibility, making it well-suited for finetuning using more specialized optimization strategies. In this guide, we will explore using Direct Preference Optimization (DPO) within the Unsloth framework to fine-tune Qwen2.5-3B. This approach focuses on aligning model responses to specific preferences and behaviors, yielding fine-tuned models that can respond to tasks more accurately."
  },
  {
    "objectID": "draft/unsloth-qwen-dpo.html#implementation",
    "href": "draft/unsloth-qwen-dpo.html#implementation",
    "title": "Finetuning Qwen2.5-3B using DPO with Unsloth",
    "section": "Implementation",
    "text": "Implementation\nStep 1: Import Necessary Libraries\nfrom unsloth import PatchDPOTrainer\nPatchDPOTrainer()\n\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, TextStreamer\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\nfrom trl import DPOConfig, DPOTrainer\nfrom google.colab import userdata\nStep 2: Initialize Comet ML for Experiment Tracking\nimport comet_ml\ncomet_ml.login(project_name=\"dpo-lora-unsloth\")\nStep 3: Load Pretrained Model and Tokenizer\nmax_seq_length = 2048\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"Qwen/Qwen2.5-3B\",\n    max_seq_length=max_seq_length,\n    load_in_4bit=False,\n    )\nStep 4: Apply LoRA Adaptation\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=32,\n    lora_alpha=32,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n    )\nStep 5: Dataset Preparation\nFormat the dataset using a specific template and split it for training and testing.\nalpaca_template = \"\"\"Below is an instruction that describes a task.\nWrite a response that appropriately completes the request.\n### Instruction:\n{}\n### Response:\n\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token\ndef format_samples(example):\n    example[\"prompt\"] = alpaca_template.format(example[\"prompt\"])\n    example[\"chosen\"] = example['chosen'] + EOS_TOKEN\n    example[\"rejected\"] = example['rejected'] + EOS_TOKEN\n    return {\n        \"prompt\": example[\"prompt\"],\n        \"chosen\": example[\"chosen\"],\n        \"rejected\": example[\"rejected\"]\n    }\ndataset = dataset.map(format_samples)\ndataset = dataset.train_test_split(test_size=0.05)\nStep 6: Training Using DPOTrainer\nConfigure and train the model using the DPOTrainer class.\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=None,\n    tokenizer=tokenizer,\n    beta=0.5,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    max_length=max_seq_length//2,\n    max_prompt_length=max_seq_length//2,\n    args=DPOConfig(\n        learning_rate=2e-6,\n        lr_scheduler_type=\"linear\",\n        per_device_train_batch_size=2,\n        per_device_eval_batch_size=2,\n        gradient_accumulation_steps=8,\n        num_train_epochs=1,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        warmup_steps=10,\n        output_dir=\"output\",\n        eval_strategy=\"steps\",\n        eval_steps=0.2,\n        logging_steps=1,\n        report_to=\"comet_ml\",\n        seed=0,\n        ),\n)\n\n\ntrainer.train()\nStep 7: Model Inference\nGenerate a response using the fine-tuned model.\nFastLanguageModel.for_inference(model)\nmessage = alpaca_template.format(\"Write a story about a humble little bunny \\\nnamed Ben who follows a mysterious trail in the woods, \\\ndiscovering beautiful flowers, new friends, and a lovely pond along the way.\", \"\")\ninputs = tokenizer([message], return_tensors=\"pt\").to(\"cuda\")\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=2048, use_cache=True)\nStep 8: Save and Push to Hugging Face Hub\nfrom huggingface_hub import login\n# Log in to the Hugging Face Hub\nlogin(token=userdata.get('HF_TOKEN'))\n\nmodel.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\nmodel.push_to_hub_merged(\"tanquangduong/Qwen2.5-3B-DPO-TinyStories\", tokenizer, save_method=\"merged_16bit\")"
  },
  {
    "objectID": "draft/unsloth-qwen-dpo.html#inference",
    "href": "draft/unsloth-qwen-dpo.html#inference",
    "title": "Finetuning Qwen2.5-3B using DPO with Unsloth",
    "section": "Inference",
    "text": "Inference\nUsing the fine-tuned model for generating outputs:\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"tanquangduong/Qwen2.5-3B-DPO-TinyStories\")\nmodel = AutoModelForCausalLM.from_pretrained(\"tanquangduong/Qwen2.5-3B-DPO-TinyStories\")\nalpaca_template = \"\"\"Below is an instruction that describes a task.\nWrite a response that appropriately completes the request.\n### Instruction:\n{}\n### Response:\n{}\"\"\"\n\nmodel = model.to(\"cuda\")\nFastLanguageModel.for_inference(model)\nmessage = alpaca_template.format(\"Write a story about a humble little bunny \\\nnamed Ben who follows a mysterious trail in the woods, \\\ndiscovering beautiful flowers, new friends, and a lovely pond along the way.\", \"\")\ninputs = tokenizer([message], return_tensors=\"pt\").to(\"cuda\")\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=2048, use_cache=True)"
  },
  {
    "objectID": "draft/unsloth-qwen-dpo.html#conclusion",
    "href": "draft/unsloth-qwen-dpo.html#conclusion",
    "title": "Finetuning Qwen2.5-3B using DPO with Unsloth",
    "section": "Conclusion",
    "text": "Conclusion\nIn this guide, we demonstrated how to fine-tune Qwen2.5-3B using Direct Preference Optimization (DPO) within the Unsloth framework. By leveraging LoRA for parameter-efficient adaptation, we tailored the model‚Äôs output behavior to better suit our target use case of generating child-friendly Tiny Stories. This methodology highlights the effectiveness of combining DPO and LoRA to achieve powerful, specialized fine-tuned models."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I‚Äôm Quang. I am an AI/ML engineer. I live and work in Paris, France."
  },
  {
    "objectID": "draft/course.html",
    "href": "draft/course.html",
    "title": "Hands-On Generative AI Engineering with Large Language Model",
    "section": "",
    "text": "I am thrilled to introduce my Udemy course Hands-On Generative AI Engineering with Large Language Model.\nThis course equips you with a wide range of tools, frameworks, and techniques to create your GenAI applications using Large Language Models, including Python, PyTorch, LangChain, LlamaIndex, Hugging Face, FAISS, Chroma, Tavily, Streamlit, Gradio, FastAPI, Docker, and more.\nThis hands-on course covers essential topics such as implementing Transformers from scratch, fine-tuning Transformer models for downstream tasks, prompt engineering, vector embeddings, vector stores, and creating cutting-edge AI applications like AI Assistants, Chatbots, Retrieval-Augmented Generation (RAG) systems, autonomous agents, and serving your GenAI applications from scratch using REST APIs and Docker containerization.\nBy the end of this course, you will have the practical skills and theoretical knowledge needed to develop and serving your own LLM-based applications.\nIf you are interested in these above topics, checking it out."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "‚ú® AI/ML, MLOps, LLMOps",
    "section": "",
    "text": "Instruction Dataset Creation for Supervised Fine-Tuning\n\n\nLeveraging LLMs for creating instruction dataset for Supervised Fine-Tuning\n\n\n\ninstruction-dataset\n\n\n\n\n\n\nAug 27, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nPreference Dataset Creation for DPO Fine-Tuning\n\n\nLeveraging LLMs for creating preference dataset for DPO Fine-Tuning\n\n\n\npreference-dataset\n\n\n\n\n\n\nAug 27, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFinetuning Qwen2.5-3B with Unscloth\n\n\nFinetuning Qwen2.5-3B with SFT-Lora using Unsloth on TinyStories instruction dataset\n\n\n\nFinetuning\n\n\nLORA\n\n\nUnsloth\n\n\n\n\n\n\nAug 24, 2024\n\n\n12 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/preference-dataset-dpo.html#introduction",
    "href": "posts/preference-dataset-dpo.html#introduction",
    "title": "Preference Dataset Creation for DPO Fine-Tuning",
    "section": "Introduction",
    "text": "Introduction\nDirect Preference Optimization (DPO) is a technique used to align AI-generated outputs with human preferences by optimizing language models. To achieve this, a preference dataset is required, containing data that enables models to understand which responses are preferred by humans and which are not. In this article, we‚Äôll walk through a code implementation to create such a dataset using Python, OpenAI‚Äôs API, and Hugging Face‚Äôs Datasets library.",
    "crumbs": [
      "üìùPosts",
      "üß™ **Data Processing**",
      "2. Preference Dataset"
    ]
  },
  {
    "objectID": "posts/preference-dataset-dpo.html#components-of-a-preference-dataset-for-dpo",
    "href": "posts/preference-dataset-dpo.html#components-of-a-preference-dataset-for-dpo",
    "title": "Preference Dataset Creation for DPO Fine-Tuning",
    "section": "Components of a Preference Dataset for DPO",
    "text": "Components of a Preference Dataset for DPO\nA preference dataset typically includes:\nPrompts: Inputs or questions given to the AI model. Chosen Responses: AI-generated responses preferred by human evaluators. Rejected Responses: Less preferred responses or responses not selected by human evaluators. By providing this structure, the dataset allows a model to learn which responses are preferable, making it better aligned with human preferences.",
    "crumbs": [
      "üìùPosts",
      "üß™ **Data Processing**",
      "2. Preference Dataset"
    ]
  },
  {
    "objectID": "posts/preference-dataset-dpo.html#our-use-case",
    "href": "posts/preference-dataset-dpo.html#our-use-case",
    "title": "Preference Dataset Creation for DPO Fine-Tuning",
    "section": "Our use-case",
    "text": "Our use-case\nIn our previous post, we created an instruction dataset, TinyStories_Instruction, from the raw TinyStories dataset. This dataset was specifically designed for fine-tuning a pretrained Large/Small Language Model using LORA/QLORA to develop a story generator tailored to 5-year-olds.\nIn this guide, we take the next step by creating a preference dataset from the previously generated instruction dataset. This dataset is used for fine-tuning a pretrained Large/Small Language Model through Direct Preference Optimization (DPO), enhancing our story generator to align even better with human preferences and produce engaging, age-appropriate content for young children.\nThe process for creating a preference dataset is illustrated below:",
    "crumbs": [
      "üìùPosts",
      "üß™ **Data Processing**",
      "2. Preference Dataset"
    ]
  },
  {
    "objectID": "posts/preference-dataset-dpo.html#implementation",
    "href": "posts/preference-dataset-dpo.html#implementation",
    "title": "Preference Dataset Creation for DPO Fine-Tuning",
    "section": "Implementation",
    "text": "Implementation\nThis implementation involves a series of steps: extracting data, generating AI responses, and creating preference triplets.\nimport concurrent.futures\nimport json\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import List, Tuple\nfrom datasets import Dataset, load_dataset, concatenate_datasets\nfrom openai import OpenAI\nfrom tqdm.auto import tqdm\nfrom google.colab import userdata\n1. Data Extraction Function\nThe extract_ground_instruction_story function extracts pairs of instructions and desired outputs from a given dataset.\ndef extract_ground_instruction_story(dataset):\n    return [(example['instruction'], example['output']) for example in dataset]\n2. Creating a PreferenceSet Class\nThe PreferenceSet class manages and stores the triples of (instruction, generated story, desired story).\nclass PreferenceSet:\n    def __init__(self, triples: List[Tuple[str, str, str]]):\n        self.triples = triples\n\n    @classmethod\n    def from_json(cls, json_str: str, instruction, desired_story) -&gt; 'PreferenceSet':\n        data = json.loads(json_str)\n        triples = [(instruction, data['generated_story'], desired_story)]\n        return cls(triples)\n\n    def __iter__(self):\n        return iter(self.triples)\n3. Generating Preference-Response Triplets\nThis function generates a story using OpenAI‚Äôs API and returns a preference triple in the format (instruction, generated response, desired response).\ndef generate_preference_answer_triples(instruction: str, desired_story: str, client: OpenAI) -&gt; List[Tuple[str, str, str]]:\n    prompt = f\"\"\"Based on the following instruction, generate a story. \\\n        Story should be no longer than 50 words. Story uses several complex words or structures \\\n        that are not suitable for 5-year-olds.\n\n        Provide your response in JSON format with the following structure:\n        {{\"generated_story\": \"...\"}}\n\n        Instruction:\n        {instruction}\n        \"\"\"\n    completion = client.chat.completions.create(model=\"gpt-4o-mini\",\n                                                    messages=[\n                                                        {\"role\": \"system\",\n                                                        \"content\": \"You are a helpful assistant who \\\n                                                        generates story based on the given instruction. \\\n                                                        Provide your response in JSON format.\",},\n                                                        {\"role\": \"user\", \"content\": prompt},\n                                                        ],\n                                                    response_format={\"type\": \"json_object\"},\n                                                    max_tokens=512,\n                                                    temperature=0.2,)\n    result = PreferenceSet.from_json(completion.choices[0].message.content, instruction, desired_story)\n\n    # Convert to list of tuples\n    return result.triples\n4. Creating the Preference Dataset\nThis function creates a dataset using the extracted stories and generated responses.\ndef create_preference_dataset(dataset: Dataset, client: OpenAI, num_workers: int = 4) -&gt; Dataset:\n    stories = extract_ground_instruction_story(dataset)\n    instruction_answer_triples = []\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n        futures = [executor.submit(generate_instruction_answer_triples, instruction, desired_story, client) for instruction, desired_story in stories]\n\n        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n        instruction_answer_triples.extend(future.result())\n\n    instructions, rejected_story, chosen_story = zip(*instruction_answer_triples)\n    return Dataset.from_dict({\n        \"prompt\": list(instructions),\n        \"rejected\": list(rejected_story),\n        \"chosen\": list(chosen_story)\n        })\n5. The main function\nThis function initializes the OpenAI client, loads the dataset, creates a preference dataset, and uploads it to the Hugging Face Hub.\ndef main() -&gt; Dataset:\n    client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n\n    # 1. Load the raw data\n    # Load the train and test splits\n    train_dataset = load_dataset(\"tanquangduong/TinyStories_Instruction\", split=\"train\")\n    test_dataset = load_dataset(\"tanquangduong/TinyStories_Instruction\", split=\"test\")\n\n    # Combine the datasets\n    raw_dataset = concatenate_datasets([train_dataset, test_dataset])\n\n    print(\"Raw dataset:\")\n    print(raw_dataset.to_pandas())\n\n    # 2. Create preference dataset\n    preference_dataset = create_preference_dataset(raw_dataset, client)\n    print(\"Preference dataset:\")\n    print(preference_dataset.to_pandas())\n\n    # 3. Train/test split and export\n    filtered_dataset = preference_dataset.train_test_split(test_size=0.1)\n    filtered_dataset.push_to_hub(\"tanquangduong/TinyStories_Preference\")\n6. Hugging Face Hub Login\nTo authenticate with Hugging Face and run the pipeline:\nfrom huggingface_hub import login\n# Log in to the Hugging Face Hub\nlogin(token=userdata.get('HF_TOKEN'))\n\n# Launch the pipeline to create instruction dataset\nmain()",
    "crumbs": [
      "üìùPosts",
      "üß™ **Data Processing**",
      "2. Preference Dataset"
    ]
  },
  {
    "objectID": "posts/preference-dataset-dpo.html#conclusion",
    "href": "posts/preference-dataset-dpo.html#conclusion",
    "title": "Preference Dataset Creation for DPO Fine-Tuning",
    "section": "Conclusion",
    "text": "Conclusion\nThe article outlines the process of creating a Preference Dataset for Direct Preference Optimization (DPO) to align AI-generated outputs with human preferences. It focuses on enhancing a story-generation model for 5-year-olds by building on a previously created instruction dataset. The dataset consists of prompts, human-preferred responses, and rejected responses, allowing the model to learn desired behavior. Key steps include extracting instruction-output pairs, generating AI responses using OpenAI‚Äôs API, and organizing the data into preference triplets. The final dataset, prepared for fine-tuning, is uploaded to the Hugging Face Hub, improving the AI‚Äôs ability to produce engaging, age-appropriate content.",
    "crumbs": [
      "üìùPosts",
      "üß™ **Data Processing**",
      "2. Preference Dataset"
    ]
  }
]